{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Flask ã‚’ä½¿ã£ãŸç°¡æ˜“Webã‚¢ãƒ—ãƒª"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ã¾ãšã¯å¿…è¦ãªãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting flask\n",
      "  Downloading flask-3.1.0-py3-none-any.whl (102 kB)\n",
      "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 102 kB 931 kB/s eta 0:00:01\n",
      "\u001b[?25hCollecting flask_cors\n",
      "  Downloading flask_cors-5.0.1-py3-none-any.whl (11 kB)\n",
      "Requirement already satisfied: speechrecognition in ./.whisper_env/lib/python3.9/site-packages (from -r requirements.txt (line 3)) (3.14.1)\n",
      "Requirement already satisfied: openai in ./.whisper_env/lib/python3.9/site-packages (from -r requirements.txt (line 4)) (1.63.2)\n",
      "Requirement already satisfied: python-dotenv in ./.whisper_env/lib/python3.9/site-packages (from -r requirements.txt (line 5)) (1.0.1)\n",
      "Collecting flask_socketio\n",
      "  Downloading Flask_SocketIO-5.5.1-py3-none-any.whl (18 kB)\n",
      "Requirement already satisfied: requests in ./.whisper_env/lib/python3.9/site-packages (from -r requirements.txt (line 7)) (2.32.3)\n",
      "Collecting itsdangerous>=2.2\n",
      "  Downloading itsdangerous-2.2.0-py3-none-any.whl (16 kB)\n",
      "Collecting Werkzeug>=3.1\n",
      "  Downloading werkzeug-3.1.3-py3-none-any.whl (224 kB)\n",
      "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 224 kB 1.5 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: blinker>=1.9 in ./.whisper_env/lib/python3.9/site-packages (from flask->-r requirements.txt (line 1)) (1.9.0)\n",
      "Requirement already satisfied: importlib-metadata>=3.6 in ./.whisper_env/lib/python3.9/site-packages (from flask->-r requirements.txt (line 1)) (8.6.1)\n",
      "Requirement already satisfied: click>=8.1.3 in ./.whisper_env/lib/python3.9/site-packages (from flask->-r requirements.txt (line 1)) (8.1.8)\n",
      "Requirement already satisfied: Jinja2>=3.1.2 in ./.whisper_env/lib/python3.9/site-packages (from flask->-r requirements.txt (line 1)) (3.1.5)\n",
      "Requirement already satisfied: typing-extensions in ./.whisper_env/lib/python3.9/site-packages (from speechrecognition->-r requirements.txt (line 3)) (4.12.2)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in ./.whisper_env/lib/python3.9/site-packages (from openai->-r requirements.txt (line 4)) (0.28.1)\n",
      "Requirement already satisfied: sniffio in ./.whisper_env/lib/python3.9/site-packages (from openai->-r requirements.txt (line 4)) (1.3.1)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in ./.whisper_env/lib/python3.9/site-packages (from openai->-r requirements.txt (line 4)) (0.8.2)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in ./.whisper_env/lib/python3.9/site-packages (from openai->-r requirements.txt (line 4)) (2.10.6)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in ./.whisper_env/lib/python3.9/site-packages (from openai->-r requirements.txt (line 4)) (4.8.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in ./.whisper_env/lib/python3.9/site-packages (from openai->-r requirements.txt (line 4)) (1.9.0)\n",
      "Requirement already satisfied: tqdm>4 in ./.whisper_env/lib/python3.9/site-packages (from openai->-r requirements.txt (line 4)) (4.67.1)\n",
      "Collecting python-socketio>=5.12.0\n",
      "  Downloading python_socketio-5.12.1-py3-none-any.whl (76 kB)\n",
      "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 76 kB 6.6 MB/s  eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: urllib3<3,>=1.21.1 in ./.whisper_env/lib/python3.9/site-packages (from requests->-r requirements.txt (line 7)) (2.3.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./.whisper_env/lib/python3.9/site-packages (from requests->-r requirements.txt (line 7)) (3.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./.whisper_env/lib/python3.9/site-packages (from requests->-r requirements.txt (line 7)) (2025.1.31)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./.whisper_env/lib/python3.9/site-packages (from requests->-r requirements.txt (line 7)) (3.4.1)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in ./.whisper_env/lib/python3.9/site-packages (from anyio<5,>=3.5.0->openai->-r requirements.txt (line 4)) (1.2.2)\n",
      "Requirement already satisfied: httpcore==1.* in ./.whisper_env/lib/python3.9/site-packages (from httpx<1,>=0.23.0->openai->-r requirements.txt (line 4)) (1.0.7)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in ./.whisper_env/lib/python3.9/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai->-r requirements.txt (line 4)) (0.14.0)\n",
      "Requirement already satisfied: zipp>=3.20 in ./.whisper_env/lib/python3.9/site-packages (from importlib-metadata>=3.6->flask->-r requirements.txt (line 1)) (3.21.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./.whisper_env/lib/python3.9/site-packages (from Jinja2>=3.1.2->flask->-r requirements.txt (line 1)) (3.0.2)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in ./.whisper_env/lib/python3.9/site-packages (from pydantic<3,>=1.9.0->openai->-r requirements.txt (line 4)) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in ./.whisper_env/lib/python3.9/site-packages (from pydantic<3,>=1.9.0->openai->-r requirements.txt (line 4)) (2.27.2)\n",
      "Collecting python-engineio>=4.11.0\n",
      "  Downloading python_engineio-4.11.2-py3-none-any.whl (59 kB)\n",
      "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 59 kB 2.2 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting bidict>=0.21.0\n",
      "  Downloading bidict-0.23.1-py3-none-any.whl (32 kB)\n",
      "Collecting simple-websocket>=0.10.0\n",
      "  Downloading simple_websocket-1.1.0-py3-none-any.whl (13 kB)\n",
      "Collecting wsproto\n",
      "  Downloading wsproto-1.2.0-py3-none-any.whl (24 kB)\n",
      "Installing collected packages: wsproto, simple-websocket, Werkzeug, python-engineio, itsdangerous, bidict, python-socketio, flask, flask-socketio, flask-cors\n",
      "Successfully installed Werkzeug-3.1.3 bidict-0.23.1 flask-3.1.0 flask-cors-5.0.1 flask-socketio-5.5.1 itsdangerous-2.2.0 python-engineio-4.11.2 python-socketio-5.12.1 simple-websocket-1.1.0 wsproto-1.2.0\n",
      "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 25.0.1 is available.\n",
      "You should consider upgrading via the '/Users/fujinohidenori/dev/project/.whisper_env/bin/python -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ãã®ï¼‘ã€€ã¨ã‚Šã‚ãˆãšéŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã§ãã‚‹ã‚ˆã†ã«ã™ã‚‹ï¼"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting static/index.html\n"
     ]
    }
   ],
   "source": [
    "%%writefile static/index.html\n",
    "\n",
    "<!DOCTYPE html>\n",
    "<html lang=\"ja\">\n",
    "  <head>\n",
    "    <meta charset=\"UTF-8\" />\n",
    "    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\" />\n",
    "    <title>Voice Chat App</title>\n",
    "  </head>\n",
    "  <body>\n",
    "    <h1>Voice Chat App</h1>\n",
    "    <button id=\"start\">é–‹å§‹</button>\n",
    "    <button id=\"stop\" disabled>åœæ­¢</button>\n",
    "    <p><strong>æ–‡å­—èµ·ã“ã—:</strong> <span id=\"transcription\"></span></p>\n",
    "    <p><strong>AIã®å¿œç­”:</strong> <span id=\"aiResponse\"></span></p>\n",
    "\n",
    "    <script>\n",
    "      document.addEventListener(\"DOMContentLoaded\", () => {\n",
    "        const startButton = document.getElementById(\"start\");\n",
    "        const stopButton = document.getElementById(\"stop\");\n",
    "        const transcriptionElement = document.getElementById(\"transcription\");\n",
    "        const aiResponseElement = document.getElementById(\"aiResponse\");\n",
    "        let mediaRecorder;\n",
    "        let audioChunks = [];\n",
    "\n",
    "        startButton.addEventListener(\"click\", async () => {\n",
    "          const stream = await navigator.mediaDevices.getUserMedia({\n",
    "            audio: true,\n",
    "          });\n",
    "          mediaRecorder = new MediaRecorder(stream);\n",
    "\n",
    "          mediaRecorder.ondataavailable = (event) => {\n",
    "            audioChunks.push(event.data);\n",
    "          };\n",
    "\n",
    "          mediaRecorder.onstop = async () => {\n",
    "            const audioBlob = new Blob(audioChunks, { type: \"audio/webm\" });\n",
    "            audioChunks = [];\n",
    "\n",
    "            const formData = new FormData();\n",
    "            formData.append(\"audio\", audioBlob, \"recording.webm\");\n",
    "\n",
    "            fetch(\"/upload\", {\n",
    "              method: \"POST\",\n",
    "              body: formData,\n",
    "            })\n",
    "              .then((response) => response.json())\n",
    "              .then((data) => {\n",
    "                transcriptionElement.textContent =\n",
    "                  data.text || \"èªè­˜ã§ãã¾ã›ã‚“ã§ã—ãŸã€‚\";\n",
    "                aiResponseElement.textContent =\n",
    "                  data.ai_response || \"AIã®å¿œç­”ãªã—ã€‚\";\n",
    "              })\n",
    "              .catch((error) => {\n",
    "                console.error(\"Upload failed:\", error);\n",
    "                transcriptionElement.textContent = \"ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸã€‚\";\n",
    "                aiResponseElement.textContent = \"\";\n",
    "              });\n",
    "          };\n",
    "\n",
    "          mediaRecorder.start();\n",
    "          startButton.disabled = true;\n",
    "          stopButton.disabled = false;\n",
    "        });\n",
    "\n",
    "        stopButton.addEventListener(\"click\", () => {\n",
    "          mediaRecorder.stop();\n",
    "          startButton.disabled = false;\n",
    "          stopButton.disabled = true;\n",
    "        });\n",
    "      });\n",
    "    </script>\n",
    "  </body>\n",
    "</html>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting app01.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile app01.py\n",
    "\n",
    "from flask import Flask, request, jsonify, send_from_directory\n",
    "import os\n",
    "\n",
    "app = Flask(__name__, static_folder=\"static\")  \n",
    "\n",
    "@app.route(\"/\")\n",
    "def index():\n",
    "    return send_from_directory(\"static\", \"index.html\")\n",
    "\n",
    "@app.route(\"/upload\", methods=[\"POST\"])\n",
    "def upload_audio():\n",
    "    if \"audio\" not in request.files:\n",
    "        return jsonify({\"error\": \"No audio file provided\"}), 400\n",
    "    \n",
    "    audio_file = request.files[\"audio\"]\n",
    "    audio_path = os.path.join(\"uploads\", audio_file.filename)\n",
    "    audio_file.save(audio_path)\n",
    "    \n",
    "    text = \"test\"\n",
    "    ai_response = \"test\"\n",
    "    return jsonify({\"text\": text, \"ai_response\": ai_response})\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    app.run(debug=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ãµã‚€ï¼ã¨ã‚Šã‚ãˆãšéŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«.webmã¯ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã§ãã‚‹ã‚ˆã†ã«ãªã£ãŸï¼\n",
    "ãŸã speech_recgnitionã§ã¯webmã¯å—ã‘å…¥ã‚Œãªã„ã®ã§ï¼Œwavãƒ•ã‚¡ã‚¤ãƒ«ã«å¤‰ãˆã‚‹å¿…è¦ãŒã‚ã‚‹"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ãã®ï¼’ã€€ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ãŸéŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’Wavãƒ•ã‚¡ã‚¤ãƒ«ã«å¤‰æ›"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing app02.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile app02.py\n",
    "\n",
    "from flask import Flask, request, jsonify, render_template\n",
    "import os\n",
    "import subprocess\n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "@app.route(\"/\")\n",
    "def index():\n",
    "    return render_template(\"index.html\")  # ãƒ•ãƒ­ãƒ³ãƒˆã‚¨ãƒ³ãƒ‰ã®HTMLã‚’è¡¨ç¤º\n",
    "\n",
    "@app.route(\"/upload\", methods=[\"POST\"])\n",
    "def upload_audio():\n",
    "    if \"audio\" not in request.files:\n",
    "        return jsonify({\"error\": \"No audio file provided\"}), 400\n",
    "    \n",
    "    audio_file = request.files[\"audio\"]\n",
    "    audio_path = os.path.join(\"uploads\", audio_file.filename)\n",
    "    audio_file.save(audio_path)\n",
    "    convert_webm_to_wav(audio_path, \"uploads/output.wav\")\n",
    "    \n",
    "    text = \"test\"\n",
    "    ai_response = \"test\"\n",
    "    return jsonify({\"text\": text, \"ai_response\": ai_response})\n",
    "\n",
    "\n",
    "def convert_webm_to_wav(input_path, output_path):\n",
    "    command = [\n",
    "        \"ffmpeg\",\n",
    "        \"-i\", input_path,  # å…¥åŠ›ãƒ•ã‚¡ã‚¤ãƒ«\n",
    "        \"-ar\", \"16000\",  # ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ãƒ¬ãƒ¼ãƒˆ 16kHz\n",
    "        \"-ac\", \"1\",  # ãƒ¢ãƒãƒ©ãƒ«å¤‰æ›\n",
    "        \"-preset\", \"ultrafast\",  # é€Ÿåº¦æœ€å„ªå…ˆ\n",
    "        output_path\n",
    "    ]\n",
    "    subprocess.run(command, check=True)\n",
    "\n",
    "# ä½¿ã„æ–¹\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    app.run(debug=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "å¤‰æ›ã«å°‘ã€…æ™‚é–“å–ã‚‰ã‚Œã‚‹ãªï¼ï¼ï¼"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ãã®ï¼“ã€€ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã‚’ç›´æ¥Wavãƒ•ã‚¡ã‚¤ãƒ«ã«ã™ã‚‹ï¼"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "å¤‰æ›ã«å°‘ã€…æ™‚é–“ãŒã‹ã‹ã‚‹ã®ãŒæ°—ã«ãªã‚‹ã®ã§ï¼Œã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã®æ®µéšã§ç›´æ¥Wavãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ã§ããªã„ã‹æ¢ã£ã¦ã¿ãŸã‚‰ï¼ŒRecord.jsãªã‚‹ã‚‚ã®ãŒã‚ã‚‹ã‚ˆã†ã ï¼\n",
    "https://github.com/mattdiamond/Recorderjs\n",
    "Recorder.jsã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ã¦\n",
    "ã“ã‚Œã‚’htmlã«çµ„ã¿è¾¼ã‚“ã§ã¿ã‚‹ï¼\n",
    "\n",
    "ã‘ã©ï¼Œæœ€åˆã‚„ã£ã¦ã¿ãŸã‚‰ï¼Œrecorder.jsã§ã‚¨ãƒ©ãƒ¼ã§ãŸï¼\n",
    "CDNãŒã‚ã‚‹ã‚ˆã†ãªã®ã§ï¼Œãã¡ã‚‰ã§ã‚„ã£ãŸã‚‰ã†ã¾ãè¡Œã£ãŸï¼"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting static/index.html\n"
     ]
    }
   ],
   "source": [
    "%%writefile static/index.html\n",
    "\n",
    "<!DOCTYPE html>\n",
    "<html lang=\"ja\">\n",
    "<head>\n",
    "    <meta charset=\"UTF-8\">\n",
    "    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n",
    "    <title>WAVéŒ²éŸ³ï¼†ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰</title>\n",
    "    <!-- Recorder.js ã‚’èª­ã¿è¾¼ã‚€ -->\n",
    "    <script src=\"https://cdn.jsdelivr.net/gh/mattdiamond/Recorderjs@master/dist/recorder.js\"></script>\n",
    "    <!-- <script src=\"recorder.js\"></script> --> \n",
    "    \n",
    "</head>\n",
    "<body>\n",
    "    <h1>WAVéŒ²éŸ³ï¼†ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰</h1>\n",
    "    <button id=\"startRecording\">éŒ²éŸ³é–‹å§‹</button>\n",
    "    <button id=\"stopRecording\" disabled>éŒ²éŸ³åœæ­¢</button>\n",
    "    <!-- <audio id=\"audioPlayback\" controls></audio> -->\n",
    "    <!-- <button id=\"uploadAudio\" disabled>ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰</button> -->\n",
    "    <p><strong>æ–‡å­—èµ·ã“ã—:</strong> <span id=\"transcription\"></span></p>\n",
    "    <p><strong>AIã®å¿œç­”:</strong> <span id=\"aiResponse\"></span></p>\n",
    "\n",
    "    <script>\n",
    "        let audioContext;\n",
    "        let recorder;\n",
    "        let audioBlob;\n",
    "                \n",
    "\n",
    "        document.getElementById(\"startRecording\").addEventListener(\"click\", async () => {\n",
    "            const stream = await navigator.mediaDevices.getUserMedia({ audio: true });\n",
    "            audioContext = new AudioContext();\n",
    "            const source = audioContext.createMediaStreamSource(stream);\n",
    "            recorder = new Recorder(source, { numChannels: 1 }); // ãƒ¢ãƒãƒ©ãƒ«éŒ²éŸ³\n",
    "            recorder.record();\n",
    "\n",
    "            document.getElementById(\"startRecording\").disabled = true;\n",
    "            document.getElementById(\"stopRecording\").disabled = false;\n",
    "        });\n",
    "\n",
    "        document.getElementById(\"stopRecording\").addEventListener(\"click\", () => {\n",
    "            recorder.stop();\n",
    "            recorder.exportWAV((blob) => {\n",
    "                audioBlob = blob;\n",
    "\n",
    "                if (!audioBlob) {\n",
    "                    console.error(\"No audio to upload\");    \n",
    "                    return;\n",
    "                }\n",
    "\n",
    "                const formData = new FormData();\n",
    "                formData.append(\"file\", audioBlob, \"recorded_audio.wav\");\n",
    "\n",
    "                fetch(\"/upload\", {\n",
    "                    method: \"POST\",\n",
    "                    body: formData,\n",
    "                })\n",
    "                .then((response) => response.json())\n",
    "                .then((data) => {\n",
    "                    document.getElementById(\"transcription\").textContent =\n",
    "                    data.text || \"èªè­˜ã§ãã¾ã›ã‚“ã§ã—ãŸã€‚\";\n",
    "                    document.getElementById(\"aiResponse\").textContent =\n",
    "                    data.ai_response || \"AIã®å¿œç­”ãªã—ã€‚\";\n",
    "                })\n",
    "                .catch((error) => {\n",
    "                    console.error(\"Upload failed:\");\n",
    "                    document.getElementById(\"transcription\").textContent = \"ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸã€‚\";\n",
    "                    document.getElementById(\"aiResponse\").textContent = \"\";\n",
    "                });\n",
    "            });\n",
    "\n",
    "            document.getElementById(\"startRecording\").disabled = false;\n",
    "            document.getElementById(\"stopRecording\").disabled = true;\n",
    "\n",
    "\n",
    "        });\n",
    "    </script>\n",
    "</body>\n",
    "</html>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "app01.pyã§å®Ÿè¡Œï¼\n",
    "CORSã®å•é¡Œã§ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã§å¼¾ã‹ã‚Œã¦ã„ã‚‹ã‚ˆã†ã ï¼ï¼ï¼"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "flask_corsã‚’ä½¿ã£ã¦ï¼Œã‚µãƒ¼ãƒã®å´ã§CORSå•é¡Œã‚’ç„¡è¦–ã™ã‚‹ã‚ˆã†ã«è¨­å®šã™ã‚‹ï¼"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting flask_cors\n",
      "  Downloading flask_cors-5.0.1-py3-none-any.whl.metadata (961 bytes)\n",
      "Requirement already satisfied: flask>=0.9 in /Users/fujinohidenori/dev/project/venv/lib/python3.13/site-packages (from flask_cors) (3.1.0)\n",
      "Requirement already satisfied: Werkzeug>=0.7 in /Users/fujinohidenori/dev/project/venv/lib/python3.13/site-packages (from flask_cors) (3.1.3)\n",
      "Requirement already satisfied: Jinja2>=3.1.2 in /Users/fujinohidenori/dev/project/venv/lib/python3.13/site-packages (from flask>=0.9->flask_cors) (3.1.5)\n",
      "Requirement already satisfied: itsdangerous>=2.2 in /Users/fujinohidenori/dev/project/venv/lib/python3.13/site-packages (from flask>=0.9->flask_cors) (2.2.0)\n",
      "Requirement already satisfied: click>=8.1.3 in /Users/fujinohidenori/dev/project/venv/lib/python3.13/site-packages (from flask>=0.9->flask_cors) (8.1.8)\n",
      "Requirement already satisfied: blinker>=1.9 in /Users/fujinohidenori/dev/project/venv/lib/python3.13/site-packages (from flask>=0.9->flask_cors) (1.9.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /Users/fujinohidenori/dev/project/venv/lib/python3.13/site-packages (from Werkzeug>=0.7->flask_cors) (3.0.2)\n",
      "Downloading flask_cors-5.0.1-py3-none-any.whl (11 kB)\n",
      "Installing collected packages: flask_cors\n",
      "Successfully installed flask_cors-5.0.1\n"
     ]
    }
   ],
   "source": [
    "!pip install flask_cors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting app03.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile app03.py\n",
    "from flask import Flask, request, jsonify, send_from_directory\n",
    "from flask_cors import CORS\n",
    "import os\n",
    "\n",
    "app = Flask(__name__, static_folder=\"static\")  \n",
    "CORS(app)\n",
    "\n",
    "@app.route(\"/\")\n",
    "def index():\n",
    "    return send_from_directory(\"static\", \"index.html\")\n",
    "\n",
    "@app.route(\"/upload\", methods=[\"POST\"])\n",
    "def upload_audio():\n",
    "    if \"file\" not in request.files:\n",
    "        return jsonify({\"error\": \"No audio file provided\"}), 400\n",
    "    \n",
    "    audio_file = request.files[\"file\"]\n",
    "    audio_path = os.path.join(\"uploads\", audio_file.filename)\n",
    "    audio_file.save(audio_path)\n",
    "    \n",
    "    text = \"test\"\n",
    "    ai_response = \"test\"\n",
    "    return jsonify({\"text\": text, \"ai_response\": ai_response})\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    app.run(debug=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ã‚ˆã—ï¼Œã¨ã‚Šã‚ãˆãšå•é¡Œã¯è§£æ±ºã—ãŸï¼\n",
    "ãƒãƒã£ãŸç†ç”±ã¯ï¼Œfetchã®ã‚¤ãƒ³ã‚¿ãƒ•ã‚§ãƒ¼ã‚¹ã‚’ãƒ•ã‚©ãƒ«ãƒ€åã¨å‹˜é•ã„ã—ã¦ã„ãŸã“ã¨ï¼ã¤ã¾ã‚Šï¼Œapp.routeã§ã¯/uploadã¨ã—ã¦ã„ã‚‹ã®ã«ï¼Œjavascriptã®æ–¹ã§/uploadsã¨ã—ã¦ã„ãŸï¼ã“ã‚Œã«ã‚ˆã‚Šå½“ç„¶ãªãŒã‚‰ã‚¤ãƒ³ã‚¿ãƒ•ã‚§ãƒ¼ã‚¹ãŒãªã„ã‚ã‘ã§ï¼”ï¼ï¼”ã‚¨ãƒ©ãƒ¼ãŒè¿”ã•ã‚Œã‚Œã‚‹ã¨ã„ã†ã“ã¨ã«ãªã£ã¦ã„ãŸï¼åˆ†ã‚Œã°é¦¬é¹¿é¦¬é¹¿ã—ã„å‹˜é•ã„ã‚„ã£ãŸğŸ˜‚\n",
    "\n",
    "ã‚ã¨ï¼Œãƒ‡ãƒãƒƒã‚°ç’°å¢ƒã§ã¯ãƒ«ãƒ¼ãƒˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãŒprojectã«ãªã‚‹ã¨ã„ã†ã¨ã“ã‚ã‚‚ãƒãƒã£ãŸğŸ˜‚"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ãã®ï¼”ã€€Speech Recognitionã«ã‹ã‘ã‚‹\n",
    "ã‚ˆã—ï¼Œã“ã“ã‹ã‚‰ã¯pythonã®å´ã®å‡¦ç†ã«é›†ä¸­"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting app04.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile app04.py\n",
    "from flask import Flask, request, jsonify, send_from_directory\n",
    "from flask_cors import CORS\n",
    "import os\n",
    "import speech_recognition as sr\n",
    "\n",
    "app = Flask(__name__, static_folder=\"static\")  \n",
    "CORS(app)\n",
    "\n",
    "@app.route(\"/\")\n",
    "def index():\n",
    "    return send_from_directory(\"static\", \"index.html\")\n",
    "\n",
    "@app.route(\"/upload\", methods=[\"POST\"])\n",
    "def upload_audio():\n",
    "    if \"file\" not in request.files:\n",
    "        return jsonify({\"error\": \"No audio file provided\"}), 400\n",
    "    \n",
    "    audio_file = request.files[\"file\"]\n",
    "    audio_path = os.path.join(\"uploads\", audio_file.filename)\n",
    "    audio_file.save(audio_path)\n",
    "\n",
    "    # éŸ³å£°èªè­˜\n",
    "    r = sr.Recognizer()\n",
    "    with sr.AudioFile(audio_path) as source:\n",
    "        audio = r.record(source)\n",
    "        text = r.recognize_google(audio, language=\"ja-JP\")\n",
    "        if __debug__: # ãƒ‡ãƒãƒƒã‚°ãƒ¢ãƒ¼ãƒ‰ã®å ´åˆ\n",
    "            print(text)\n",
    "            \n",
    "        ai_response = \"test\"\n",
    "        return jsonify({\"text\": text, \"ai_response\": ai_response})\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    app.run(debug=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ãµã‚€ï¼ã“ã‚Œã§ã¨ã‚Šã‚ãˆãšï¼ŒéŸ³å£°èªè­˜çµæœã‚’è¿”ã›ã‚‹ã‚ˆã†ã«ãªã£ãŸï¼"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ãã®ï¼•ã€€ openai ã®ã€€Chatï¼¿Compelationã‚’ä½¿ã†"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing app05.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile app05.py\n",
    "from flask import Flask, request, jsonify, send_from_directory\n",
    "from flask_cors import CORS\n",
    "import os\n",
    "import speech_recognition as sr\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "app = Flask(__name__, static_folder=\"static\")  \n",
    "CORS(app)\n",
    "\n",
    "@app.route(\"/\")\n",
    "def index():\n",
    "    return send_from_directory(\"static\", \"index.html\")\n",
    "\n",
    "@app.route(\"/upload\", methods=[\"POST\"])\n",
    "def upload_audio():\n",
    "    if \"file\" not in request.files:\n",
    "        return jsonify({\"error\": \"No audio file provided\"}), 400\n",
    "    \n",
    "    audio_file = request.files[\"file\"]\n",
    "    audio_path = os.path.join(\"uploads\", audio_file.filename)\n",
    "    audio_file.save(audio_path)\n",
    "\n",
    "    # éŸ³å£°èªè­˜\n",
    "    r = sr.Recognizer()\n",
    "    with sr.AudioFile(audio_path) as source:\n",
    "        audio = r.record(source)\n",
    "        text = r.recognize_google(audio, language=\"ja-JP\")\n",
    "        if __debug__: # ãƒ‡ãƒãƒƒã‚°ãƒ¢ãƒ¼ãƒ‰ã®å ´åˆ\n",
    "            print(text)\n",
    "\n",
    "        # AIã®å¿œç­”\n",
    "        client = OpenAI()\n",
    "        completion = client.chat.completions.create(\n",
    "            model=\"gpt-4o-mini\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "                {\"role\": \"user\", \"content\": text},\n",
    "            ]\n",
    "        )\n",
    "        ai_response = completion.choices[0].message.content\n",
    "        return jsonify({\"text\": text, \"ai_response\": ai_response})\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    app.run(debug=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ãµã‚€ï¼ã¨ã‚Šã‚ãˆãšã¯å˜ç™ºä¼šè©±ã¯ã§ãã‚‹ã‚ˆã†ã«ãªã£ãŸï¼\n",
    "ç¾æ™‚ç‚¹ã®é•å’Œæ„Ÿãƒ»ä¿®æ­£ã—ãŸã„ç‚¹ã¯\n",
    "ç¾çŠ¶ã ã¨ï¼Œå…¥åŠ›ãƒ†ã‚­ã‚¹ãƒˆã¨ãƒ¬ã‚¹ãƒãƒ³ã‚¹ãŒåŒæ™‚ã«å¸°ã£ã¦ãã¦ã—ã¾ã†ï¼\n",
    "ã©ã†ã«ã‹ï¼Œãã®éƒ¨åˆ†ã‚’ã„ã˜ã‚Œãªã„ã‹ï¼ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚’ã¾ãŸã¤ã«å…ˆã«å…¥åŠ›ãƒ†ã‚­ã‚¹ãƒˆã‚’è¿”ã—ã¦ï¼Œç”»é¢ã«è¡¨ç¤ºã•ã›ã¦ãŠã„ã¦ï¼Œãƒ¬ã‚¹ãƒãƒ³ã‚¹ãŒå¸°ã£ã¦ããŸã‚‰ï¼Œæ”¹ã‚ã¦ãã‚Œã‚’è¿”ã™ã¨ã„ã†æ„Ÿã˜ï¼"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ãã®6ã€€å…¥åŠ›ãƒ†ã‚­ã‚¹ãƒˆã¨ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚’åˆ†ã‘ã¦è¡¨è¨˜ã§ãã‚‹ã‚ˆã†ã«ã™ã‚‹\n",
    "ã‚„ã‚‹ã¨ã—ãŸã‚‰ï¼Œpostã‚’2å›ã«åˆ†ã‘ã‚‹å½¢ã‹ãªï¼Ÿ\n",
    "Copilotã«èã„ã¦ã¿ãŸã‚‰Web Socketã‚’ä½¿ãˆã°ã§ãã‚‹ã¨ãªï¼ï¼ï¼\n",
    "ã¨ã‚Šã‚ãˆãšã‚„ã£ã¦ã¿ã‚‹ã‹ï¼"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install flask-socketio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting app06.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile app06.py\n",
    "from flask import Flask, request, jsonify, send_from_directory\n",
    "from flask_cors import CORS\n",
    "import os\n",
    "import speech_recognition as sr\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "from flask_socketio import SocketIO, emit\n",
    "import threading\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "app = Flask(__name__, static_folder=\"static\")  \n",
    "CORS(app)\n",
    "socketio = SocketIO(app)\n",
    "\n",
    "@app.route(\"/\")\n",
    "def index():\n",
    "    return send_from_directory(\"static\", \"index06.html\")\n",
    "\n",
    "@app.route(\"/upload\", methods=[\"POST\"])\n",
    "def upload_audio():\n",
    "    if \"file\" not in request.files:\n",
    "        return jsonify({\"error\": \"No audio file provided\"}), 400\n",
    "    \n",
    "    audio_file = request.files[\"file\"]\n",
    "    audio_path = os.path.join(\"uploads\", audio_file.filename)\n",
    "    audio_file.save(audio_path)\n",
    "\n",
    "    # éŸ³å£°èªè­˜\n",
    "    r = sr.Recognizer()\n",
    "    with sr.AudioFile(audio_path) as source:\n",
    "        audio = r.record(source)\n",
    "        text = r.recognize_google(audio, language=\"ja-JP\")\n",
    "        if __debug__: # ãƒ‡ãƒãƒƒã‚°ãƒ¢ãƒ¼ãƒ‰ã®å ´åˆ\n",
    "            print(text)\n",
    "\n",
    "    # éŸ³å£°èªè­˜ã®çµæœã‚’æœ€åˆã«è¿”ã™\n",
    "    response = jsonify({\"text\": text})\n",
    "    \n",
    "    # åˆ¥ã‚¹ãƒ¬ãƒƒãƒ‰ã§AIã®å¿œç­”ã‚’å–å¾—\n",
    "    threading.Thread(target=get_ai_response, args=(text,)).start()\n",
    "    \n",
    "    return response\n",
    "\n",
    "def get_ai_response(text):\n",
    "    client = OpenAI()\n",
    "    completion = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "            {\"role\": \"user\", \"content\": text},\n",
    "        ]\n",
    "    )\n",
    "    ai_response = completion.choices[0].message.content\n",
    "    # WebSocketã‚’é€šã˜ã¦ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã«é€šçŸ¥\n",
    "    socketio.emit('ai_response', {'ai_response': ai_response})\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    socketio.run(app, debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting static/index06.html\n"
     ]
    }
   ],
   "source": [
    "%%writefile static/index06.html\n",
    "\n",
    "<!DOCTYPE html>\n",
    "<html lang=\"ja\">\n",
    "<head>\n",
    "    <meta charset=\"UTF-8\">\n",
    "    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n",
    "    <title>WAVéŒ²éŸ³ï¼†ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰</title>\n",
    "    <!-- Recorder.js ã‚’èª­ã¿è¾¼ã‚€ -->\n",
    "    <script src=\"https://cdn.jsdelivr.net/gh/mattdiamond/Recorderjs@master/dist/recorder.js\"></script>\n",
    "\n",
    "    <!-- Socket.IO ã‚’èª­ã¿è¾¼ã‚€ -->\n",
    "    <script src=\"https://cdn.socket.io/4.0.0/socket.io.min.js\"></script>\n",
    "\n",
    "    \n",
    "</head>\n",
    "<body>\n",
    "    <h1>WAVéŒ²éŸ³ï¼†ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰</h1>\n",
    "    <button id=\"startRecording\">éŒ²éŸ³é–‹å§‹</button>\n",
    "    <button id=\"stopRecording\" disabled>éŒ²éŸ³åœæ­¢</button>\n",
    "    <!-- <audio id=\"audioPlayback\" controls></audio> -->\n",
    "    <!-- <button id=\"uploadAudio\" disabled>ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰</button> -->\n",
    "    <p><strong>æ–‡å­—èµ·ã“ã—:</strong> <span id=\"transcription\"></span></p>\n",
    "    <p><strong>AIã®å¿œç­”:</strong> <span id=\"aiResponse\"></span></p>\n",
    "\n",
    "    <script>\n",
    "        let audioContext;\n",
    "        let recorder;\n",
    "        let audioBlob;\n",
    "\n",
    "        document.addEventListener(\"DOMContentLoaded\", () => {\n",
    "            const socket = io();\n",
    "\n",
    "            socket.on('ai_response', (data) => {\n",
    "                document.getElementById(\"aiResponse\").textContent = data.ai_response;\n",
    "            });\n",
    "        });        \n",
    "\n",
    "        document.getElementById(\"startRecording\").addEventListener(\"click\", async () => {\n",
    "            const stream = await navigator.mediaDevices.getUserMedia({ audio: true });\n",
    "            audioContext = new AudioContext();\n",
    "            const source = audioContext.createMediaStreamSource(stream);\n",
    "            recorder = new Recorder(source, { numChannels: 1 }); // ãƒ¢ãƒãƒ©ãƒ«éŒ²éŸ³\n",
    "            recorder.record();\n",
    "\n",
    "            document.getElementById(\"startRecording\").disabled = true;\n",
    "            document.getElementById(\"stopRecording\").disabled = false;\n",
    "        });\n",
    "\n",
    "        document.getElementById(\"stopRecording\").addEventListener(\"click\", () => {\n",
    "            recorder.stop();\n",
    "            recorder.exportWAV((blob) => {\n",
    "                audioBlob = blob;\n",
    "\n",
    "                if (!audioBlob) {\n",
    "                    console.error(\"No audio to upload\");    \n",
    "                    return;\n",
    "                }\n",
    "\n",
    "                const formData = new FormData();\n",
    "                formData.append(\"file\", audioBlob, \"recorded_audio.wav\");\n",
    "\n",
    "                fetch(\"/upload\", {\n",
    "                    method: \"POST\",\n",
    "                    body: formData,\n",
    "                })\n",
    "                .then((response) => response.json())\n",
    "                .then((data) => {\n",
    "                    document.getElementById(\"transcription\").textContent =\n",
    "                    data.text || \"èªè­˜ã§ãã¾ã›ã‚“ã§ã—ãŸã€‚\";\n",
    "                })\n",
    "                .catch((error) => {\n",
    "                    console.error(\"Upload failed:\");\n",
    "                    document.getElementById(\"transcription\").textContent = \"ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸã€‚\";\n",
    "                    document.getElementById(\"aiResponse\").textContent = \"\";\n",
    "                });\n",
    "            });\n",
    "\n",
    "            document.getElementById(\"startRecording\").disabled = false;\n",
    "            document.getElementById(\"stopRecording\").disabled = true;\n",
    "\n",
    "\n",
    "        });\n",
    "    </script>\n",
    "</body>\n",
    "</html>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ãµã‚€ï¼å…¥åŠ›ã¨ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚’åˆ¥ã€…ã«è¡¨è¨˜ã§ãã‚‹ã‚ˆã†ã«ãªã£ãŸï¼"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ãã®ï¼—ã€€ç¶™ç¶šçš„ãªä¼šè©±ã‚’å‡ºæ¥ã‚‹ã‚ˆã†ã«ã™ã‚‹ï¼\n",
    "ã¨ã‚Šã‚ãˆãšï¼Œãƒ•ãƒ­ãƒ³ãƒˆã‚¨ãƒ³ãƒ‰ã§ãƒ­ã‚°ã‚’è¨˜è¼‰ã™ã‚‹éƒ¨åˆ†ã«ã¤ã„ã¦ã¯åˆ¥ã«è€ƒãˆã¦ï¼Œã¾ãšã¯ãƒ‘ã‚¤ã‚½ãƒ³ã§å‹•ã‹ã™"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing app07.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile app07.py\n",
    "from flask import Flask, request, jsonify, send_from_directory\n",
    "from flask_cors import CORS\n",
    "import os\n",
    "import speech_recognition as sr\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "from flask_socketio import SocketIO, emit\n",
    "import threading\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "app = Flask(__name__, static_folder=\"static\")  \n",
    "CORS(app)\n",
    "socketio = SocketIO(app)\n",
    "\n",
    "# ä¼šè©±ãƒ­ã‚°ã‚’ä¿æŒã™ã‚‹å¤‰æ•°\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}\n",
    "]\n",
    "\n",
    "@app.route(\"/\")\n",
    "def index():\n",
    "    return send_from_directory(\"static\", \"index06.html\")\n",
    "\n",
    "@app.route(\"/upload\", methods=[\"POST\"])\n",
    "def upload_audio():\n",
    "    if \"file\" not in request.files:\n",
    "        return jsonify({\"error\": \"No audio file provided\"}), 400\n",
    "    \n",
    "    audio_file = request.files[\"file\"]\n",
    "    audio_path = os.path.join(\"uploads\", audio_file.filename)\n",
    "    audio_file.save(audio_path)\n",
    "\n",
    "    # éŸ³å£°èªè­˜\n",
    "    r = sr.Recognizer()\n",
    "    with sr.AudioFile(audio_path) as source:\n",
    "        audio = r.record(source)\n",
    "        text = r.recognize_google(audio, language=\"ja-JP\")\n",
    "        if __debug__: # ãƒ‡ãƒãƒƒã‚°ãƒ¢ãƒ¼ãƒ‰ã®å ´åˆ\n",
    "            print(text)\n",
    "\n",
    "    # éŸ³å£°èªè­˜ã®çµæœã‚’æœ€åˆã«è¿”ã™\n",
    "    response = jsonify({\"text\": text})\n",
    "    \n",
    "    # åˆ¥ã‚¹ãƒ¬ãƒƒãƒ‰ã§AIã®å¿œç­”ã‚’å–å¾—\n",
    "    threading.Thread(target=get_ai_response, args=(text,)).start()\n",
    "    \n",
    "    return response\n",
    "\n",
    "def get_ai_response(text):\n",
    "    client = OpenAI()\n",
    "    messages.append({\"role\": \"user\", \"content\": text})\n",
    "    completion = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=messages\n",
    "    )\n",
    "    ai_response = completion.choices[0].message.content\n",
    "    messages.append({\"role\": \"assistant\", \"content\": ai_response})\n",
    "    # WebSocketã‚’é€šã˜ã¦ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã«é€šçŸ¥\n",
    "    socketio.emit('ai_response', {'ai_response': ai_response})\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    socketio.run(app, debug=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ãµã‚€ï¼ç¶™ç¶šæ€§ã®ã‚ã‚‹ä¼šè©±ã‚‚ã§ãã‚‹ã‚ˆã†ã«ãªã£ãŸï¼\n",
    "ã˜ã‚ƒã‚æ¬¡ã¯ï¼ŒTTSã‚’çµ„ã¿è¾¼ã¿ãŸã„ã­ï¼\n",
    "\n",
    "èª¿ã¹ãŸã‚‰Voice Voxã§ã‚ã‚Œã°APIãŒä½¿ãˆã‚‹ã¨ã®ã“ã¨ï¼ãŸã ï¼Œã“ã®APIã¯ã‚ãã¾ã§ãƒ­ãƒ¼ã‚«ãƒ«ã‚µãƒ¼ãƒã§å‹•ãã‚‚ã®ã«ãªã£ã¦ã„ã‚‹ï¼\n",
    "ãƒ­ãƒ¼ã‚«ãƒ«ã§ã©ã‚Œãã‚‰ã„å‹•ä½œæ™‚é–“ã‹ã‹ã‚‹ã‚“ã‚„ã‚ï¼Ÿï¼Ÿ\n",
    "ã¨ã‚Šã‚ãˆãšè©¦ã™ã‹ãƒ»ãƒ»ãƒ»ï¼"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ãã®8ã€€éŸ³å£°åˆæˆæ©Ÿèƒ½ã‚’ä½¿ã£ã¦ã¿ã‚‹\n",
    "VoiceVox GUIã‚’ç«‹ã¡ä¸Šã’ã¦ãŠãï¼ã“ã‚Œã«ã‚ˆã‚Šãƒ­ãƒ¼ã‚«ãƒ«ã«VoiceVoxãŒç«‹ã¡ä¸ŠãŒã‚ŠAPIãŒä½¿ãˆã‚‹ï¼\n",
    "ãã®ã¾ã¾é–¢æ•°ã‚’ä½œã£ã¦ãã‚Œã¦ã„ã‚‹äººãŒã„ãŸã®ã§æ‹å€Ÿ\n",
    "\n",
    "https://zenn.dev/zenn24yykiitos/articles/fff3c954ddf42c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in c:\\work\\vscode\\aichatbot\\.venv\\lib\\site-packages (2.32.3)"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\work\\vscode\\aichatbot\\.venv\\lib\\site-packages (from requests) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\work\\vscode\\aichatbot\\.venv\\lib\\site-packages (from requests) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\work\\vscode\\aichatbot\\.venv\\lib\\site-packages (from requests) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\work\\vscode\\aichatbot\\.venv\\lib\\site-packages (from requests) (2025.1.31)\n"
     ]
    }
   ],
   "source": [
    "!pip install requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing app08.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile app08.py\n",
    "from flask import Flask, request, jsonify, send_from_directory, send_file\n",
    "from flask_cors import CORS\n",
    "import os\n",
    "import speech_recognition as sr\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "from flask_socketio import SocketIO, emit\n",
    "import threading\n",
    "import requests\n",
    "import json\n",
    "import time\n",
    "\n",
    "#ã€€ç’°å¢ƒå¤‰æ•°ã®èª­ã¿è¾¼ã¿\n",
    "load_dotenv()\n",
    "\n",
    "# Flaskã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã®ä½œæˆ\n",
    "app = Flask(__name__, static_folder=\"static\")  \n",
    "\n",
    "# CORSã®è¨­å®š\n",
    "CORS(app)\n",
    "\n",
    "# Socket.IOã®è¨­å®š\n",
    "socketio = SocketIO(app)\n",
    "\n",
    "# ä¼šè©±ãƒ­ã‚°ã‚’ä¿æŒã™ã‚‹å¤‰æ•°\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}\n",
    "]\n",
    "\n",
    "#--------------------------------------------------\n",
    "\n",
    "# ãƒ«ãƒ¼ãƒˆãƒ‘ã‚¹ã¸ã®ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚’å‡¦ç†ã™ã‚‹\n",
    "@app.route(\"/\")\n",
    "def index():\n",
    "    return send_from_directory(\"static\", \"index08.html\")\n",
    "\n",
    "# /upload ã¸ã®ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚’å‡¦ç†ã™ã‚‹\n",
    "@app.route(\"/upload\", methods=[\"POST\"])\n",
    "def upload_audio():\n",
    "    if \"file\" not in request.files:\n",
    "        return jsonify({\"error\": \"No audio file provided\"}), 400\n",
    "    \n",
    "    audio_file = request.files[\"file\"]\n",
    "    audio_path = os.path.join(\"uploads\", audio_file.filename)\n",
    "    audio_file.save(audio_path)\n",
    "\n",
    "    # éŸ³å£°èªè­˜\n",
    "    r = sr.Recognizer()\n",
    "    with sr.AudioFile(audio_path) as source:\n",
    "        audio = r.record(source)\n",
    "        text = r.recognize_google(audio, language=\"ja-JP\")\n",
    "        if __debug__: # ãƒ‡ãƒãƒƒã‚°ãƒ¢ãƒ¼ãƒ‰ã®å ´åˆ\n",
    "            print(text)\n",
    "\n",
    "    # éŸ³å£°èªè­˜ã®çµæœã‚’æœ€åˆã«è¿”ã™\n",
    "    response = jsonify({\"text\": text})\n",
    "    \n",
    "    # åˆ¥ã‚¹ãƒ¬ãƒƒãƒ‰ã§AIã®å¿œç­”ã‚’å–å¾—\n",
    "    threading.Thread(target=get_ai_response, args=(text,)).start()\n",
    "    \n",
    "    return response\n",
    "\n",
    "# éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æä¾›ã™ã‚‹ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ\n",
    "@app.route(\"/audio/<filename>\")\n",
    "def get_audio(filename):\n",
    "    return send_file(os.path.join(\"uploads\",filename))\n",
    "\n",
    "\n",
    "#--------------------------------------------------\n",
    "\n",
    "# AIã®å¿œç­”ã‚’å–å¾—ã™ã‚‹é–¢æ•° \n",
    "def get_ai_response(text):\n",
    "    \n",
    "    # ç¾åœ¨ã®æ™‚åˆ»å–å¾—\n",
    "    start = time.time()\n",
    "\n",
    "    # OpenAIã®APIã‚’å‘¼ã³å‡ºã—ã¦AIã®å¿œç­”ã‚’å–å¾—\n",
    "    client = OpenAI()\n",
    "    messages.append({\"role\": \"user\", \"content\": text})\n",
    "    completion = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=messages\n",
    "    )\n",
    "    ai_response = completion.choices[0].message.content\n",
    "    messages.append({\"role\": \"assistant\", \"content\": ai_response})\n",
    "\n",
    "    # å‡¦ç†æ™‚é–“ã®è¨ˆç®—\n",
    "    ai_time = time.time() - start\n",
    "    print(f\"å‡¦ç†æ™‚é–“: {ai_time} [sec]\") \n",
    "\n",
    "    # éŸ³å£°åˆæˆ\n",
    "    filename = synthesize_voice(ai_response)\n",
    "\n",
    "    # å‡¦ç†æ™‚é–“ã®è¨ˆç®—\n",
    "    voice_time = time.time() - start - ai_time\n",
    "    print(f\"éŸ³å£°åˆæˆæ™‚é–“: {voice_time} [sec]\")\n",
    "    \n",
    "    # WebSocketã‚’é€šã˜ã¦ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã«é€šçŸ¥\n",
    "    socketio.emit('ai_response', {'ai_response': ai_response, 'audio': filename})\n",
    "\n",
    "\n",
    "\n",
    "# éŸ³å£°åˆæˆã‚’è¡Œãªã†é–¢æ•°\n",
    "def synthesize_voice(text, speaker=1, filename=\"uploads/output.wav\"):\n",
    "    # 1. ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰éŸ³å£°åˆæˆã®ãŸã‚ã®ã‚¯ã‚¨ãƒªã‚’ä½œæˆ\n",
    "    query_payload = {'text': text, 'speaker': speaker}\n",
    "    query_response = requests.post(f'http://localhost:50021/audio_query', params=query_payload)\n",
    "\n",
    "    if query_response.status_code != 200:\n",
    "        print(f\"Error in audio_query: {query_response.text}\")\n",
    "        return\n",
    "\n",
    "    query = query_response.json()\n",
    "\n",
    "    # 2. ã‚¯ã‚¨ãƒªã‚’å…ƒã«éŸ³å£°ãƒ‡ãƒ¼ã‚¿ã‚’ç”Ÿæˆ\n",
    "    synthesis_payload = {'speaker': speaker}\n",
    "    synthesis_response = requests.post(f'http://localhost:50021/synthesis', params=synthesis_payload, json=query)\n",
    "\n",
    "    if synthesis_response.status_code == 200:\n",
    "        # éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã¨ã—ã¦ä¿å­˜\n",
    "        with open(filename, 'wb') as f:\n",
    "            f.write(synthesis_response.content)\n",
    "        print(f\"éŸ³å£°ãŒ {filename} ã«ä¿å­˜ã•ã‚Œã¾ã—ãŸã€‚\")\n",
    "        return \"output.wav\"\n",
    "    else:\n",
    "        print(f\"Error in synthesis: {synthesis_response.text}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    socketio.run(app, debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting static/index08.html\n"
     ]
    }
   ],
   "source": [
    "%%writefile static/index08.html\n",
    "\n",
    "<!DOCTYPE html>\n",
    "<html lang=\"ja\">\n",
    "<head>\n",
    "    <meta charset=\"UTF-8\">\n",
    "    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n",
    "    <title>WAVéŒ²éŸ³ï¼†ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰</title>\n",
    "    <!-- Recorder.js ã‚’èª­ã¿è¾¼ã‚€ -->\n",
    "    <script src=\"https://cdn.jsdelivr.net/gh/mattdiamond/Recorderjs@master/dist/recorder.js\"></script>\n",
    "\n",
    "    <!-- Socket.IO ã‚’èª­ã¿è¾¼ã‚€ -->\n",
    "    <script src=\"https://cdn.socket.io/4.0.0/socket.io.min.js\"></script>\n",
    "\n",
    "    \n",
    "</head>\n",
    "<body>\n",
    "    <h1>WAVéŒ²éŸ³ï¼†ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰</h1>\n",
    "    <button id=\"startRecording\">éŒ²éŸ³é–‹å§‹</button>\n",
    "    <button id=\"stopRecording\" disabled>éŒ²éŸ³åœæ­¢</button>\n",
    "    <!-- <audio id=\"audioPlayback\" controls></audio> -->\n",
    "    <!-- <button id=\"uploadAudio\" disabled>ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰</button> -->\n",
    "    <p><strong>æ–‡å­—èµ·ã“ã—:</strong> <span id=\"transcription\"></span></p>\n",
    "    <p><strong>AIã®å¿œç­”:</strong> <span id=\"aiResponse\"></span></p>\n",
    "\n",
    "    <script>\n",
    "        let audioContext;\n",
    "        let recorder;\n",
    "        let audioBlob;\n",
    "\n",
    "        document.addEventListener(\"DOMContentLoaded\", () => {\n",
    "            const socket = io();\n",
    "\n",
    "            socket.on('ai_response', (data) => {\n",
    "                document.getElementById(\"aiResponse\").textContent = data.ai_response;\n",
    "                \n",
    "                // éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’è‡ªå‹•å†ç”Ÿã™ã‚‹å‡¦ç†\n",
    "                if (data.audio) {\n",
    "                    const audio = new Audio(`/audio/${data.audio}`);\n",
    "                    audio.play();\n",
    "                }    \n",
    "            });\n",
    "        });        \n",
    "\n",
    "        document.getElementById(\"startRecording\").addEventListener(\"click\", async () => {\n",
    "            const stream = await navigator.mediaDevices.getUserMedia({ audio: true });\n",
    "            audioContext = new AudioContext();\n",
    "            const source = audioContext.createMediaStreamSource(stream);\n",
    "            recorder = new Recorder(source, { numChannels: 1 }); // ãƒ¢ãƒãƒ©ãƒ«éŒ²éŸ³\n",
    "            recorder.record();\n",
    "\n",
    "            document.getElementById(\"startRecording\").disabled = true;\n",
    "            document.getElementById(\"stopRecording\").disabled = false;\n",
    "        });\n",
    "\n",
    "        document.getElementById(\"stopRecording\").addEventListener(\"click\", () => {\n",
    "            recorder.stop();\n",
    "            recorder.exportWAV((blob) => {\n",
    "                audioBlob = blob;\n",
    "\n",
    "                if (!audioBlob) {\n",
    "                    console.error(\"No audio to upload\");    \n",
    "                    return;\n",
    "                }\n",
    "\n",
    "                const formData = new FormData();\n",
    "                formData.append(\"file\", audioBlob, \"recorded_audio.wav\");\n",
    "\n",
    "                fetch(\"/upload\", {\n",
    "                    method: \"POST\",\n",
    "                    body: formData,\n",
    "                })\n",
    "                .then((response) => response.json())\n",
    "                .then((data) => {\n",
    "                    document.getElementById(\"transcription\").textContent =\n",
    "                    data.text || \"èªè­˜ã§ãã¾ã›ã‚“ã§ã—ãŸã€‚\";\n",
    "                })\n",
    "                .catch((error) => {\n",
    "                    console.error(\"Upload failed:\");\n",
    "                    document.getElementById(\"transcription\").textContent = \"ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸã€‚\";\n",
    "                    document.getElementById(\"aiResponse\").textContent = \"\";\n",
    "                });\n",
    "            });\n",
    "\n",
    "            document.getElementById(\"startRecording\").disabled = false;\n",
    "            document.getElementById(\"stopRecording\").disabled = true;\n",
    "\n",
    "\n",
    "        });\n",
    "    </script>\n",
    "</body>\n",
    "</html>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ã¨ã‚Šã‚ãˆãšå‹•ãå½¢ã«ã¯ã§ããŸï¼ï¼\n",
    "ãŸã ï¼Œã©ã†ã—ã¦ã‚‚ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã¯é…ã„ï¼ï¼ï¼ï¼"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ãã®ï¼™ï¼šloggingãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã‚’ã¤ã‹ã£ã¦ã¿ã‚‹ã“ã¨ã«ã™ã‚‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting voicecahtapp09.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile voicecahtapp09.py\n",
    "from flask import Flask, request, jsonify, send_from_directory, send_file\n",
    "from flask_cors import CORS\n",
    "import os\n",
    "import speech_recognition as sr\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "from flask_socketio import SocketIO, emit\n",
    "import threading\n",
    "import requests\n",
    "import json\n",
    "import time\n",
    "import logging\n",
    "\n",
    "#ã€€ç’°å¢ƒå¤‰æ•°ã®èª­ã¿è¾¼ã¿\n",
    "load_dotenv()\n",
    "\n",
    "# Flaskã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã®ä½œæˆ\n",
    "app = Flask(__name__, static_folder=\"static\")  \n",
    "\n",
    "# CORSã®è¨­å®š\n",
    "CORS(app)\n",
    "\n",
    "# Socket.IOã®è¨­å®š\n",
    "socketio = SocketIO(app)\n",
    "\n",
    "# ä¼šè©±ãƒ­ã‚°ã‚’ä¿æŒã™ã‚‹å¤‰æ•°\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}\n",
    "]\n",
    "\n",
    "#--------------------------------------------------\n",
    "#loggingã®è¨­å®š\n",
    "logging.basicConfig(\n",
    "    level=logging.DEBUG,\n",
    "    format=\"%(asctime)s [%(levelname)s] %(message)s\",\n",
    "    filename=\"app.log\"\n",
    ")\n",
    "#--------------------------------------------------\n",
    "\n",
    "\n",
    "# ãƒ«ãƒ¼ãƒˆãƒ‘ã‚¹ã¸ã®ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚’å‡¦ç†ã™ã‚‹\n",
    "@app.route(\"/\")\n",
    "def index():\n",
    "    logging.info(\"index.html ã‚’è¿”ã—ã¾ã™ã€‚\")\n",
    "    return send_from_directory(\"static\", \"index08.html\")\n",
    "\n",
    "# /upload ã¸ã®ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚’å‡¦ç†ã™ã‚‹\n",
    "@app.route(\"/upload\", methods=[\"POST\"])\n",
    "def upload_audio():\n",
    "    logging.info(\"éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¾ã™ã€‚\")\n",
    "    if \"file\" not in request.files:\n",
    "        logging.error(\"No audio file provided\")\n",
    "        return jsonify({\"error\": \"No audio file provided\"}), 400\n",
    "    \n",
    "    audio_file = request.files[\"file\"]\n",
    "    audio_path = os.path.join(\"uploads\", f\"input_{len(messages)}.wav\")\n",
    "    logging.debug(f\"Saving audio file to {audio_path}\")\n",
    "    audio_file.save(audio_path)\n",
    "\n",
    "    # éŸ³å£°èªè­˜\n",
    "    r = sr.Recognizer()\n",
    "    with sr.AudioFile(audio_path) as source:\n",
    "        audio = r.record(source)\n",
    "        text = r.recognize_google(audio, language=\"ja-JP\")\n",
    "        if __debug__: # ãƒ‡ãƒãƒƒã‚°ãƒ¢ãƒ¼ãƒ‰ã®å ´åˆ\n",
    "            logging.debug(text)\n",
    "            print(text)\n",
    "\n",
    "    # éŸ³å£°èªè­˜ã®çµæœã‚’æœ€åˆã«è¿”ã™\n",
    "    response = jsonify({\"text\": text})\n",
    "    \n",
    "    # åˆ¥ã‚¹ãƒ¬ãƒƒãƒ‰ã§AIã®å¿œç­”ã‚’å–å¾—\n",
    "    threading.Thread(target=get_ai_response, args=(text,)).start()\n",
    "    \n",
    "    return response\n",
    "\n",
    "# éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æä¾›ã™ã‚‹ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ\n",
    "@app.route(\"/audio/<filename>\")\n",
    "def get_audio(filename):\n",
    "    return send_file(os.path.join(\"output\",filename))\n",
    "\n",
    "\n",
    "#--------------------------------------------------\n",
    "\n",
    "# AIã®å¿œç­”ã‚’å–å¾—ã™ã‚‹é–¢æ•° \n",
    "def get_ai_response(text):\n",
    "    \n",
    "    # ç¾åœ¨ã®æ™‚åˆ»å–å¾—\n",
    "    start = time.time()\n",
    "\n",
    "    # OpenAIã®APIã‚’å‘¼ã³å‡ºã—ã¦AIã®å¿œç­”ã‚’å–å¾—\n",
    "    client = OpenAI()\n",
    "    messages.append({\"role\": \"user\", \"content\": text})\n",
    "    completion = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=messages\n",
    "    )\n",
    "    ai_response = completion.choices[0].message.content\n",
    "    messages.append({\"role\": \"assistant\", \"content\": ai_response})\n",
    "\n",
    "    # å‡¦ç†æ™‚é–“ã®è¨ˆç®—\n",
    "    ai_time = time.time() - start\n",
    "    logging.debug(f\"å‡¦ç†æ™‚é–“: {ai_time} [sec]\")\n",
    "    print(f\"AIãƒ¬ã‚¹ãƒãƒ³ã‚¹æ™‚é–“: {ai_time} [sec]\") \n",
    "\n",
    "    # éŸ³å£°åˆæˆ\n",
    "    filename = synthesize_voice(ai_response)\n",
    "\n",
    "    # å‡¦ç†æ™‚é–“ã®è¨ˆç®—\n",
    "    voice_time = time.time() - start - ai_time\n",
    "    logging.debug(f\"éŸ³å£°åˆæˆæ™‚é–“: {voice_time} [sec]\")\n",
    "    print(f\"éŸ³å£°åˆæˆæ™‚é–“: {voice_time} [sec]\")\n",
    "    \n",
    "    # WebSocketã‚’é€šã˜ã¦ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã«é€šçŸ¥\n",
    "    socketio.emit('ai_response', {'ai_response': ai_response, 'audio': filename})\n",
    "\n",
    "\n",
    "\n",
    "# éŸ³å£°åˆæˆã‚’è¡Œãªã†é–¢æ•°\n",
    "def synthesize_voice(text, speaker=1):\n",
    "    # 1. ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰éŸ³å£°åˆæˆã®ãŸã‚ã®ã‚¯ã‚¨ãƒªã‚’ä½œæˆ\n",
    "    query_payload = {'text': text, 'speaker': speaker}\n",
    "    query_response = requests.post(f'http://localhost:50021/audio_query', params=query_payload)\n",
    "\n",
    "    if query_response.status_code != 200:\n",
    "        logging.error(f\"Error in audio_query: {query_response.text}\")\n",
    "        print(f\"Error in audio_query: {query_response.text}\")\n",
    "        return\n",
    "\n",
    "    query = query_response.json()\n",
    "\n",
    "    # 2. ã‚¯ã‚¨ãƒªã‚’å…ƒã«éŸ³å£°ãƒ‡ãƒ¼ã‚¿ã‚’ç”Ÿæˆ\n",
    "    synthesis_payload = {'speaker': speaker}\n",
    "    synthesis_response = requests.post(f'http://localhost:50021/synthesis', params=synthesis_payload, json=query)\n",
    "\n",
    "    if synthesis_response.status_code == 200:\n",
    "        # éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã¨ã—ã¦ä¿å­˜\n",
    "        filename = f\"output_{len(messages)}.wav\"\n",
    "        file_path = \"output/\" + filename\n",
    "        with open(file_path, 'wb') as f:\n",
    "            f.write(synthesis_response.content)\n",
    "        logging.debug(f\"éŸ³å£°ãŒ {filename} ã«ä¿å­˜ã•ã‚Œã¾ã—ãŸã€‚\")\n",
    "        print(f\"éŸ³å£°ãŒ {filename} ã«ä¿å­˜ã•ã‚Œã¾ã—ãŸã€‚\")\n",
    "        return filename\n",
    "    else:\n",
    "        logging.error(f\"Error in synthesis: {synthesis_response.text}\")\n",
    "        print(f\"Error in synthesis: {synthesis_response.text}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    socketio.run(app, debug=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ãã®ï¼‘ï¼ï¼šhtmlã‚’ãƒãƒ£ãƒƒãƒˆãƒ­ã‚°ãŒæ®‹ã‚‹ã‚ˆã†ã«ã™ã‚‹ï¼"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting voicecahtapp10.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile voicecahtapp10.py\n",
    "from flask import Flask, request, jsonify, send_from_directory, send_file\n",
    "from flask_cors import CORS\n",
    "import os\n",
    "import speech_recognition as sr\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "from flask_socketio import SocketIO, emit\n",
    "import threading\n",
    "import requests\n",
    "import json\n",
    "import time\n",
    "import logging\n",
    "\n",
    "#ã€€ç’°å¢ƒå¤‰æ•°ã®èª­ã¿è¾¼ã¿\n",
    "load_dotenv()\n",
    "\n",
    "# Flaskã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã®ä½œæˆ\n",
    "app = Flask(__name__, static_folder=\"static\")  \n",
    "\n",
    "# CORSã®è¨­å®š\n",
    "CORS(app)\n",
    "\n",
    "# Socket.IOã®è¨­å®š\n",
    "socketio = SocketIO(app)\n",
    "\n",
    "# ä¼šè©±ãƒ­ã‚°ã‚’ä¿æŒã™ã‚‹å¤‰æ•°\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}\n",
    "]\n",
    "\n",
    "#--------------------------------------------------\n",
    "#loggingã®è¨­å®š\n",
    "logging.basicConfig(\n",
    "    level=logging.DEBUG,\n",
    "    format=\"%(asctime)s [%(levelname)s] %(message)s\",\n",
    "    filename=\"app.log\",\n",
    "    encoding=\"utf-8\"\n",
    ")\n",
    "#--------------------------------------------------\n",
    "\n",
    "\n",
    "# ãƒ«ãƒ¼ãƒˆãƒ‘ã‚¹ã¸ã®ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚’å‡¦ç†ã™ã‚‹\n",
    "@app.route(\"/\")\n",
    "def index():\n",
    "    logging.info(\"index.html ã‚’è¿”ã—ã¾ã™ã€‚\")\n",
    "    return send_from_directory(\"static\", \"index10.html\")\n",
    "\n",
    "# /upload ã¸ã®ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚’å‡¦ç†ã™ã‚‹\n",
    "@app.route(\"/upload\", methods=[\"POST\"])\n",
    "def upload_audio():\n",
    "    logging.info(\"éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¾ã™ã€‚\")\n",
    "    if \"file\" not in request.files:\n",
    "        logging.error(\"No audio file provided\")\n",
    "        return jsonify({\"error\": \"No audio file provided\"}), 400\n",
    "    \n",
    "    audio_file = request.files[\"file\"]\n",
    "    audio_path = os.path.join(\"uploads\", f\"input_{len(messages)}.wav\")\n",
    "    audio_file.save(audio_path)\n",
    "    logging.info(f\"Saved audio file to {audio_path}\")\n",
    "\n",
    "    # éŸ³å£°èªè­˜\n",
    "    r = sr.Recognizer()\n",
    "    start_time = time.time()\n",
    "    with sr.AudioFile(audio_path) as source:\n",
    "        audio = r.record(source)\n",
    "        text = r.recognize_google(audio, language=\"ja-JP\")\n",
    "    logging.info(f\"éŸ³å£°èªè­˜çµæœ: {text}\")\n",
    "    logging.info(f\"éŸ³å£°èªè­˜æ™‚é–“: {time.time() - start_time} [sec]\")\n",
    "\n",
    "    # éŸ³å£°èªè­˜ã®çµæœã‚’æœ€åˆã«è¿”ã™\n",
    "    response = jsonify({\"text\": text})\n",
    "    \n",
    "    # åˆ¥ã‚¹ãƒ¬ãƒƒãƒ‰ã§AIã®å¿œç­”ã‚’å–å¾—\n",
    "    threading.Thread(target=get_ai_response, args=(text,)).start()\n",
    "    \n",
    "    return response\n",
    "\n",
    "# éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æä¾›ã™ã‚‹ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ\n",
    "@app.route(\"/audio/<filename>\")\n",
    "def get_audio(filename):\n",
    "    logging.info(f\"éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ« {filename} ã‚’è¿”ã—ã¾ã™ã€‚\")\n",
    "    return send_file(os.path.join(\"output\",filename))\n",
    "\n",
    "\n",
    "#--------------------------------------------------\n",
    "\n",
    "# AIã®å¿œç­”ã‚’å–å¾—ã™ã‚‹é–¢æ•° \n",
    "def get_ai_response(text):\n",
    "    \n",
    "    # ç¾åœ¨ã®æ™‚åˆ»å–å¾—\n",
    "    start = time.time()\n",
    "\n",
    "    # OpenAIã®APIã‚’å‘¼ã³å‡ºã—ã¦AIã®å¿œç­”ã‚’å–å¾—\n",
    "    client = OpenAI()\n",
    "    messages.append({\"role\": \"user\", \"content\": text})\n",
    "    completion = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=messages\n",
    "    )\n",
    "    ai_response = completion.choices[0].message.content\n",
    "    messages.append({\"role\": \"assistant\", \"content\": ai_response})\n",
    "    logging.info(f\"AIã®å¿œç­”: {ai_response}\")\n",
    "\n",
    "    # å‡¦ç†æ™‚é–“ã®è¨ˆç®—\n",
    "    ai_time = time.time() - start\n",
    "    logging.info(f\"AIãƒ¬ã‚¹ãƒãƒ³ã‚¹æ™‚é–“: {ai_time} [sec]\")\n",
    "\n",
    "    # éŸ³å£°åˆæˆ\n",
    "    filename = synthesize_voice(ai_response)\n",
    "\n",
    "    # å‡¦ç†æ™‚é–“ã®è¨ˆç®—\n",
    "    voice_time = time.time() - start - ai_time\n",
    "    logging.info(f\"éŸ³å£°åˆæˆæ™‚é–“: {voice_time} [sec]\")\n",
    "    \n",
    "    # WebSocketã‚’é€šã˜ã¦ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã«é€šçŸ¥\n",
    "    socketio.emit('ai_response', {'ai_response': ai_response, 'audio': filename})\n",
    "\n",
    "\n",
    "\n",
    "# éŸ³å£°åˆæˆã‚’è¡Œãªã†é–¢æ•°\n",
    "def synthesize_voice(text, speaker=1):\n",
    "    # 1. ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰éŸ³å£°åˆæˆã®ãŸã‚ã®ã‚¯ã‚¨ãƒªã‚’ä½œæˆ\n",
    "    query_payload = {'text': text, 'speaker': speaker}\n",
    "    query_response = requests.post(f'http://localhost:50021/audio_query', params=query_payload)\n",
    "\n",
    "    if query_response.status_code != 200:\n",
    "        logging.error(f\"Error in audio_query: {query_response.text}\")\n",
    "        print(f\"Error in audio_query: {query_response.text}\")\n",
    "        return\n",
    "\n",
    "    query = query_response.json()\n",
    "\n",
    "    # 2. ã‚¯ã‚¨ãƒªã‚’å…ƒã«éŸ³å£°ãƒ‡ãƒ¼ã‚¿ã‚’ç”Ÿæˆ\n",
    "    synthesis_payload = {'speaker': speaker}\n",
    "    synthesis_response = requests.post(f'http://localhost:50021/synthesis', params=synthesis_payload, json=query)\n",
    "\n",
    "    if synthesis_response.status_code == 200:\n",
    "        # éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã¨ã—ã¦ä¿å­˜\n",
    "        filename = f\"output_{len(messages)}.wav\"\n",
    "        file_path = \"output/\" + filename\n",
    "        with open(file_path, 'wb') as f:\n",
    "            f.write(synthesis_response.content)\n",
    "        logging.info(f\"éŸ³å£°ãŒ {filename} ã«ä¿å­˜ã•ã‚Œã¾ã—ãŸã€‚\")\n",
    "        print(f\"éŸ³å£°ãŒ {filename} ã«ä¿å­˜ã•ã‚Œã¾ã—ãŸã€‚\")\n",
    "        return filename\n",
    "    else:\n",
    "        logging.error(f\"Error in synthesis: {synthesis_response.text}\")\n",
    "        print(f\"Error in synthesis: {synthesis_response.text}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    logging.info(\"#####ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã‚’èµ·å‹•ã—ã¾ã™ã€‚#####\")\n",
    "    socketio.run(app, debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting static/index10.html\n"
     ]
    }
   ],
   "source": [
    "%%writefile static/index10.html\n",
    "\n",
    "<html lang=\"ja\">\n",
    "<head>\n",
    "    <meta charset=\"UTF-8\">\n",
    "    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n",
    "    <title>WAVéŒ²éŸ³ï¼†ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰</title>\n",
    "    <!-- Recorder.js ã‚’èª­ã¿è¾¼ã‚€ -->\n",
    "    <script src=\"https://cdn.jsdelivr.net/gh/mattdiamond/Recorderjs@master/dist/recorder.js\"></script>\n",
    "\n",
    "    <!-- Socket.IO ã‚’èª­ã¿è¾¼ã‚€ -->\n",
    "    <script src=\"https://cdn.socket.io/4.0.0/socket.io.min.js\"></script>\n",
    "\n",
    "    <!-- marked.js ã‚’èª­ã¿è¾¼ã‚€ -->\n",
    "    <script src=\"https://cdn.jsdelivr.net/npm/marked/marked.min.js\"></script>\n",
    "\n",
    "    <!-- cssã®é©ç”¨-->\n",
    "    <link rel=\"stylesheet\" href=\"/static/voicechatapp.css\">\n",
    "\n",
    "    \n",
    "</head>\n",
    "<body>\n",
    "    <h1>WAVéŒ²éŸ³ï¼†ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰</h1>\n",
    "    <button id=\"startRecording\">éŒ²éŸ³é–‹å§‹</button>\n",
    "    <button id=\"stopRecording\" disabled>éŒ²éŸ³åœæ­¢</button>\n",
    "    <div id=\"chatlog\"></div>\n",
    "\n",
    "    <script>\n",
    "        let audioContext;\n",
    "        let recorder;\n",
    "        let audioBlob;\n",
    "\n",
    "        document.addEventListener(\"DOMContentLoaded\", () => {\n",
    "            const socket = io();\n",
    "\n",
    "            socket.on('ai_response', (data) => {\n",
    "                const markdownText = data.ai_response;\n",
    "                const htmlContent = marked.parse(markdownText);\n",
    "                document.getElementById(\"chatlog\").innerHTML += `<div class=\"assistant\">${htmlContent}</div>`;\n",
    "                \n",
    "                // éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’è‡ªå‹•å†ç”Ÿã™ã‚‹å‡¦ç†\n",
    "                if (data.audio) {\n",
    "                    const audio = new Audio(`/audio/${data.audio}`);\n",
    "                    audio.play();\n",
    "                }\n",
    "                document.getElementById(\"startRecording\").disabled = false;\n",
    "                document.getElementById(\"stopRecording\").disabled = true;                    \n",
    "            });\n",
    "\n",
    "            // Spaceã‚­ãƒ¼ãŒæŠ¼ã•ã‚ŒãŸã¨ãã«startRecordingãƒœã‚¿ãƒ³ã‚’ã‚¯ãƒªãƒƒã‚¯\n",
    "            document.addEventListener(\"keydown\", (event) => {\n",
    "                if(document.getElementById(\"startRecording\").disabled){ \n",
    "                    console.log(\"å‡¦ç†ä¸­ã®ãŸã‚å…¥åŠ›ã¯ã§ãã¾ã›ã‚“\");\n",
    "                    return;\n",
    "                }\n",
    "                if (event.code === \"Space\" && !event.repeat) {\n",
    "                    document.getElementById(\"startRecording\").click();\n",
    "                }\n",
    "            });\n",
    "\n",
    "            // Spaceã‚­ãƒ¼ã‹ã‚‰æŒ‡ãŒé›¢ã•ã‚ŒãŸã¨ãã«stopRecordingãƒœã‚¿ãƒ³ã‚’ã‚¯ãƒªãƒƒã‚¯\n",
    "            document.addEventListener(\"keyup\", (event) => {\n",
    "                if(document.getElementById(\"stopRecording\").disabled){\n",
    "                    console.log(\"ä¸æ­£ãªéŒ²éŸ³åœæ­¢æ“ä½œã§ã™\");\n",
    "                    return;\n",
    "                }\n",
    "                if (event.code === \"Space\" && !event.repeat) {\n",
    "                    document.getElementById(\"stopRecording\").click();\n",
    "                }\n",
    "            });\n",
    "\n",
    "        });        \n",
    "\n",
    "        document.getElementById(\"startRecording\").addEventListener(\"click\", async () => {\n",
    "            const stream = await navigator.mediaDevices.getUserMedia({ audio: true });\n",
    "            audioContext = new AudioContext();\n",
    "            const source = audioContext.createMediaStreamSource(stream);\n",
    "            recorder = new Recorder(source, { numChannels: 1 }); // ãƒ¢ãƒãƒ©ãƒ«éŒ²éŸ³\n",
    "            recorder.record();\n",
    "\n",
    "            document.getElementById(\"startRecording\").disabled = true;\n",
    "            document.getElementById(\"stopRecording\").disabled = false;\n",
    "        });\n",
    "\n",
    "        document.getElementById(\"stopRecording\").addEventListener(\"click\", () => {\n",
    "            recorder.stop();\n",
    "            recorder.exportWAV((blob) => {\n",
    "                audioBlob = blob;\n",
    "\n",
    "                if (!audioBlob) {\n",
    "                    console.error(\"No audio to upload\");    \n",
    "                    return;\n",
    "                }\n",
    "\n",
    "                const formData = new FormData();\n",
    "                formData.append(\"file\", audioBlob, \"recorded_audio.wav\");\n",
    "\n",
    "                fetch(\"/upload\", {\n",
    "                    method: \"POST\",\n",
    "                    body: formData,\n",
    "                })\n",
    "                .then((response) => response.json())\n",
    "                .then((data) => {\n",
    "                    if(data.text){\n",
    "                         document.getElementById(\"chatlog\").innerHTML += `<div class=\"user\">${marked.parse(data.text)}</div>`;\n",
    "                    }\n",
    "                    else console.log(\"Error: éŸ³å£°ã‚’èªè­˜ã§ãã¾ã›ã‚“ã§ã—ãŸã€‚\");\n",
    "                })\n",
    "                .catch((error) => {\n",
    "                    console.error(\"Upload failed:\");\n",
    "                });\n",
    "            });\n",
    "\n",
    "            document.getElementById(\"startRecording\").disabled = true;\n",
    "            document.getElementById(\"stopRecording\").disabled = true;\n",
    "        });\n",
    "\n",
    "\n",
    "    </script>\n",
    "</body>\n",
    "</html>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "æ¦‚ã­ä¸Šæ‰‹ãè¡Œãã‚ˆã†ã«ã§ããŸï¼\n",
    "å¾Œã®å•é¡Œã¯ï¼ŒAIã‹ã‚‰ã®ãƒ¬ã‚¹ãƒãƒ³ã‚¹ãŒãƒãƒ¼ã‚¯ãƒ€ã‚¦ãƒ³å½¢å¼ã«ãªã£ã¦ã„ã‚‹ã®ã‚’ã†ã¾ãè¡¨è¨˜ã™ã‚‹ã“ã¨ã‹ãªï¼\n",
    "\n",
    "-> å¯¾å¿œã—ãŸã€€marked.jsãªã‚“ã¦ã®ãŒã‚ã‚‹ã‚“ã‚„ï¼ã“ã‚Œä½¿ãˆã°ç°¡å˜ï¼"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://zenn.dev/zenn24yykiitos/articles/f3e983fe650e08"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ã“ã‚Œã‚’å‚è€ƒã«ï¼Œãƒ•ãƒ­ãƒ³ãƒˆã‚¨ãƒ³ãƒ‰ã‹ã‚‰ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼ã‚’é¸ã¹ã‚‹ã‚ˆã†ã«ã™ã‚‹ï¼"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
