{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Flask を使った簡易Webアプリ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "まずは必要なライブラリをインストール"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting flask (from -r requirements.txt (line 1))\n",
      "  Using cached flask-3.1.0-py3-none-any.whl.metadata (2.7 kB)\n",
      "Collecting flask_cors (from -r requirements.txt (line 2))\n",
      "  Using cached flask_cors-5.0.1-py3-none-any.whl.metadata (961 bytes)\n",
      "Collecting speechrecognition (from -r requirements.txt (line 3))\n",
      "  Using cached SpeechRecognition-3.14.1-py3-none-any.whl.metadata (31 kB)\n",
      "Collecting openai (from -r requirements.txt (line 4))\n",
      "  Using cached openai-1.65.2-py3-none-any.whl.metadata (27 kB)\n",
      "Collecting python-dotenv (from -r requirements.txt (line 5))\n",
      "  Using cached python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\n",
      "Collecting flask-socketio (from -r requirements.txt (line 6))\n",
      "  Using cached Flask_SocketIO-5.5.1-py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting requests (from -r requirements.txt (line 7))\n",
      "  Using cached requests-2.32.3-py3-none-any.whl.metadata (4.6 kB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not find a version that satisfies the requirement re (from versions: none)\n",
      "ERROR: No matching distribution found for re\n"
     ]
    }
   ],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## その１　とりあえず音声ファイルをアップロードできるようにする．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting static/index.html\n"
     ]
    }
   ],
   "source": [
    "%%writefile static/index.html\n",
    "\n",
    "<!DOCTYPE html>\n",
    "<html lang=\"ja\">\n",
    "  <head>\n",
    "    <meta charset=\"UTF-8\" />\n",
    "    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\" />\n",
    "    <title>Voice Chat App</title>\n",
    "  </head>\n",
    "  <body>\n",
    "    <h1>Voice Chat App</h1>\n",
    "    <button id=\"start\">開始</button>\n",
    "    <button id=\"stop\" disabled>停止</button>\n",
    "    <p><strong>文字起こし:</strong> <span id=\"transcription\"></span></p>\n",
    "    <p><strong>AIの応答:</strong> <span id=\"aiResponse\"></span></p>\n",
    "\n",
    "    <script>\n",
    "      document.addEventListener(\"DOMContentLoaded\", () => {\n",
    "        const startButton = document.getElementById(\"start\");\n",
    "        const stopButton = document.getElementById(\"stop\");\n",
    "        const transcriptionElement = document.getElementById(\"transcription\");\n",
    "        const aiResponseElement = document.getElementById(\"aiResponse\");\n",
    "        let mediaRecorder;\n",
    "        let audioChunks = [];\n",
    "\n",
    "        startButton.addEventListener(\"click\", async () => {\n",
    "          const stream = await navigator.mediaDevices.getUserMedia({\n",
    "            audio: true,\n",
    "          });\n",
    "          mediaRecorder = new MediaRecorder(stream);\n",
    "\n",
    "          mediaRecorder.ondataavailable = (event) => {\n",
    "            audioChunks.push(event.data);\n",
    "          };\n",
    "\n",
    "          mediaRecorder.onstop = async () => {\n",
    "            const audioBlob = new Blob(audioChunks, { type: \"audio/webm\" });\n",
    "            audioChunks = [];\n",
    "\n",
    "            const formData = new FormData();\n",
    "            formData.append(\"audio\", audioBlob, \"recording.webm\");\n",
    "\n",
    "            fetch(\"/upload\", {\n",
    "              method: \"POST\",\n",
    "              body: formData,\n",
    "            })\n",
    "              .then((response) => response.json())\n",
    "              .then((data) => {\n",
    "                transcriptionElement.textContent =\n",
    "                  data.text || \"認識できませんでした。\";\n",
    "                aiResponseElement.textContent =\n",
    "                  data.ai_response || \"AIの応答なし。\";\n",
    "              })\n",
    "              .catch((error) => {\n",
    "                console.error(\"Upload failed:\", error);\n",
    "                transcriptionElement.textContent = \"エラーが発生しました。\";\n",
    "                aiResponseElement.textContent = \"\";\n",
    "              });\n",
    "          };\n",
    "\n",
    "          mediaRecorder.start();\n",
    "          startButton.disabled = true;\n",
    "          stopButton.disabled = false;\n",
    "        });\n",
    "\n",
    "        stopButton.addEventListener(\"click\", () => {\n",
    "          mediaRecorder.stop();\n",
    "          startButton.disabled = false;\n",
    "          stopButton.disabled = true;\n",
    "        });\n",
    "      });\n",
    "    </script>\n",
    "  </body>\n",
    "</html>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting app01.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile app01.py\n",
    "\n",
    "from flask import Flask, request, jsonify, send_from_directory\n",
    "import os\n",
    "\n",
    "app = Flask(__name__, static_folder=\"static\")  \n",
    "\n",
    "@app.route(\"/\")\n",
    "def index():\n",
    "    return send_from_directory(\"static\", \"index.html\")\n",
    "\n",
    "@app.route(\"/upload\", methods=[\"POST\"])\n",
    "def upload_audio():\n",
    "    if \"audio\" not in request.files:\n",
    "        return jsonify({\"error\": \"No audio file provided\"}), 400\n",
    "    \n",
    "    audio_file = request.files[\"audio\"]\n",
    "    audio_path = os.path.join(\"uploads\", audio_file.filename)\n",
    "    audio_file.save(audio_path)\n",
    "    \n",
    "    text = \"test\"\n",
    "    ai_response = \"test\"\n",
    "    return jsonify({\"text\": text, \"ai_response\": ai_response})\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    app.run(debug=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ふむ．とりあえず音声ファイル.webmはアップロードできるようになった．\n",
    "ただspeech_recgnitionではwebmは受け入れないので，wavファイルに変える必要がある"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## その２　アップロードした音声ファイルをWavファイルに変換"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing app02.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile app02.py\n",
    "\n",
    "from flask import Flask, request, jsonify, render_template\n",
    "import os\n",
    "import subprocess\n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "@app.route(\"/\")\n",
    "def index():\n",
    "    return render_template(\"index.html\")  # フロントエンドのHTMLを表示\n",
    "\n",
    "@app.route(\"/upload\", methods=[\"POST\"])\n",
    "def upload_audio():\n",
    "    if \"audio\" not in request.files:\n",
    "        return jsonify({\"error\": \"No audio file provided\"}), 400\n",
    "    \n",
    "    audio_file = request.files[\"audio\"]\n",
    "    audio_path = os.path.join(\"uploads\", audio_file.filename)\n",
    "    audio_file.save(audio_path)\n",
    "    convert_webm_to_wav(audio_path, \"uploads/output.wav\")\n",
    "    \n",
    "    text = \"test\"\n",
    "    ai_response = \"test\"\n",
    "    return jsonify({\"text\": text, \"ai_response\": ai_response})\n",
    "\n",
    "\n",
    "def convert_webm_to_wav(input_path, output_path):\n",
    "    command = [\n",
    "        \"ffmpeg\",\n",
    "        \"-i\", input_path,  # 入力ファイル\n",
    "        \"-ar\", \"16000\",  # サンプリングレート 16kHz\n",
    "        \"-ac\", \"1\",  # モノラル変換\n",
    "        \"-preset\", \"ultrafast\",  # 速度最優先\n",
    "        output_path\n",
    "    ]\n",
    "    subprocess.run(command, check=True)\n",
    "\n",
    "# 使い方\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    app.run(debug=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "変換に少々時間取られるな．．．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## その３　アップロードを直接Wavファイルにする．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "変換に少々時間がかかるのが気になるので，アップロードの段階で直接Wavファイルをアップできないか探ってみたら，Record.jsなるものがあるようだ．\n",
    "https://github.com/mattdiamond/Recorderjs\n",
    "Recorder.jsをダウンロードして\n",
    "これをhtmlに組み込んでみる．\n",
    "\n",
    "けど，最初やってみたら，recorder.jsでエラーでた．\n",
    "CDNがあるようなので，そちらでやったらうまく行った．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting static/index.html\n"
     ]
    }
   ],
   "source": [
    "%%writefile static/index.html\n",
    "\n",
    "<!DOCTYPE html>\n",
    "<html lang=\"ja\">\n",
    "<head>\n",
    "    <meta charset=\"UTF-8\">\n",
    "    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n",
    "    <title>WAV録音＆アップロード</title>\n",
    "    <!-- Recorder.js を読み込む -->\n",
    "    <script src=\"https://cdn.jsdelivr.net/gh/mattdiamond/Recorderjs@master/dist/recorder.js\"></script>\n",
    "    <!-- <script src=\"recorder.js\"></script> --> \n",
    "    \n",
    "</head>\n",
    "<body>\n",
    "    <h1>WAV録音＆アップロード</h1>\n",
    "    <button id=\"startRecording\">録音開始</button>\n",
    "    <button id=\"stopRecording\" disabled>録音停止</button>\n",
    "    <!-- <audio id=\"audioPlayback\" controls></audio> -->\n",
    "    <!-- <button id=\"uploadAudio\" disabled>アップロード</button> -->\n",
    "    <p><strong>文字起こし:</strong> <span id=\"transcription\"></span></p>\n",
    "    <p><strong>AIの応答:</strong> <span id=\"aiResponse\"></span></p>\n",
    "\n",
    "    <script>\n",
    "        let audioContext;\n",
    "        let recorder;\n",
    "        let audioBlob;\n",
    "                \n",
    "\n",
    "        document.getElementById(\"startRecording\").addEventListener(\"click\", async () => {\n",
    "            const stream = await navigator.mediaDevices.getUserMedia({ audio: true });\n",
    "            audioContext = new AudioContext();\n",
    "            const source = audioContext.createMediaStreamSource(stream);\n",
    "            recorder = new Recorder(source, { numChannels: 1 }); // モノラル録音\n",
    "            recorder.record();\n",
    "\n",
    "            document.getElementById(\"startRecording\").disabled = true;\n",
    "            document.getElementById(\"stopRecording\").disabled = false;\n",
    "        });\n",
    "\n",
    "        document.getElementById(\"stopRecording\").addEventListener(\"click\", () => {\n",
    "            recorder.stop();\n",
    "            recorder.exportWAV((blob) => {\n",
    "                audioBlob = blob;\n",
    "\n",
    "                if (!audioBlob) {\n",
    "                    console.error(\"No audio to upload\");    \n",
    "                    return;\n",
    "                }\n",
    "\n",
    "                const formData = new FormData();\n",
    "                formData.append(\"file\", audioBlob, \"recorded_audio.wav\");\n",
    "\n",
    "                fetch(\"/upload\", {\n",
    "                    method: \"POST\",\n",
    "                    body: formData,\n",
    "                })\n",
    "                .then((response) => response.json())\n",
    "                .then((data) => {\n",
    "                    document.getElementById(\"transcription\").textContent =\n",
    "                    data.text || \"認識できませんでした。\";\n",
    "                    document.getElementById(\"aiResponse\").textContent =\n",
    "                    data.ai_response || \"AIの応答なし。\";\n",
    "                })\n",
    "                .catch((error) => {\n",
    "                    console.error(\"Upload failed:\");\n",
    "                    document.getElementById(\"transcription\").textContent = \"エラーが発生しました。\";\n",
    "                    document.getElementById(\"aiResponse\").textContent = \"\";\n",
    "                });\n",
    "            });\n",
    "\n",
    "            document.getElementById(\"startRecording\").disabled = false;\n",
    "            document.getElementById(\"stopRecording\").disabled = true;\n",
    "\n",
    "\n",
    "        });\n",
    "    </script>\n",
    "</body>\n",
    "</html>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "app01.pyで実行．\n",
    "CORSの問題でアップロードで弾かれているようだ．．．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "flask_corsを使って，サーバの側でCORS問題を無視するように設定する．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting flask_cors\n",
      "  Downloading flask_cors-5.0.1-py3-none-any.whl.metadata (961 bytes)\n",
      "Requirement already satisfied: flask>=0.9 in /Users/fujinohidenori/dev/project/venv/lib/python3.13/site-packages (from flask_cors) (3.1.0)\n",
      "Requirement already satisfied: Werkzeug>=0.7 in /Users/fujinohidenori/dev/project/venv/lib/python3.13/site-packages (from flask_cors) (3.1.3)\n",
      "Requirement already satisfied: Jinja2>=3.1.2 in /Users/fujinohidenori/dev/project/venv/lib/python3.13/site-packages (from flask>=0.9->flask_cors) (3.1.5)\n",
      "Requirement already satisfied: itsdangerous>=2.2 in /Users/fujinohidenori/dev/project/venv/lib/python3.13/site-packages (from flask>=0.9->flask_cors) (2.2.0)\n",
      "Requirement already satisfied: click>=8.1.3 in /Users/fujinohidenori/dev/project/venv/lib/python3.13/site-packages (from flask>=0.9->flask_cors) (8.1.8)\n",
      "Requirement already satisfied: blinker>=1.9 in /Users/fujinohidenori/dev/project/venv/lib/python3.13/site-packages (from flask>=0.9->flask_cors) (1.9.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /Users/fujinohidenori/dev/project/venv/lib/python3.13/site-packages (from Werkzeug>=0.7->flask_cors) (3.0.2)\n",
      "Downloading flask_cors-5.0.1-py3-none-any.whl (11 kB)\n",
      "Installing collected packages: flask_cors\n",
      "Successfully installed flask_cors-5.0.1\n"
     ]
    }
   ],
   "source": [
    "!pip install flask_cors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting app03.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile app03.py\n",
    "from flask import Flask, request, jsonify, send_from_directory\n",
    "from flask_cors import CORS\n",
    "import os\n",
    "\n",
    "app = Flask(__name__, static_folder=\"static\")  \n",
    "CORS(app)\n",
    "\n",
    "@app.route(\"/\")\n",
    "def index():\n",
    "    return send_from_directory(\"static\", \"index.html\")\n",
    "\n",
    "@app.route(\"/upload\", methods=[\"POST\"])\n",
    "def upload_audio():\n",
    "    if \"file\" not in request.files:\n",
    "        return jsonify({\"error\": \"No audio file provided\"}), 400\n",
    "    \n",
    "    audio_file = request.files[\"file\"]\n",
    "    audio_path = os.path.join(\"uploads\", audio_file.filename)\n",
    "    audio_file.save(audio_path)\n",
    "    \n",
    "    text = \"test\"\n",
    "    ai_response = \"test\"\n",
    "    return jsonify({\"text\": text, \"ai_response\": ai_response})\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    app.run(debug=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "よし，とりあえず問題は解決した．\n",
    "ハマった理由は，fetchのインタフェースをフォルダ名と勘違いしていたこと．つまり，app.routeでは/uploadとしているのに，javascriptの方で/uploadsとしていた．これにより当然ながらインタフェースがないわけで４０４エラーが返されれるということになっていた．分れば馬鹿馬鹿しい勘違いやった😂\n",
    "\n",
    "あと，デバッグ環境ではルートディレクトリがprojectになるというところもハマった😂"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## その４　Speech Recognitionにかける\n",
    "よし，ここからはpythonの側の処理に集中"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting app04.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile app04.py\n",
    "from flask import Flask, request, jsonify, send_from_directory\n",
    "from flask_cors import CORS\n",
    "import os\n",
    "import speech_recognition as sr\n",
    "\n",
    "app = Flask(__name__, static_folder=\"static\")  \n",
    "CORS(app)\n",
    "\n",
    "@app.route(\"/\")\n",
    "def index():\n",
    "    return send_from_directory(\"static\", \"index.html\")\n",
    "\n",
    "@app.route(\"/upload\", methods=[\"POST\"])\n",
    "def upload_audio():\n",
    "    if \"file\" not in request.files:\n",
    "        return jsonify({\"error\": \"No audio file provided\"}), 400\n",
    "    \n",
    "    audio_file = request.files[\"file\"]\n",
    "    audio_path = os.path.join(\"uploads\", audio_file.filename)\n",
    "    audio_file.save(audio_path)\n",
    "\n",
    "    # 音声認識\n",
    "    r = sr.Recognizer()\n",
    "    with sr.AudioFile(audio_path) as source:\n",
    "        audio = r.record(source)\n",
    "        text = r.recognize_google(audio, language=\"ja-JP\")\n",
    "        if __debug__: # デバッグモードの場合\n",
    "            print(text)\n",
    "            \n",
    "        ai_response = \"test\"\n",
    "        return jsonify({\"text\": text, \"ai_response\": ai_response})\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    app.run(debug=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ふむ．これでとりあえず，音声認識結果を返せるようになった．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## その５　 openai の　Chat＿Compelationを使う"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing app05.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile app05.py\n",
    "from flask import Flask, request, jsonify, send_from_directory\n",
    "from flask_cors import CORS\n",
    "import os\n",
    "import speech_recognition as sr\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "app = Flask(__name__, static_folder=\"static\")  \n",
    "CORS(app)\n",
    "\n",
    "@app.route(\"/\")\n",
    "def index():\n",
    "    return send_from_directory(\"static\", \"index.html\")\n",
    "\n",
    "@app.route(\"/upload\", methods=[\"POST\"])\n",
    "def upload_audio():\n",
    "    if \"file\" not in request.files:\n",
    "        return jsonify({\"error\": \"No audio file provided\"}), 400\n",
    "    \n",
    "    audio_file = request.files[\"file\"]\n",
    "    audio_path = os.path.join(\"uploads\", audio_file.filename)\n",
    "    audio_file.save(audio_path)\n",
    "\n",
    "    # 音声認識\n",
    "    r = sr.Recognizer()\n",
    "    with sr.AudioFile(audio_path) as source:\n",
    "        audio = r.record(source)\n",
    "        text = r.recognize_google(audio, language=\"ja-JP\")\n",
    "        if __debug__: # デバッグモードの場合\n",
    "            print(text)\n",
    "\n",
    "        # AIの応答\n",
    "        client = OpenAI()\n",
    "        completion = client.chat.completions.create(\n",
    "            model=\"gpt-4o-mini\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "                {\"role\": \"user\", \"content\": text},\n",
    "            ]\n",
    "        )\n",
    "        ai_response = completion.choices[0].message.content\n",
    "        return jsonify({\"text\": text, \"ai_response\": ai_response})\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    app.run(debug=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ふむ．とりあえずは単発会話はできるようになった．\n",
    "現時点の違和感・修正したい点は\n",
    "現状だと，入力テキストとレスポンスが同時に帰ってきてしまう．\n",
    "どうにか，その部分をいじれないか．レスポンスをまたつに先に入力テキストを返して，画面に表示させておいて，レスポンスが帰ってきたら，改めてそれを返すという感じ．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## その6　入力テキストとレスポンスを分けて表記できるようにする\n",
    "やるとしたら，postを2回に分ける形かな？\n",
    "Copilotに聞いてみたらWeb Socketを使えばできるとな．．．\n",
    "とりあえずやってみるか．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install flask-socketio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting app06.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile app06.py\n",
    "from flask import Flask, request, jsonify, send_from_directory\n",
    "from flask_cors import CORS\n",
    "import os\n",
    "import speech_recognition as sr\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "from flask_socketio import SocketIO, emit\n",
    "import threading\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "app = Flask(__name__, static_folder=\"static\")  \n",
    "CORS(app)\n",
    "socketio = SocketIO(app)\n",
    "\n",
    "@app.route(\"/\")\n",
    "def index():\n",
    "    return send_from_directory(\"static\", \"index06.html\")\n",
    "\n",
    "@app.route(\"/upload\", methods=[\"POST\"])\n",
    "def upload_audio():\n",
    "    if \"file\" not in request.files:\n",
    "        return jsonify({\"error\": \"No audio file provided\"}), 400\n",
    "    \n",
    "    audio_file = request.files[\"file\"]\n",
    "    audio_path = os.path.join(\"uploads\", audio_file.filename)\n",
    "    audio_file.save(audio_path)\n",
    "\n",
    "    # 音声認識\n",
    "    r = sr.Recognizer()\n",
    "    with sr.AudioFile(audio_path) as source:\n",
    "        audio = r.record(source)\n",
    "        text = r.recognize_google(audio, language=\"ja-JP\")\n",
    "        if __debug__: # デバッグモードの場合\n",
    "            print(text)\n",
    "\n",
    "    # 音声認識の結果を最初に返す\n",
    "    response = jsonify({\"text\": text})\n",
    "    \n",
    "    # 別スレッドでAIの応答を取得\n",
    "    threading.Thread(target=get_ai_response, args=(text,)).start()\n",
    "    \n",
    "    return response\n",
    "\n",
    "def get_ai_response(text):\n",
    "    client = OpenAI()\n",
    "    completion = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "            {\"role\": \"user\", \"content\": text},\n",
    "        ]\n",
    "    )\n",
    "    ai_response = completion.choices[0].message.content\n",
    "    # WebSocketを通じてクライアントに通知\n",
    "    socketio.emit('ai_response', {'ai_response': ai_response})\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    socketio.run(app, debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting static/index06.html\n"
     ]
    }
   ],
   "source": [
    "%%writefile static/index06.html\n",
    "\n",
    "<!DOCTYPE html>\n",
    "<html lang=\"ja\">\n",
    "<head>\n",
    "    <meta charset=\"UTF-8\">\n",
    "    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n",
    "    <title>WAV録音＆アップロード</title>\n",
    "    <!-- Recorder.js を読み込む -->\n",
    "    <script src=\"https://cdn.jsdelivr.net/gh/mattdiamond/Recorderjs@master/dist/recorder.js\"></script>\n",
    "\n",
    "    <!-- Socket.IO を読み込む -->\n",
    "    <script src=\"https://cdn.socket.io/4.0.0/socket.io.min.js\"></script>\n",
    "\n",
    "    \n",
    "</head>\n",
    "<body>\n",
    "    <h1>WAV録音＆アップロード</h1>\n",
    "    <button id=\"startRecording\">録音開始</button>\n",
    "    <button id=\"stopRecording\" disabled>録音停止</button>\n",
    "    <!-- <audio id=\"audioPlayback\" controls></audio> -->\n",
    "    <!-- <button id=\"uploadAudio\" disabled>アップロード</button> -->\n",
    "    <p><strong>文字起こし:</strong> <span id=\"transcription\"></span></p>\n",
    "    <p><strong>AIの応答:</strong> <span id=\"aiResponse\"></span></p>\n",
    "\n",
    "    <script>\n",
    "        let audioContext;\n",
    "        let recorder;\n",
    "        let audioBlob;\n",
    "\n",
    "        document.addEventListener(\"DOMContentLoaded\", () => {\n",
    "            const socket = io();\n",
    "\n",
    "            socket.on('ai_response', (data) => {\n",
    "                document.getElementById(\"aiResponse\").textContent = data.ai_response;\n",
    "            });\n",
    "        });        \n",
    "\n",
    "        document.getElementById(\"startRecording\").addEventListener(\"click\", async () => {\n",
    "            const stream = await navigator.mediaDevices.getUserMedia({ audio: true });\n",
    "            audioContext = new AudioContext();\n",
    "            const source = audioContext.createMediaStreamSource(stream);\n",
    "            recorder = new Recorder(source, { numChannels: 1 }); // モノラル録音\n",
    "            recorder.record();\n",
    "\n",
    "            document.getElementById(\"startRecording\").disabled = true;\n",
    "            document.getElementById(\"stopRecording\").disabled = false;\n",
    "        });\n",
    "\n",
    "        document.getElementById(\"stopRecording\").addEventListener(\"click\", () => {\n",
    "            recorder.stop();\n",
    "            recorder.exportWAV((blob) => {\n",
    "                audioBlob = blob;\n",
    "\n",
    "                if (!audioBlob) {\n",
    "                    console.error(\"No audio to upload\");    \n",
    "                    return;\n",
    "                }\n",
    "\n",
    "                const formData = new FormData();\n",
    "                formData.append(\"file\", audioBlob, \"recorded_audio.wav\");\n",
    "\n",
    "                fetch(\"/upload\", {\n",
    "                    method: \"POST\",\n",
    "                    body: formData,\n",
    "                })\n",
    "                .then((response) => response.json())\n",
    "                .then((data) => {\n",
    "                    document.getElementById(\"transcription\").textContent =\n",
    "                    data.text || \"認識できませんでした。\";\n",
    "                })\n",
    "                .catch((error) => {\n",
    "                    console.error(\"Upload failed:\");\n",
    "                    document.getElementById(\"transcription\").textContent = \"エラーが発生しました。\";\n",
    "                    document.getElementById(\"aiResponse\").textContent = \"\";\n",
    "                });\n",
    "            });\n",
    "\n",
    "            document.getElementById(\"startRecording\").disabled = false;\n",
    "            document.getElementById(\"stopRecording\").disabled = true;\n",
    "\n",
    "\n",
    "        });\n",
    "    </script>\n",
    "</body>\n",
    "</html>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ふむ．入力とレスポンスを別々に表記できるようになった．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## その７　継続的な会話を出来るようにする．\n",
    "とりあえず，フロントエンドでログを記載する部分については別に考えて，まずはパイソンで動かす"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing app07.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile app07.py\n",
    "from flask import Flask, request, jsonify, send_from_directory\n",
    "from flask_cors import CORS\n",
    "import os\n",
    "import speech_recognition as sr\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "from flask_socketio import SocketIO, emit\n",
    "import threading\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "app = Flask(__name__, static_folder=\"static\")  \n",
    "CORS(app)\n",
    "socketio = SocketIO(app)\n",
    "\n",
    "# 会話ログを保持する変数\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}\n",
    "]\n",
    "\n",
    "@app.route(\"/\")\n",
    "def index():\n",
    "    return send_from_directory(\"static\", \"index06.html\")\n",
    "\n",
    "@app.route(\"/upload\", methods=[\"POST\"])\n",
    "def upload_audio():\n",
    "    if \"file\" not in request.files:\n",
    "        return jsonify({\"error\": \"No audio file provided\"}), 400\n",
    "    \n",
    "    audio_file = request.files[\"file\"]\n",
    "    audio_path = os.path.join(\"uploads\", audio_file.filename)\n",
    "    audio_file.save(audio_path)\n",
    "\n",
    "    # 音声認識\n",
    "    r = sr.Recognizer()\n",
    "    with sr.AudioFile(audio_path) as source:\n",
    "        audio = r.record(source)\n",
    "        text = r.recognize_google(audio, language=\"ja-JP\")\n",
    "        if __debug__: # デバッグモードの場合\n",
    "            print(text)\n",
    "\n",
    "    # 音声認識の結果を最初に返す\n",
    "    response = jsonify({\"text\": text})\n",
    "    \n",
    "    # 別スレッドでAIの応答を取得\n",
    "    threading.Thread(target=get_ai_response, args=(text,)).start()\n",
    "    \n",
    "    return response\n",
    "\n",
    "def get_ai_response(text):\n",
    "    client = OpenAI()\n",
    "    messages.append({\"role\": \"user\", \"content\": text})\n",
    "    completion = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=messages\n",
    "    )\n",
    "    ai_response = completion.choices[0].message.content\n",
    "    messages.append({\"role\": \"assistant\", \"content\": ai_response})\n",
    "    # WebSocketを通じてクライアントに通知\n",
    "    socketio.emit('ai_response', {'ai_response': ai_response})\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    socketio.run(app, debug=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ふむ．継続性のある会話もできるようになった．\n",
    "じゃあ次は，TTSを組み込みたいね．\n",
    "\n",
    "調べたらVoice VoxであればAPIが使えるとのこと．ただ，このAPIはあくまでローカルサーバで動くものになっている．\n",
    "ローカルでどれくらい動作時間かかるんやろ？？\n",
    "とりあえず試すか・・・．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## その8　音声合成機能を使ってみる\n",
    "VoiceVox GUIを立ち上げておく．これによりローカルにVoiceVoxが立ち上がりAPIが使える．\n",
    "そのまま関数を作ってくれている人がいたので拝借\n",
    "\n",
    "https://zenn.dev/zenn24yykiitos/articles/fff3c954ddf42c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in c:\\work\\vscode\\aichatbot\\.venv\\lib\\site-packages (2.32.3)"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\work\\vscode\\aichatbot\\.venv\\lib\\site-packages (from requests) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\work\\vscode\\aichatbot\\.venv\\lib\\site-packages (from requests) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\work\\vscode\\aichatbot\\.venv\\lib\\site-packages (from requests) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\work\\vscode\\aichatbot\\.venv\\lib\\site-packages (from requests) (2025.1.31)\n"
     ]
    }
   ],
   "source": [
    "!pip install requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing app08.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile app08.py\n",
    "from flask import Flask, request, jsonify, send_from_directory, send_file\n",
    "from flask_cors import CORS\n",
    "import os\n",
    "import speech_recognition as sr\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "from flask_socketio import SocketIO, emit\n",
    "import threading\n",
    "import requests\n",
    "import json\n",
    "import time\n",
    "\n",
    "#　環境変数の読み込み\n",
    "load_dotenv()\n",
    "\n",
    "# Flaskアプリケーションの作成\n",
    "app = Flask(__name__, static_folder=\"static\")  \n",
    "\n",
    "# CORSの設定\n",
    "CORS(app)\n",
    "\n",
    "# Socket.IOの設定\n",
    "socketio = SocketIO(app)\n",
    "\n",
    "# 会話ログを保持する変数\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}\n",
    "]\n",
    "\n",
    "#--------------------------------------------------\n",
    "\n",
    "# ルートパスへのリクエストを処理する\n",
    "@app.route(\"/\")\n",
    "def index():\n",
    "    return send_from_directory(\"static\", \"index08.html\")\n",
    "\n",
    "# /upload へのリクエストを処理する\n",
    "@app.route(\"/upload\", methods=[\"POST\"])\n",
    "def upload_audio():\n",
    "    if \"file\" not in request.files:\n",
    "        return jsonify({\"error\": \"No audio file provided\"}), 400\n",
    "    \n",
    "    audio_file = request.files[\"file\"]\n",
    "    audio_path = os.path.join(\"uploads\", audio_file.filename)\n",
    "    audio_file.save(audio_path)\n",
    "\n",
    "    # 音声認識\n",
    "    r = sr.Recognizer()\n",
    "    with sr.AudioFile(audio_path) as source:\n",
    "        audio = r.record(source)\n",
    "        text = r.recognize_google(audio, language=\"ja-JP\")\n",
    "        if __debug__: # デバッグモードの場合\n",
    "            print(text)\n",
    "\n",
    "    # 音声認識の結果を最初に返す\n",
    "    response = jsonify({\"text\": text})\n",
    "    \n",
    "    # 別スレッドでAIの応答を取得\n",
    "    threading.Thread(target=get_ai_response, args=(text,)).start()\n",
    "    \n",
    "    return response\n",
    "\n",
    "# 音声ファイルを提供するエンドポイント\n",
    "@app.route(\"/audio/<filename>\")\n",
    "def get_audio(filename):\n",
    "    return send_file(os.path.join(\"uploads\",filename))\n",
    "\n",
    "\n",
    "#--------------------------------------------------\n",
    "\n",
    "# AIの応答を取得する関数 \n",
    "def get_ai_response(text):\n",
    "    \n",
    "    # 現在の時刻取得\n",
    "    start = time.time()\n",
    "\n",
    "    # OpenAIのAPIを呼び出してAIの応答を取得\n",
    "    client = OpenAI()\n",
    "    messages.append({\"role\": \"user\", \"content\": text})\n",
    "    completion = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=messages\n",
    "    )\n",
    "    ai_response = completion.choices[0].message.content\n",
    "    messages.append({\"role\": \"assistant\", \"content\": ai_response})\n",
    "\n",
    "    # 処理時間の計算\n",
    "    ai_time = time.time() - start\n",
    "    print(f\"処理時間: {ai_time} [sec]\") \n",
    "\n",
    "    # 音声合成\n",
    "    filename = synthesize_voice(ai_response)\n",
    "\n",
    "    # 処理時間の計算\n",
    "    voice_time = time.time() - start - ai_time\n",
    "    print(f\"音声合成時間: {voice_time} [sec]\")\n",
    "    \n",
    "    # WebSocketを通じてクライアントに通知\n",
    "    socketio.emit('ai_response', {'ai_response': ai_response, 'audio': filename})\n",
    "\n",
    "\n",
    "\n",
    "# 音声合成を行なう関数\n",
    "def synthesize_voice(text, speaker=1, filename=\"uploads/output.wav\"):\n",
    "    # 1. テキストから音声合成のためのクエリを作成\n",
    "    query_payload = {'text': text, 'speaker': speaker}\n",
    "    query_response = requests.post(f'http://localhost:50021/audio_query', params=query_payload)\n",
    "\n",
    "    if query_response.status_code != 200:\n",
    "        print(f\"Error in audio_query: {query_response.text}\")\n",
    "        return\n",
    "\n",
    "    query = query_response.json()\n",
    "\n",
    "    # 2. クエリを元に音声データを生成\n",
    "    synthesis_payload = {'speaker': speaker}\n",
    "    synthesis_response = requests.post(f'http://localhost:50021/synthesis', params=synthesis_payload, json=query)\n",
    "\n",
    "    if synthesis_response.status_code == 200:\n",
    "        # 音声ファイルとして保存\n",
    "        with open(filename, 'wb') as f:\n",
    "            f.write(synthesis_response.content)\n",
    "        print(f\"音声が {filename} に保存されました。\")\n",
    "        return \"output.wav\"\n",
    "    else:\n",
    "        print(f\"Error in synthesis: {synthesis_response.text}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    socketio.run(app, debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting static/index08.html\n"
     ]
    }
   ],
   "source": [
    "%%writefile static/index08.html\n",
    "\n",
    "<!DOCTYPE html>\n",
    "<html lang=\"ja\">\n",
    "<head>\n",
    "    <meta charset=\"UTF-8\">\n",
    "    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n",
    "    <title>WAV録音＆アップロード</title>\n",
    "    <!-- Recorder.js を読み込む -->\n",
    "    <script src=\"https://cdn.jsdelivr.net/gh/mattdiamond/Recorderjs@master/dist/recorder.js\"></script>\n",
    "\n",
    "    <!-- Socket.IO を読み込む -->\n",
    "    <script src=\"https://cdn.socket.io/4.0.0/socket.io.min.js\"></script>\n",
    "\n",
    "    \n",
    "</head>\n",
    "<body>\n",
    "    <h1>WAV録音＆アップロード</h1>\n",
    "    <button id=\"startRecording\">録音開始</button>\n",
    "    <button id=\"stopRecording\" disabled>録音停止</button>\n",
    "    <!-- <audio id=\"audioPlayback\" controls></audio> -->\n",
    "    <!-- <button id=\"uploadAudio\" disabled>アップロード</button> -->\n",
    "    <p><strong>文字起こし:</strong> <span id=\"transcription\"></span></p>\n",
    "    <p><strong>AIの応答:</strong> <span id=\"aiResponse\"></span></p>\n",
    "\n",
    "    <script>\n",
    "        let audioContext;\n",
    "        let recorder;\n",
    "        let audioBlob;\n",
    "\n",
    "        document.addEventListener(\"DOMContentLoaded\", () => {\n",
    "            const socket = io();\n",
    "\n",
    "            socket.on('ai_response', (data) => {\n",
    "                document.getElementById(\"aiResponse\").textContent = data.ai_response;\n",
    "                \n",
    "                // 音声ファイルを自動再生する処理\n",
    "                if (data.audio) {\n",
    "                    const audio = new Audio(`/audio/${data.audio}`);\n",
    "                    audio.play();\n",
    "                }    \n",
    "            });\n",
    "        });        \n",
    "\n",
    "        document.getElementById(\"startRecording\").addEventListener(\"click\", async () => {\n",
    "            const stream = await navigator.mediaDevices.getUserMedia({ audio: true });\n",
    "            audioContext = new AudioContext();\n",
    "            const source = audioContext.createMediaStreamSource(stream);\n",
    "            recorder = new Recorder(source, { numChannels: 1 }); // モノラル録音\n",
    "            recorder.record();\n",
    "\n",
    "            document.getElementById(\"startRecording\").disabled = true;\n",
    "            document.getElementById(\"stopRecording\").disabled = false;\n",
    "        });\n",
    "\n",
    "        document.getElementById(\"stopRecording\").addEventListener(\"click\", () => {\n",
    "            recorder.stop();\n",
    "            recorder.exportWAV((blob) => {\n",
    "                audioBlob = blob;\n",
    "\n",
    "                if (!audioBlob) {\n",
    "                    console.error(\"No audio to upload\");    \n",
    "                    return;\n",
    "                }\n",
    "\n",
    "                const formData = new FormData();\n",
    "                formData.append(\"file\", audioBlob, \"recorded_audio.wav\");\n",
    "\n",
    "                fetch(\"/upload\", {\n",
    "                    method: \"POST\",\n",
    "                    body: formData,\n",
    "                })\n",
    "                .then((response) => response.json())\n",
    "                .then((data) => {\n",
    "                    document.getElementById(\"transcription\").textContent =\n",
    "                    data.text || \"認識できませんでした。\";\n",
    "                })\n",
    "                .catch((error) => {\n",
    "                    console.error(\"Upload failed:\");\n",
    "                    document.getElementById(\"transcription\").textContent = \"エラーが発生しました。\";\n",
    "                    document.getElementById(\"aiResponse\").textContent = \"\";\n",
    "                });\n",
    "            });\n",
    "\n",
    "            document.getElementById(\"startRecording\").disabled = false;\n",
    "            document.getElementById(\"stopRecording\").disabled = true;\n",
    "\n",
    "\n",
    "        });\n",
    "    </script>\n",
    "</body>\n",
    "</html>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "とりあえず動く形にはできた！！\n",
    "ただ，どうしてもレスポンスは遅い．．．．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## その９：loggingモジュールをつかってみることにする"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting voicecahtapp09.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile voicecahtapp09.py\n",
    "from flask import Flask, request, jsonify, send_from_directory, send_file\n",
    "from flask_cors import CORS\n",
    "import os\n",
    "import speech_recognition as sr\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "from flask_socketio import SocketIO, emit\n",
    "import threading\n",
    "import requests\n",
    "import json\n",
    "import time\n",
    "import logging\n",
    "\n",
    "#　環境変数の読み込み\n",
    "load_dotenv()\n",
    "\n",
    "# Flaskアプリケーションの作成\n",
    "app = Flask(__name__, static_folder=\"static\")  \n",
    "\n",
    "# CORSの設定\n",
    "CORS(app)\n",
    "\n",
    "# Socket.IOの設定\n",
    "socketio = SocketIO(app)\n",
    "\n",
    "# 会話ログを保持する変数\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}\n",
    "]\n",
    "\n",
    "#--------------------------------------------------\n",
    "#loggingの設定\n",
    "logging.basicConfig(\n",
    "    level=logging.DEBUG,\n",
    "    format=\"%(asctime)s [%(levelname)s] %(message)s\",\n",
    "    filename=\"app.log\"\n",
    ")\n",
    "#--------------------------------------------------\n",
    "\n",
    "\n",
    "# ルートパスへのリクエストを処理する\n",
    "@app.route(\"/\")\n",
    "def index():\n",
    "    logging.info(\"index.html を返します。\")\n",
    "    return send_from_directory(\"static\", \"index08.html\")\n",
    "\n",
    "# /upload へのリクエストを処理する\n",
    "@app.route(\"/upload\", methods=[\"POST\"])\n",
    "def upload_audio():\n",
    "    logging.info(\"音声ファイルをアップロードします。\")\n",
    "    if \"file\" not in request.files:\n",
    "        logging.error(\"No audio file provided\")\n",
    "        return jsonify({\"error\": \"No audio file provided\"}), 400\n",
    "    \n",
    "    audio_file = request.files[\"file\"]\n",
    "    audio_path = os.path.join(\"uploads\", f\"input_{len(messages)}.wav\")\n",
    "    logging.debug(f\"Saving audio file to {audio_path}\")\n",
    "    audio_file.save(audio_path)\n",
    "\n",
    "    # 音声認識\n",
    "    r = sr.Recognizer()\n",
    "    with sr.AudioFile(audio_path) as source:\n",
    "        audio = r.record(source)\n",
    "        text = r.recognize_google(audio, language=\"ja-JP\")\n",
    "        if __debug__: # デバッグモードの場合\n",
    "            logging.debug(text)\n",
    "            print(text)\n",
    "\n",
    "    # 音声認識の結果を最初に返す\n",
    "    response = jsonify({\"text\": text})\n",
    "    \n",
    "    # 別スレッドでAIの応答を取得\n",
    "    threading.Thread(target=get_ai_response, args=(text,)).start()\n",
    "    \n",
    "    return response\n",
    "\n",
    "# 音声ファイルを提供するエンドポイント\n",
    "@app.route(\"/audio/<filename>\")\n",
    "def get_audio(filename):\n",
    "    return send_file(os.path.join(\"output\",filename))\n",
    "\n",
    "\n",
    "#--------------------------------------------------\n",
    "\n",
    "# AIの応答を取得する関数 \n",
    "def get_ai_response(text):\n",
    "    \n",
    "    # 現在の時刻取得\n",
    "    start = time.time()\n",
    "\n",
    "    # OpenAIのAPIを呼び出してAIの応答を取得\n",
    "    client = OpenAI()\n",
    "    messages.append({\"role\": \"user\", \"content\": text})\n",
    "    completion = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=messages\n",
    "    )\n",
    "    ai_response = completion.choices[0].message.content\n",
    "    messages.append({\"role\": \"assistant\", \"content\": ai_response})\n",
    "\n",
    "    # 処理時間の計算\n",
    "    ai_time = time.time() - start\n",
    "    logging.debug(f\"処理時間: {ai_time} [sec]\")\n",
    "    print(f\"AIレスポンス時間: {ai_time} [sec]\") \n",
    "\n",
    "    # 音声合成\n",
    "    filename = synthesize_voice(ai_response)\n",
    "\n",
    "    # 処理時間の計算\n",
    "    voice_time = time.time() - start - ai_time\n",
    "    logging.debug(f\"音声合成時間: {voice_time} [sec]\")\n",
    "    print(f\"音声合成時間: {voice_time} [sec]\")\n",
    "    \n",
    "    # WebSocketを通じてクライアントに通知\n",
    "    socketio.emit('ai_response', {'ai_response': ai_response, 'audio': filename})\n",
    "\n",
    "\n",
    "\n",
    "# 音声合成を行なう関数\n",
    "def synthesize_voice(text, speaker=1):\n",
    "    # 1. テキストから音声合成のためのクエリを作成\n",
    "    query_payload = {'text': text, 'speaker': speaker}\n",
    "    query_response = requests.post(f'http://localhost:50021/audio_query', params=query_payload)\n",
    "\n",
    "    if query_response.status_code != 200:\n",
    "        logging.error(f\"Error in audio_query: {query_response.text}\")\n",
    "        print(f\"Error in audio_query: {query_response.text}\")\n",
    "        return\n",
    "\n",
    "    query = query_response.json()\n",
    "\n",
    "    # 2. クエリを元に音声データを生成\n",
    "    synthesis_payload = {'speaker': speaker}\n",
    "    synthesis_response = requests.post(f'http://localhost:50021/synthesis', params=synthesis_payload, json=query)\n",
    "\n",
    "    if synthesis_response.status_code == 200:\n",
    "        # 音声ファイルとして保存\n",
    "        filename = f\"output_{len(messages)}.wav\"\n",
    "        file_path = \"output/\" + filename\n",
    "        with open(file_path, 'wb') as f:\n",
    "            f.write(synthesis_response.content)\n",
    "        logging.debug(f\"音声が {filename} に保存されました。\")\n",
    "        print(f\"音声が {filename} に保存されました。\")\n",
    "        return filename\n",
    "    else:\n",
    "        logging.error(f\"Error in synthesis: {synthesis_response.text}\")\n",
    "        print(f\"Error in synthesis: {synthesis_response.text}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    socketio.run(app, debug=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## その１０：htmlをチャットログが残るようにする．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting voicecahtapp10.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile voicecahtapp10.py\n",
    "from flask import Flask, request, jsonify, send_from_directory, send_file\n",
    "from flask_cors import CORS\n",
    "import os\n",
    "import speech_recognition as sr\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "from flask_socketio import SocketIO, emit\n",
    "import threading\n",
    "import requests\n",
    "import json\n",
    "import time\n",
    "import logging\n",
    "\n",
    "#　環境変数の読み込み\n",
    "load_dotenv()\n",
    "\n",
    "# Flaskアプリケーションの作成\n",
    "app = Flask(__name__, static_folder=\"static\")  \n",
    "\n",
    "# CORSの設定\n",
    "CORS(app)\n",
    "\n",
    "# Socket.IOの設定\n",
    "socketio = SocketIO(app)\n",
    "\n",
    "# 会話ログを保持する変数\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}\n",
    "]\n",
    "\n",
    "#--------------------------------------------------\n",
    "#loggingの設定\n",
    "logging.basicConfig(\n",
    "    level=logging.DEBUG,\n",
    "    format=\"%(asctime)s [%(levelname)s] %(message)s\",\n",
    "    filename=\"app.log\",\n",
    "    encoding=\"utf-8\"\n",
    ")\n",
    "#--------------------------------------------------\n",
    "\n",
    "\n",
    "# ルートパスへのリクエストを処理する\n",
    "@app.route(\"/\")\n",
    "def index():\n",
    "    logging.info(\"index.html を返します。\")\n",
    "    return send_from_directory(\"static\", \"index10.html\")\n",
    "\n",
    "# /upload へのリクエストを処理する\n",
    "@app.route(\"/upload\", methods=[\"POST\"])\n",
    "def upload_audio():\n",
    "    logging.info(\"音声ファイルをアップロードします。\")\n",
    "    if \"file\" not in request.files:\n",
    "        logging.error(\"No audio file provided\")\n",
    "        return jsonify({\"error\": \"No audio file provided\"}), 400\n",
    "    \n",
    "    audio_file = request.files[\"file\"]\n",
    "    audio_path = os.path.join(\"uploads\", f\"input_{len(messages)}.wav\")\n",
    "    audio_file.save(audio_path)\n",
    "    logging.info(f\"Saved audio file to {audio_path}\")\n",
    "\n",
    "    # 音声認識\n",
    "    r = sr.Recognizer()\n",
    "    start_time = time.time()\n",
    "    with sr.AudioFile(audio_path) as source:\n",
    "        audio = r.record(source)\n",
    "        text = r.recognize_google(audio, language=\"ja-JP\")\n",
    "    logging.info(f\"音声認識結果: {text}\")\n",
    "    logging.info(f\"音声認識時間: {time.time() - start_time} [sec]\")\n",
    "\n",
    "    # 音声認識の結果を最初に返す\n",
    "    response = jsonify({\"text\": text})\n",
    "    \n",
    "    # 別スレッドでAIの応答を取得\n",
    "    threading.Thread(target=get_ai_response, args=(text,)).start()\n",
    "    \n",
    "    return response\n",
    "\n",
    "# 音声ファイルを提供するエンドポイント\n",
    "@app.route(\"/audio/<filename>\")\n",
    "def get_audio(filename):\n",
    "    logging.info(f\"音声ファイル {filename} を返します。\")\n",
    "    return send_file(os.path.join(\"output\",filename))\n",
    "\n",
    "\n",
    "#--------------------------------------------------\n",
    "\n",
    "# AIの応答を取得する関数 \n",
    "def get_ai_response(text):\n",
    "    \n",
    "    # 現在の時刻取得\n",
    "    start = time.time()\n",
    "\n",
    "    # OpenAIのAPIを呼び出してAIの応答を取得\n",
    "    client = OpenAI()\n",
    "    messages.append({\"role\": \"user\", \"content\": text})\n",
    "    completion = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=messages\n",
    "    )\n",
    "    ai_response = completion.choices[0].message.content\n",
    "    messages.append({\"role\": \"assistant\", \"content\": ai_response})\n",
    "    logging.info(f\"AIの応答: {ai_response}\")\n",
    "\n",
    "    # 処理時間の計算\n",
    "    ai_time = time.time() - start\n",
    "    logging.info(f\"AIレスポンス時間: {ai_time} [sec]\")\n",
    "\n",
    "    # 音声合成\n",
    "    filename = synthesize_voice(ai_response)\n",
    "\n",
    "    # 処理時間の計算\n",
    "    voice_time = time.time() - start - ai_time\n",
    "    logging.info(f\"音声合成時間: {voice_time} [sec]\")\n",
    "    \n",
    "    # WebSocketを通じてクライアントに通知\n",
    "    socketio.emit('ai_response', {'ai_response': ai_response, 'audio': filename})\n",
    "\n",
    "\n",
    "\n",
    "# 音声合成を行なう関数\n",
    "def synthesize_voice(text, speaker=1):\n",
    "    # 1. テキストから音声合成のためのクエリを作成\n",
    "    query_payload = {'text': text, 'speaker': speaker}\n",
    "    query_response = requests.post(f'http://localhost:50021/audio_query', params=query_payload)\n",
    "\n",
    "    if query_response.status_code != 200:\n",
    "        logging.error(f\"Error in audio_query: {query_response.text}\")\n",
    "        print(f\"Error in audio_query: {query_response.text}\")\n",
    "        return\n",
    "\n",
    "    query = query_response.json()\n",
    "\n",
    "    # 2. クエリを元に音声データを生成\n",
    "    synthesis_payload = {'speaker': speaker}\n",
    "    synthesis_response = requests.post(f'http://localhost:50021/synthesis', params=synthesis_payload, json=query)\n",
    "\n",
    "    if synthesis_response.status_code == 200:\n",
    "        # 音声ファイルとして保存\n",
    "        filename = f\"output_{len(messages)}.wav\"\n",
    "        file_path = \"output/\" + filename\n",
    "        with open(file_path, 'wb') as f:\n",
    "            f.write(synthesis_response.content)\n",
    "        logging.info(f\"音声が {filename} に保存されました。\")\n",
    "        print(f\"音声が {filename} に保存されました。\")\n",
    "        return filename\n",
    "    else:\n",
    "        logging.error(f\"Error in synthesis: {synthesis_response.text}\")\n",
    "        print(f\"Error in synthesis: {synthesis_response.text}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    logging.info(\"#####アプリケーションを起動します。#####\")\n",
    "    socketio.run(app, debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting static/index10.html\n"
     ]
    }
   ],
   "source": [
    "%%writefile static/index10.html\n",
    "\n",
    "<html lang=\"ja\">\n",
    "<head>\n",
    "    <meta charset=\"UTF-8\">\n",
    "    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n",
    "    <title>WAV録音＆アップロード</title>\n",
    "    <!-- Recorder.js を読み込む -->\n",
    "    <script src=\"https://cdn.jsdelivr.net/gh/mattdiamond/Recorderjs@master/dist/recorder.js\"></script>\n",
    "\n",
    "    <!-- Socket.IO を読み込む -->\n",
    "    <script src=\"https://cdn.socket.io/4.0.0/socket.io.min.js\"></script>\n",
    "\n",
    "    <!-- marked.js を読み込む -->\n",
    "    <script src=\"https://cdn.jsdelivr.net/npm/marked/marked.min.js\"></script>\n",
    "\n",
    "    <!-- cssの適用-->\n",
    "    <link rel=\"stylesheet\" href=\"/static/voicechatapp.css\">\n",
    "\n",
    "    \n",
    "</head>\n",
    "<body>\n",
    "    <h1>WAV録音＆アップロード</h1>\n",
    "    <button id=\"startRecording\">録音開始</button>\n",
    "    <button id=\"stopRecording\" disabled>録音停止</button>\n",
    "    <div id=\"h_chatlog\"></div>\n",
    "\n",
    "    <script>\n",
    "        let audioContext;\n",
    "        let recorder;\n",
    "        let audioBlob;\n",
    "\n",
    "        document.addEventListener(\"DOMContentLoaded\", () => {\n",
    "            const socket = io();\n",
    "\n",
    "            socket.on('ai_response', (data) => {\n",
    "                const markdownText = data.ai_response;\n",
    "                const htmlContent = marked.parse(markdownText);\n",
    "                document.getElementById(\"h_chatlog\").innerHTML += `<div class=\"assistant\">${htmlContent}</div>`;\n",
    "                \n",
    "                // 音声ファイルを自動再生する処理\n",
    "                if (data.audio) {\n",
    "                    const audio = new Audio(`/audio/${data.audio}`);\n",
    "                    audio.play();\n",
    "                }\n",
    "                document.getElementById(\"startRecording\").disabled = false;\n",
    "                document.getElementById(\"stopRecording\").disabled = true;                    \n",
    "            });\n",
    "\n",
    "            // Spaceキーが押されたときにstartRecordingボタンをクリック\n",
    "            document.addEventListener(\"keydown\", (event) => {\n",
    "                if(document.getElementById(\"startRecording\").disabled){ \n",
    "                    console.log(\"処理中のため入力はできません\");\n",
    "                    return;\n",
    "                }\n",
    "                if (event.code === \"Space\" && !event.repeat) {\n",
    "                    document.getElementById(\"startRecording\").click();\n",
    "                }\n",
    "            });\n",
    "\n",
    "            // Spaceキーから指が離されたときにstopRecordingボタンをクリック\n",
    "            document.addEventListener(\"keyup\", (event) => {\n",
    "                if(document.getElementById(\"stopRecording\").disabled){\n",
    "                    console.log(\"不正な録音停止操作です\");\n",
    "                    return;\n",
    "                }\n",
    "                if (event.code === \"Space\" && !event.repeat) {\n",
    "                    document.getElementById(\"stopRecording\").click();\n",
    "                }\n",
    "            });\n",
    "\n",
    "        });        \n",
    "\n",
    "        document.getElementById(\"startRecording\").addEventListener(\"click\", async () => {\n",
    "            const stream = await navigator.mediaDevices.getUserMedia({ audio: true });\n",
    "            audioContext = new AudioContext();\n",
    "            const source = audioContext.createMediaStreamSource(stream);\n",
    "            recorder = new Recorder(source, { numChannels: 1 }); // モノラル録音\n",
    "            recorder.record();\n",
    "\n",
    "            document.getElementById(\"startRecording\").disabled = true;\n",
    "            document.getElementById(\"stopRecording\").disabled = false;\n",
    "        });\n",
    "\n",
    "        document.getElementById(\"stopRecording\").addEventListener(\"click\", () => {\n",
    "            recorder.stop();\n",
    "            recorder.exportWAV((blob) => {\n",
    "                audioBlob = blob;\n",
    "\n",
    "                if (!audioBlob) {\n",
    "                    console.error(\"No audio to upload\");    \n",
    "                    return;\n",
    "                }\n",
    "\n",
    "                const formData = new FormData();\n",
    "                formData.append(\"file\", audioBlob, \"recorded_audio.wav\");\n",
    "\n",
    "                fetch(\"/upload\", {\n",
    "                    method: \"POST\",\n",
    "                    body: formData,\n",
    "                })\n",
    "                .then((response) => response.json())\n",
    "                .then((data) => {\n",
    "                    if(data.text){\n",
    "                         document.getElementById(\"h_chatlog\").innerHTML += `<div class=\"user\">${marked.parse(data.text)}</div>`;\n",
    "                    }\n",
    "                    else console.log(\"Error: 音声を認識できませんでした。\");\n",
    "                })\n",
    "                .catch((error) => {\n",
    "                    console.error(\"Upload failed:\");\n",
    "                });\n",
    "            });\n",
    "\n",
    "            document.getElementById(\"startRecording\").disabled = true;\n",
    "            document.getElementById(\"stopRecording\").disabled = true;\n",
    "        });\n",
    "\n",
    "\n",
    "    </script>\n",
    "</body>\n",
    "</html>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "概ね上手く行くようにできた．\n",
    "後の問題は，AIからのレスポンスがマークダウン形式になっているのをうまく表記することかな．\n",
    "\n",
    "-> 対応した　marked.jsなんてのがあるんや．これ使えば簡単！"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## その１１　キャラクターボイスの選択"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://zenn.dev/zenn24yykiitos/articles/f3e983fe650e08"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "これを参考に，フロントエンドからキャラクターを選べるようにする．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting voicechatapp11.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile voicechatapp11.py\n",
    "\n",
    "from flask import Flask, request, jsonify, send_from_directory, send_file\n",
    "from flask_cors import CORS\n",
    "import os\n",
    "import speech_recognition as sr\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "from flask_socketio import SocketIO, emit\n",
    "import threading\n",
    "import requests\n",
    "import json\n",
    "import time\n",
    "import logging\n",
    "\n",
    "#　環境変数の読み込み\n",
    "load_dotenv()\n",
    "\n",
    "# Flaskアプリケーションの作成\n",
    "app = Flask(__name__, static_folder=\"static\")  \n",
    "\n",
    "# CORSの設定\n",
    "CORS(app)\n",
    "\n",
    "# Socket.IOの設定\n",
    "socketio = SocketIO(app)\n",
    "\n",
    "# 会話ログを保持する変数\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}\n",
    "]\n",
    "\n",
    "#loggingの設定\n",
    "logging.basicConfig(\n",
    "    level=logging.DEBUG,\n",
    "    format=\"%(asctime)s [%(levelname)s] %(message)s\",\n",
    "    filename=\"app.log\",\n",
    "    encoding=\"utf-8\"\n",
    ")\n",
    "\n",
    "#--------------------------------------------------\n",
    "# Flaskのエンドポイントの作成\n",
    "#--------------------------------------------------\n",
    "\n",
    "# ルートパスへのリクエストを処理する\n",
    "@app.route(\"/\")\n",
    "def index():\n",
    "    logging.info(\"index.html を返します。\")\n",
    "    return send_from_directory(\"static\", \"index11.html\")\n",
    "\n",
    "# /upload へのリクエストを処理する\n",
    "@app.route(\"/upload\", methods=[\"POST\"])\n",
    "def upload_audio():\n",
    "    logging.info(\"音声ファイルをアップロードします。\")\n",
    "    if \"file\" not in request.files:\n",
    "        logging.error(\"No audio file provided\")\n",
    "        return jsonify({\"error\": \"No audio file provided\"}), 400\n",
    "    \n",
    "    audio_file = request.files[\"file\"]\n",
    "    #audio_path = os.path.join(\"uploads\", f\"input_{len(messages)}.wav\") #Uploadされたファイルを残すならこっちをOn\n",
    "    audio_path = os.path.join(\"uploads\", \"input.wav\") #Uploadされたファイルを残さないならこっちをOn\n",
    "    audio_file.save(audio_path)\n",
    "    logging.info(f\"Saved audio file to {audio_path}\")\n",
    "\n",
    "    # 音声認識\n",
    "    r = sr.Recognizer()\n",
    "    start_time = time.time()\n",
    "    with sr.AudioFile(audio_path) as source:\n",
    "        audio = r.record(source)\n",
    "        text = r.recognize_google(audio, language=\"ja-JP\")\n",
    "    logging.info(f\"音声認識結果: {text}\")\n",
    "    logging.info(f\"音声認識時間: {time.time() - start_time} [sec]\")\n",
    "\n",
    "    # 音声認識の結果を最初に返す\n",
    "    response = jsonify({\"text\": text})\n",
    "    \n",
    "    # 別スレッドでAIの応答を取得\n",
    "    speaker = request.form[\"speaker\"]\n",
    "    logging.info(f\"AIの応答を取得します。音声合成スピーカーID: {speaker}\")\n",
    "    threading.Thread(target=get_ai_response, args=(text, speaker)).start()\n",
    "    \n",
    "    return response\n",
    "\n",
    "# 音声ファイルを提供するエンドポイント\n",
    "@app.route(\"/audio/<filename>\")\n",
    "def get_audio(filename):\n",
    "    logging.info(f\"音声ファイル {filename} を返します。\")\n",
    "    return send_file(os.path.join(\"output\",filename))\n",
    "\n",
    "# VoiceVoxのSpeakerIDリストを取得するエンドポイント\n",
    "@app.route(\"/speaker_ids\")\n",
    "def get_speaker_ids():\n",
    "    url = \"http://localhost:50021/speakers\"  # VOICEVOX APIのエンドポイント\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error: {e}\")\n",
    "        return jsonify([])\n",
    "\n",
    "    voicevox_speakers = []\n",
    "    if response.status_code == 200:\n",
    "        speakers = response.json()\n",
    "        for speaker in speakers:\n",
    "            name = speaker['name']\n",
    "            style_names = [style['name'] for style in speaker['styles']]\n",
    "            style_ids = [style['id'] for style in speaker['styles']]\n",
    "            for style_id, style_name in zip(style_ids, style_names):\n",
    "                voicevox_speakers.append(f\"<option value={style_id}>Speaker: {name}, {style_name} </option>\")\n",
    "        logging.info(\"speaker_ids を取得しました。\")\n",
    "        return jsonify(voicevox_speakers)\n",
    "    else:\n",
    "        logging.error(f\"Error: {response.status_code}\")\n",
    "        return jsonify([])    \n",
    "\n",
    "# VoiceVoxの音声テストを行うエンドポイント\n",
    "@app.route(\"/speaker_test\" , methods=[\"POST\"])\n",
    "def speaker_test():\n",
    "    speaker = request.json[\"speaker\"]\n",
    "    text = \"こんにちは．初めまして．何かお手伝いできることはありますか？\"\n",
    "    filename = synthesize_voice(text, speaker)\n",
    "    return jsonify({\"audio\": filename})\n",
    "\n",
    "\n",
    "#--------------------------------------------------\n",
    "# Flaskの各エンドポイント内の処理関数\n",
    "#--------------------------------------------------\n",
    "# AIの応答を取得する関数 \n",
    "def get_ai_response(text, speaker):\n",
    "    \n",
    "    # 現在の時刻取得\n",
    "    start = time.time()\n",
    "\n",
    "    # OpenAIのAPIを呼び出してAIの応答を取得\n",
    "    client = OpenAI()\n",
    "    messages.append({\"role\": \"user\", \"content\": text})\n",
    "    completion = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=messages\n",
    "    )\n",
    "    ai_response = completion.choices[0].message.content\n",
    "    messages.append({\"role\": \"assistant\", \"content\": ai_response})\n",
    "    logging.info(f\"AIの応答: {ai_response}\")\n",
    "\n",
    "    # 処理時間の計算\n",
    "    ai_time = time.time() - start\n",
    "    logging.info(f\"AIレスポンス時間: {ai_time} [sec]\")\n",
    "\n",
    "    # 音声合成\n",
    "    filename = synthesize_voice(ai_response, speaker)\n",
    "\n",
    "    # 処理時間の計算\n",
    "    voice_time = time.time() - start - ai_time\n",
    "    logging.info(f\"音声合成時間: {voice_time} [sec]\")\n",
    "    \n",
    "    # WebSocketを通じてクライアントに通知\n",
    "    socketio.emit('ai_response', {'ai_response': ai_response, 'audio': filename})\n",
    "\n",
    "\n",
    "\n",
    "# VoiceVox APIで音声合成を行なう関数\n",
    "def synthesize_voice(text, speaker):\n",
    "    # 1. テキストから音声合成のためのクエリを作成\n",
    "    query_payload = {'text': text, 'speaker': speaker}\n",
    "    query_response = requests.post(f'http://localhost:50021/audio_query', params=query_payload)\n",
    "\n",
    "    if query_response.status_code != 200:\n",
    "        logging.error(f\"Error in audio_query: {query_response.text}\")\n",
    "        print(f\"Error in audio_query: {query_response.text}\")\n",
    "        return\n",
    "\n",
    "    query = query_response.json()\n",
    "\n",
    "    # 2. クエリを元に音声データを生成\n",
    "    synthesis_payload = {'speaker': speaker}\n",
    "    synthesis_response = requests.post(f'http://localhost:50021/synthesis', params=synthesis_payload, json=query)\n",
    "\n",
    "    if synthesis_response.status_code == 200:\n",
    "        # 音声ファイルとして保存\n",
    "        #filename = f\"output_{len(messages)}.wav\" #合成音声を全部残すならこっちをON\n",
    "        filename = \"output.wav\" #合成音声を全部残さないならこっちをON\n",
    "        file_path = \"output/\" + filename\n",
    "        with open(file_path, 'wb') as f:\n",
    "            f.write(synthesis_response.content)\n",
    "        logging.info(f\"音声が {filename} に保存されました。\")\n",
    "        return filename\n",
    "    else:\n",
    "        logging.error(f\"Error in synthesis: {synthesis_response.text}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    logging.info(\"#####アプリケーションを起動します。#####\")\n",
    "    socketio.run(app, debug=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting static/index11.html\n"
     ]
    }
   ],
   "source": [
    "%%writefile static/index11.html\n",
    "<html lang=\"ja\">\n",
    "  <head>\n",
    "    <meta charset=\"UTF-8\" />\n",
    "    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\" />\n",
    "    <title>WAV録音＆アップロード</title>\n",
    "    <!-- Recorder.js を読み込む -->\n",
    "    <script src=\"https://cdn.jsdelivr.net/gh/mattdiamond/Recorderjs@master/dist/recorder.js\"></script>\n",
    "\n",
    "    <!-- Socket.IO を読み込む -->\n",
    "    <script src=\"https://cdn.socket.io/4.0.0/socket.io.min.js\"></script>\n",
    "\n",
    "    <!-- marked.js を読み込む -->\n",
    "    <script src=\"https://cdn.jsdelivr.net/npm/marked/marked.min.js\"></script>\n",
    "\n",
    "    <!-- cssの適用-->\n",
    "    <link rel=\"stylesheet\" href=\"/static/voicechatapp.css\" />\n",
    "  </head>\n",
    "\n",
    "  <body>\n",
    "    <h1>WAV録音＆アップロード</h1>\n",
    "    <button id=\"startRecording\">録音開始</button>\n",
    "    <button id=\"stopRecording\" disabled>録音停止</button>\n",
    "    <select id=\"h_speakerSelect\"></select>\n",
    "    <button id=\"speakerTest\">音声テスト</button>\n",
    "    <div id=\"h_chatlog\"></div>\n",
    "\n",
    "    <script>\n",
    "      navigator.mediaDevices\n",
    "        .getUserMedia({ audio: true })\n",
    "        .then((stream) => {\n",
    "          window.stream = stream;\n",
    "        })\n",
    "        .catch((error) => {\n",
    "          console.error(\"Error accessing the microphone: \" + error);\n",
    "        });\n",
    "      let audioContext;\n",
    "      let recorder;\n",
    "      let audioBlob;\n",
    "\n",
    "      document.addEventListener(\"DOMContentLoaded\", () => {\n",
    "        const socket = io();\n",
    "\n",
    "        // SpeakerIDリストを取得\n",
    "        fetch(\"/speaker_ids\")\n",
    "          .then((response) => response.json())\n",
    "          .then((data) => {\n",
    "            const h_speakerSelect = document.getElementById(\"h_speakerSelect\");\n",
    "            h_speakerSelect.innerHTML = data.join(\"\");\n",
    "          });\n",
    "\n",
    "        // AIの応答を受信したときの処理\n",
    "        socket.on(\"ai_response\", (data) => {\n",
    "          const markdownText = data.ai_response;\n",
    "          const htmlContent = marked.parse(markdownText);\n",
    "          document.getElementById(\n",
    "            \"h_chatlog\"\n",
    "          ).innerHTML += `<div class=\"assistant\">${htmlContent}</div>`;\n",
    "\n",
    "          // 音声ファイルを自動再生する処理\n",
    "          console.log(\"audioリクエスト\");\n",
    "          const audio = new Audio(`/audio/${data.audio}`);\n",
    "          console.log(\"audio受信しました\", audio);\n",
    "          audio.play();\n",
    "\n",
    "          document.getElementById(\"startRecording\").disabled = false;\n",
    "          document.getElementById(\"stopRecording\").disabled = true;\n",
    "          document.getElementById(\"selectSpeaker\").disabled = false;\n",
    "        });\n",
    "\n",
    "        // Spaceキーが押されたときにstartRecordingボタンをクリック\n",
    "        document.addEventListener(\"keydown\", (event) => {\n",
    "          if (document.getElementById(\"startRecording\").disabled) {\n",
    "            console.log(\"処理中のため入力はできません\");\n",
    "            return;\n",
    "          }\n",
    "          if (event.code === \"Space\" && !event.repeat) {\n",
    "            document.getElementById(\"startRecording\").click();\n",
    "          }\n",
    "        });\n",
    "\n",
    "        // Spaceキーから指が離されたときにstopRecordingボタンをクリック\n",
    "        document.addEventListener(\"keyup\", (event) => {\n",
    "          if (document.getElementById(\"stopRecording\").disabled) {\n",
    "            console.log(\"不正な録音停止操作です\");\n",
    "            return;\n",
    "          }\n",
    "          if (event.code === \"Space\" && !event.repeat) {\n",
    "            document.getElementById(\"stopRecording\").click();\n",
    "          }\n",
    "        });\n",
    "\n",
    "        document.getElementById(\"speakerTest\").addEventListener(\"click\", () => {\n",
    "          const speaker = document.getElementById(\"h_speakerSelect\").value;\n",
    "          fetch(\"/speaker_test\", {\n",
    "            method: \"POST\",\n",
    "            headers: {\n",
    "              \"Content-Type\": \"application/json\",\n",
    "            },\n",
    "            body: JSON.stringify({ speaker }),\n",
    "          })\n",
    "            .then((response) => response.json())\n",
    "            .then((data) => {\n",
    "              const audio = new Audio(`/audio/${data.audio}`);\n",
    "              audio.play();\n",
    "            });\n",
    "        });\n",
    "\n",
    "        document\n",
    "          .getElementById(\"startRecording\")\n",
    "          .addEventListener(\"click\", () => {\n",
    "            audioContext = new AudioContext();\n",
    "            const source = audioContext.createMediaStreamSource(window.stream);\n",
    "            recorder = new Recorder(source, { numChannels: 1 }); // モノラル録音\n",
    "            recorder.record();\n",
    "\n",
    "            document.getElementById(\"startRecording\").disabled = true;\n",
    "            document.getElementById(\"stopRecording\").disabled = false;\n",
    "            document.getElementById(\"selectSpeaker\").disabled = true;\n",
    "          });\n",
    "\n",
    "        document\n",
    "          .getElementById(\"stopRecording\")\n",
    "          .addEventListener(\"click\", () => {\n",
    "            recorder.stop();\n",
    "            recorder.exportWAV((blob) => {\n",
    "              audioBlob = blob;\n",
    "\n",
    "              if (!audioBlob) {\n",
    "                console.error(\"No audio to upload\");\n",
    "                return;\n",
    "              }\n",
    "\n",
    "              const formData = new FormData();\n",
    "              formData.append(\"file\", audioBlob, \"recorded_audio.wav\");\n",
    "\n",
    "              const speaker = document.getElementById(\"h_speakerSelect\").value;\n",
    "              formData.append(\"speaker\", speaker);\n",
    "\n",
    "              fetch(\"/upload\", {\n",
    "                method: \"POST\",\n",
    "                body: formData,\n",
    "              })\n",
    "                .then((response) => response.json())\n",
    "                .then((data) => {\n",
    "                  if (data.text) {\n",
    "                    document.getElementById(\n",
    "                      \"h_chatlog\"\n",
    "                    ).innerHTML += `<div class=\"user\">${marked.parse(\n",
    "                      data.text\n",
    "                    )}</div>`;\n",
    "                  } else console.log(\"Error: 音声を認識できませんでした。\");\n",
    "                })\n",
    "                .catch((error) => {\n",
    "                  console.error(\"Upload failed:\");\n",
    "                });\n",
    "            });\n",
    "\n",
    "            document.getElementById(\"startRecording\").disabled = true;\n",
    "            document.getElementById(\"stopRecording\").disabled = true;\n",
    "          });\n",
    "      });\n",
    "    </script>\n",
    "  </body>\n",
    "</html>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## その１２　よりリアルタイム化したい\n",
    "テキストを句点や感嘆符などの句単位に区切り，それをflask側でストリーミングで音声合成し，フロントエンドにストリーミで返す．フロントエンドでは受け取ったストリームを再生する．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing voicechatapp12.py\n"
     ]
    }
   ],
   "source": [
    "from flask import Flask, request, Response, jsonify, send_from_directory, send_file, stream_with_context\n",
    "from flask_cors import CORS\n",
    "import os\n",
    "import speech_recognition as sr\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "from flask_socketio import SocketIO, emit\n",
    "import threading\n",
    "import requests\n",
    "import json\n",
    "import time\n",
    "import logging\n",
    "import re\n",
    "\n",
    "#　環境変数の読み込み\n",
    "load_dotenv()\n",
    "\n",
    "# Flaskアプリケーションの作成\n",
    "app = Flask(__name__, static_folder=\"static\")  \n",
    "\n",
    "# CORSの設定\n",
    "CORS(app)\n",
    "\n",
    "# Socket.IOの設定\n",
    "socketio = SocketIO(app)\n",
    "\n",
    "# 会話ログを保持する変数\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}\n",
    "]\n",
    "\n",
    "#loggingの設定\n",
    "logging.basicConfig(\n",
    "    level=logging.DEBUG,\n",
    "    format=\"%(asctime)s [%(levelname)s] %(message)s\",\n",
    "    filename=\"app.log\",\n",
    "    encoding=\"utf-8\"\n",
    ")\n",
    "\n",
    "#--------------------------------------------------\n",
    "# Flaskのエンドポイントの作成\n",
    "#--------------------------------------------------\n",
    "\n",
    "# ルートパスへのリクエストを処理する\n",
    "@app.route(\"/\")\n",
    "def index():\n",
    "    logging.info(\"index.html を返します。\")\n",
    "    return send_from_directory(\"static\", \"index12.html\")\n",
    "\n",
    "# VoiceVoxのSpeakerIDリストを取得するエンドポイント\n",
    "@app.route(\"/speaker_ids\")\n",
    "def get_speaker_ids():\n",
    "    url = \"http://localhost:50021/speakers\"  # VOICEVOX APIのエンドポイント\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error: {e}\")\n",
    "        return jsonify([])\n",
    "\n",
    "    voicevox_speakers = []\n",
    "    if response.status_code == 200:\n",
    "        speakers = response.json()\n",
    "        for speaker in speakers:\n",
    "            name = speaker['name']\n",
    "            style_names = [style['name'] for style in speaker['styles']]\n",
    "            style_ids = [style['id'] for style in speaker['styles']]\n",
    "            for style_id, style_name in zip(style_ids, style_names):\n",
    "                voicevox_speakers.append(f\"<option value={style_id}>Speaker: {name}, {style_name} </option>\")\n",
    "        logging.info(\"speaker_ids を取得しました。\")\n",
    "        return jsonify(voicevox_speakers)\n",
    "    else:\n",
    "        logging.error(f\"Error: {response.status_code}\")\n",
    "        return jsonify([])    \n",
    "\n",
    "# VoiceVoxの音声テストを行うエンドポイント\n",
    "@app.route(\"/speaker_test\" , methods=[\"POST\"])\n",
    "def speaker_test():\n",
    "    speaker = request.json[\"speaker\"]\n",
    "    text = \"こんにちは．初めまして．何かお手伝いできることはありますか？\"\n",
    "    filename = synthesize_voice(text, speaker)\n",
    "    return jsonify({\"audio\": filename})\n",
    "\n",
    "\n",
    "# /upload へのリクエストを処理する\n",
    "@app.route(\"/upload\", methods=[\"POST\"])\n",
    "def upload_audio():\n",
    "    # uploads ディレクトリがなければ作成\n",
    "    if not os.path.exists(\"uploads\"):\n",
    "        os.makedirs(\"uploads\")\n",
    "    # uploads ディレクトリにファイルがあれば削除\n",
    "    else:\n",
    "        for file in os.listdir(\"uploads\"):\n",
    "            os.remove(os.path.join(\"uploads\", file))\n",
    "\n",
    "    # 音声ファイルをアップロード\n",
    "    if \"file\" not in request.files:\n",
    "        logging.error(\"No audio file provided\")\n",
    "        return jsonify({\"error\": \"No audio file provided\"}), 400\n",
    "    \n",
    "    audio_file = request.files[\"file\"]\n",
    "    audio_path = os.path.join(\"uploads\", f\"input_{len(messages)}.wav\") #Uploadされたファイルを残すならこっちをOn\n",
    "    audio_file.save(audio_path)\n",
    "\n",
    "    # 音声認識\n",
    "    text = recognize_speech(audio_path)\n",
    "    ## 音声認識の結果をWebSocketを通じてクライアントに通知\n",
    "    if text:\n",
    "        socketio.emit(\"SpeechRecognition\",{\"text\": text})\n",
    "    else:\n",
    "        return jsonify({\"error\": \"Failed to recognize speech\"}), 400    \n",
    "    \n",
    "    # AIの応答を取得\n",
    "    ai_response = get_ai_response(text)\n",
    "    ## WebSocketを通じてクライアントに通知\n",
    "    if ai_response:\n",
    "        socketio.emit('ai_response', {'ai_response': ai_response}) #, 'audio': filename})\n",
    "    else:\n",
    "        return jsonify({\"error\": \"Failed to get AI response\"}), 400\n",
    "\n",
    "    # AIの応答から音声合成\n",
    "    speaker = request.form[\"speaker\"]\n",
    "    filename = synthesize_voice(ai_response, speaker)\n",
    "    ## WebSocketを通じてクライアントに通知\n",
    "    if filename:\n",
    "        socketio.emit('play_audio', {'audio': filename})\n",
    "    else:\n",
    "        return jsonify({\"error\": \"Failed to synthesize voice\"}), 400\n",
    "    \n",
    "    return jsonify({\"info\": \"Process Succeeded\"}), 200\n",
    "\n",
    "\n",
    "# 音声ファイルを提供するエンドポイント\n",
    "@app.route(\"/audio/<filename>\")\n",
    "def get_audio(filename):\n",
    "    return send_file(os.path.join(\"output\",filename))\n",
    "\n",
    "\n",
    "\n",
    "# streaming処理するエンドポイント\n",
    "@app.route(\"/streaming\", methods=[\"POST\"])\n",
    "def streaming():\n",
    "    # uploads ディレクトリがなければ作成\n",
    "    if not os.path.exists(\"uploads\"):\n",
    "        os.makedirs(\"uploads\")\n",
    "    # uploads ディレクトリにファイルがあれば削除\n",
    "    else:\n",
    "        for file in os.listdir(\"uploads\"):\n",
    "            os.remove(os.path.join(\"uploads\", file))\n",
    "\n",
    "    # 音声ファイルをアップロード\n",
    "    if \"file\" not in request.files:\n",
    "        logging.error(\"No audio file provided\")\n",
    "        return jsonify({\"error\": \"No audio file provided\"}), 400\n",
    "    \n",
    "    audio_file = request.files[\"file\"]\n",
    "    audio_path = os.path.join(\"uploads\", \"input.wav\") #Uploadされたファイルを残さないならこっちをOn\n",
    "    audio_file.save(audio_path)\n",
    "\n",
    "    # 音声認識\n",
    "    text = recognize_speech(audio_path)\n",
    "    # 音声認識の結果を最初に返す\n",
    "    socketio.emit(\"SpeechRecognition\",{\"text\": text})\n",
    "    \n",
    "    # 続けてストリームで音声合成\n",
    "    ## まずは openai で応答を取得\n",
    "    ai_response = get_ai_response(text)\n",
    "    socketio.emit(\"AIResponse\", {\"ai_response\": ai_response})\n",
    "\n",
    "    ## 音声合成\n",
    "    speaker = request.form[\"speaker\"]\n",
    "    logging.info(f\"AIの応答を取得します。音声合成スピーカーID: {speaker}\")    \n",
    "    \n",
    "\n",
    "    def generate():\n",
    "        yield from synthesize_streaming(ai_response, speaker)\n",
    "\n",
    "    socketio.emit(\"Streaming\", {\n",
    "        \"Response\": Response(\n",
    "            stream_with_context(generate()),\n",
    "            content_type=\"application/octet-stream\"\n",
    "        )\n",
    "    })\n",
    "    \n",
    "    return jsonify({\"info\": \"Process Succeeded\"}), 200\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#--------------------------------------------------\n",
    "# Flaskの各エンドポイント内の処理関数\n",
    "#--------------------------------------------------\n",
    "# 音声認識を行う関数\n",
    "def recognize_speech(audio_path):\n",
    "    r = sr.Recognizer()\n",
    "    with sr.AudioFile(audio_path) as source:\n",
    "        audio = r.record(source)\n",
    "        text = r.recognize_google(audio, language=\"ja-JP\")\n",
    "    return text\n",
    "\n",
    "# OpenAIのAPIを呼び出してAIの応答を取得する関数\n",
    "def get_ai_response(text):\n",
    "    client = OpenAI()\n",
    "    messages.append({\"role\": \"user\", \"content\": text})\n",
    "    completion = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=messages\n",
    "    )\n",
    "    ai_response = completion.choices[0].message.content\n",
    "    messages.append({\"role\": \"assistant\", \"content\": ai_response})\n",
    "    logging.info(f\"AIの応答: {ai_response}\")\n",
    "    return ai_response\n",
    "\n",
    "\n",
    "# VoiceVox APIで音声合成を行なう関数\n",
    "def synthesize_voice(text, speaker):\n",
    "    # 1. テキストから音声合成のためのクエリを作成\n",
    "    query_payload = {'text': text, 'speaker': speaker}\n",
    "    query_response = requests.post(f'http://localhost:50021/audio_query', params=query_payload)\n",
    "\n",
    "    if query_response.status_code != 200:\n",
    "        logging.error(f\"Error in audio_query: {query_response.text}\")\n",
    "        print(f\"Error in audio_query: {query_response.text}\")\n",
    "        return\n",
    "\n",
    "    query = query_response.json()\n",
    "\n",
    "    # 2. クエリを元に音声データを生成\n",
    "    synthesis_payload = {'speaker': speaker}\n",
    "    synthesis_response = requests.post(f'http://localhost:50021/synthesis', params=synthesis_payload, json=query)\n",
    "\n",
    "    if synthesis_response.status_code == 200:\n",
    "        # 音声ファイルとして保存\n",
    "        filename = f\"output_{len(messages)}.wav\" #合成音声を全部残すならこっちをON\n",
    "        #filename = \"output.wav\" #合成音声を全部残さないならこっちをON\n",
    "        file_path = \"output/\" + filename\n",
    "        with open(file_path, 'wb') as f:\n",
    "            f.write(synthesis_response.content)\n",
    "        logging.info(f\"音声が {filename} に保存されました。\")\n",
    "        return filename\n",
    "    else:\n",
    "        logging.error(f\"Error in synthesis: {synthesis_response.text}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "# テキストを句単位に区切る\n",
    "def preprocess_text(text):\n",
    "    # テキストの前処理\n",
    "    text = re.sub(r\"[。．.]\", \"。\\n\", text)\n",
    "    text = re.sub(r\"[？?]\", \"？\\n\", text)\n",
    "    text = re.sub(r\"[！!]\", \"！\\n\", text)\n",
    "    return text\n",
    "\n",
    "# テキストを句ごとに音声合成してストリーミング\n",
    "def synthesize_streaming(text, speaker):\n",
    "    # テキストを句単位に区切る\n",
    "    logging.debug(\"テキストを句単位に区切ります。\")\n",
    "    text = preprocess_text(text)\n",
    "    sentences = text.split(\"\\n\")\n",
    "\n",
    "    # 句ごとに音声合成\n",
    "    for sentence in sentences:\n",
    "        if sentence == \"\": continue\n",
    "        \n",
    "        ## クエリ\n",
    "        query_response = requests.post(\n",
    "            f'http://localhost:50021/audio_query', \n",
    "            params={'text': sentence, 'speaker': speaker}\n",
    "        )\n",
    "        if query_response.status_code != 200:\n",
    "            logging.error(f\"Error in audio_query: {query_response.text}\")\n",
    "            return\n",
    "        \n",
    "        ## 音声合成\n",
    "        logging.debug(\"音声データを生成します。\")\n",
    "        with requests.post(\n",
    "            f'http://localhost:50021/synthesis', \n",
    "            params={'speaker': speaker}, \n",
    "            json=query_response.json(), \n",
    "            stream=True\n",
    "        ) as synthesis_response:\n",
    "            if synthesis_response.status_code != 200:\n",
    "                logging.error(f\"Error in synthesis: {synthesis_response.text}\")\n",
    "                return\n",
    "            yield \"---start---\\n\".encode(\"utf-8\")\n",
    "            for chunk in synthesis_response.iter_content(chunk_size=1024):\n",
    "                logging.info(\"チャンク生成\")\n",
    "                yield chunk\n",
    "            yield \"---end---\\n\".encode(\"utf-8\")\n",
    "\n",
    "            time.sleep(0.2)\n",
    "        \n",
    "    \n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    logging.info(\"#####アプリケーションを起動します。#####\")\n",
    "    socketio.run(app, debug=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing static/index12.html\n"
     ]
    }
   ],
   "source": [
    "%%writefile static/index12.html\n",
    "<html lang=\"ja\">\n",
    "  <head>\n",
    "    <meta charset=\"UTF-8\" />\n",
    "    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\" />\n",
    "    <title>WAV録音＆アップロード</title>\n",
    "    <!-- Recorder.js を読み込む -->\n",
    "    <script src=\"https://cdn.jsdelivr.net/gh/mattdiamond/Recorderjs@master/dist/recorder.js\"></script>\n",
    "\n",
    "    <!-- Socket.IO を読み込む -->\n",
    "    <script src=\"https://cdn.socket.io/4.0.0/socket.io.min.js\"></script>\n",
    "\n",
    "    <!-- marked.js を読み込む -->\n",
    "    <script src=\"https://cdn.jsdelivr.net/npm/marked/marked.min.js\"></script>\n",
    "\n",
    "    <!-- cssの適用-->\n",
    "    <link rel=\"stylesheet\" href=\"/static/voicechatapp.css\" />\n",
    "  </head>\n",
    "\n",
    "  <body>\n",
    "    <h1>WAV録音＆アップロード</h1>\n",
    "    <button id=\"startRecording\">録音開始</button>\n",
    "    <button id=\"stopRecording\">録音停止</button>\n",
    "    <button id=\"stopRecordingWithStreaming\">停止とストリーミング処理</button>\n",
    "    <select id=\"h_speakerSelect\"></select>\n",
    "    <button id=\"speakerTest\">音声テスト</button>\n",
    "    <div id=\"h_chatlog\"></div>\n",
    "\n",
    "    <script>\n",
    "      navigator.mediaDevices\n",
    "        .getUserMedia({ audio: true })\n",
    "        .then((stream) => {\n",
    "          window.stream = stream;\n",
    "        })\n",
    "        .catch((error) => {\n",
    "          console.error(\"Error accessing the microphone: \" + error);\n",
    "        });\n",
    "      let audioContext;\n",
    "      let recorder;\n",
    "      let audioBlob;\n",
    "\n",
    "      // フォーム要素取得\n",
    "      const h_startRecButton = document.getElementById(\"startRecording\");\n",
    "      const h_stopRecButton = document.getElementById(\"stopRecording\");\n",
    "      const stopRecwithStreamingButton = document.getElementById(\n",
    "        \"stopRecordingWithStreaming\"\n",
    "      );\n",
    "      const h_speakerSelect = document.getElementById(\"h_speakerSelect\");\n",
    "      const h_speakerTestButton = document.getElementById(\"speakerTest\");\n",
    "\n",
    "      // 録音開始時のボタンを無効化\n",
    "      function setBtnonStart() {\n",
    "        h_startRecButton.disabled = true;\n",
    "        h_stopRecButton.disabled = false;\n",
    "        stopRecwithStreamingButton.disabled = false;\n",
    "        h_speakerSelect.disabled = true;\n",
    "        h_speakerTestButton.disabled = true;\n",
    "      }\n",
    "\n",
    "      // 処理中のボタン無効化\n",
    "      function setBtnunderProcessing() {\n",
    "        h_startRecButton.disabled = true;\n",
    "        h_stopRecButton.disabled = true;\n",
    "        stopRecwithStreamingButton.disabled = true;\n",
    "        h_speakerSelect.disabled = true;\n",
    "        h_speakerTestButton.disabled = true;\n",
    "      }\n",
    "\n",
    "      // 復帰時のボタン有効化\n",
    "      function setBtnonRestart() {\n",
    "        h_startRecButton.disabled = false;\n",
    "        h_stopRecButton.disabled = true;\n",
    "        stopRecwithStreamingButton.disabled = true;\n",
    "        h_speakerSelect.disabled = false;\n",
    "        h_speakerTestButton.disabled = false;\n",
    "      }\n",
    "\n",
    "      // 音声合成のストリーミング処理\n",
    "      async function playSnetence(chunks, audioContext) {\n",
    "        const combined = new Uint8Array(\n",
    "          chunks.reduce((acc, chunk) => [...acc, ...chunk], [])\n",
    "        );\n",
    "        const audioBuffer = await audioContext.decodeAudioData(combined.buffer);\n",
    "        const source = audioContext.createBufferSource();\n",
    "        source.buffer = audioBuffer;\n",
    "        source.connect(audioContext.destination);\n",
    "        source.start();\n",
    "\n",
    "        await new Promise((resolve) => {\n",
    "          source.onended = resolve;\n",
    "        });\n",
    "      }\n",
    "\n",
    "      document.addEventListener(\"DOMContentLoaded\", () => {\n",
    "        const socket = io();\n",
    "\n",
    "        // SpeakerIDリストを取得\n",
    "        fetch(\"/speaker_ids\")\n",
    "          .then((response) => response.json())\n",
    "          .then((data) => {\n",
    "            h_speakerSelect.innerHTML = data.join(\"\");\n",
    "          });\n",
    "\n",
    "        // 音声認識の結果を受信\n",
    "        socket.on(\"SpeechRecognition\", (data) => {\n",
    "          const markdownText = data.text;\n",
    "          const htmlContent = marked.parse(markdownText);\n",
    "          document.getElementById(\n",
    "            \"h_chatlog\"\n",
    "          ).innerHTML += `<div class=\"user\">${htmlContent}</div>`;\n",
    "        });\n",
    "\n",
    "        // AIの応答を受信したときの処理\n",
    "        socket.on(\"ai_response\", (data) => {\n",
    "          const markdownText = data.ai_response;\n",
    "          const htmlContent = marked.parse(markdownText);\n",
    "          document.getElementById(\n",
    "            \"h_chatlog\"\n",
    "          ).innerHTML += `<div class=\"assistant\">${htmlContent}</div>`;\n",
    "        });\n",
    "\n",
    "        // 音声ファイルを再生する処理\n",
    "        socket.on(\"play_audio\", (data) => {\n",
    "          const audio = new Audio(`/audio/${data.audio}`);\n",
    "          audio.play();\n",
    "        });\n",
    "\n",
    "        //Stremingで音声合成の結果を受信\n",
    "        socket.on(\"Streaming\", async (data) => {\n",
    "          const reader = data.Response.body.getReader();\n",
    "          const audioContext = new AudioContext();\n",
    "          let chunks = [];\n",
    "          let proccesing = false;\n",
    "\n",
    "          while (true) {\n",
    "            const { done, value } = await reader.read();\n",
    "            if (done) break;\n",
    "            const textChunk = new TextDecorder(\"utf-8\").decode(value);\n",
    "            if (textChunk.include(\"---start---\")) {\n",
    "              if (chunks.length > 0 && !proccesing) {\n",
    "                proccesing = true;\n",
    "                await playSnetence(chunks, audioContext);\n",
    "                chunks = [];\n",
    "                proccesing = false;\n",
    "              }\n",
    "            } else if (textChunk.include(\"---end---\")) {\n",
    "              proccesing = true;\n",
    "              await playSnetence(chunks, audioContext);\n",
    "              chunks = [];\n",
    "              proccesing = false;\n",
    "            } else {\n",
    "              chunks.push(value);\n",
    "            }\n",
    "          }\n",
    "        });\n",
    "\n",
    "        // Spaceキーが押されたときにstartRecordingボタンをクリック\n",
    "        document.addEventListener(\"keydown\", (event) => {\n",
    "          if (h_startRecButton.disabled) {\n",
    "            console.log(\"処理中のため入力はできません\");\n",
    "            return;\n",
    "          }\n",
    "          if (event.code === \"Space\" && !event.repeat) {\n",
    "            h_startRecButton.click();\n",
    "          }\n",
    "        });\n",
    "\n",
    "        // Spaceキーから指が離されたときにstopRecordingボタンをクリック\n",
    "        document.addEventListener(\"keyup\", (event) => {\n",
    "          if (h_stopRecButton.disabled) {\n",
    "            console.log(\"不正な録音停止操作です\");\n",
    "            return;\n",
    "          }\n",
    "          if (event.code === \"Space\" && !event.repeat) {\n",
    "            h_stopRecButton.click();\n",
    "          }\n",
    "        });\n",
    "\n",
    "        //Speakerの音声確認テスト\n",
    "        h_speakerTestButton.addEventListener(\"click\", () => {\n",
    "          const speaker = h_speakerSelect.value;\n",
    "          fetch(\"/speaker_test\", {\n",
    "            method: \"POST\",\n",
    "            headers: {\n",
    "              \"Content-Type\": \"application/json\",\n",
    "            },\n",
    "            body: JSON.stringify({ speaker }),\n",
    "          })\n",
    "            .then((response) => response.json())\n",
    "            .then((data) => {\n",
    "              const audio = new Audio(`/audio/${data.audio}`);\n",
    "              audio.play();\n",
    "            });\n",
    "        });\n",
    "\n",
    "        // 録音開始ボタンがクリックされたときの処理\n",
    "        h_startRecButton.addEventListener(\"click\", () => {\n",
    "          audioContext = new AudioContext();\n",
    "          const source = audioContext.createMediaStreamSource(window.stream);\n",
    "          recorder = new Recorder(source, { numChannels: 1 }); // モノラル録音\n",
    "          recorder.record();\n",
    "\n",
    "          // ボタンを無効化\n",
    "          setBtnonStart();\n",
    "        });\n",
    "\n",
    "        // 録音停止ボタンがクリックされたときの処理\n",
    "        h_stopRecButton.addEventListener(\"click\", () => {\n",
    "          // ボタンを無効化\n",
    "          setBtnunderProcessing();\n",
    "\n",
    "          // 録音を停止\n",
    "          recorder.stop();\n",
    "\n",
    "          // 録音した音声をファイルに保存して送信\n",
    "          recorder.exportWAV((blob) => {\n",
    "            audioBlob = blob;\n",
    "\n",
    "            if (!audioBlob) {\n",
    "              console.error(\"No audio to upload\");\n",
    "              return;\n",
    "            }\n",
    "\n",
    "            const formData = new FormData();\n",
    "            formData.append(\"file\", audioBlob, \"recorded_audio.wav\");\n",
    "\n",
    "            const speaker = h_speakerSelect.value;\n",
    "            formData.append(\"speaker\", speaker);\n",
    "\n",
    "            fetch(\"/upload\", {\n",
    "              method: \"POST\",\n",
    "              body: formData,\n",
    "            })\n",
    "              .then((response) => response.json())\n",
    "              .then((data) => {\n",
    "                console.log(data);\n",
    "                // ボタン状態の初期化\n",
    "                setBtnonRestart();\n",
    "              })\n",
    "              .catch((error) => {\n",
    "                console.error(\"Upload failed:\");\n",
    "                // ボタン状態の初期化\n",
    "                setBtnonRestart();\n",
    "              });\n",
    "          });\n",
    "        });\n",
    "\n",
    "        stopRecwithStreamingButton.addEventListener(\"click\", () => {\n",
    "          // ボタンを無効化\n",
    "          setBtnunderProcessing();\n",
    "\n",
    "          // 録音を停止\n",
    "          recorder.stop();\n",
    "\n",
    "          // 録音した音声をファイルに保存して送信\n",
    "          recorder.exportWAV((blob) => {\n",
    "            audioBlob = blob;\n",
    "\n",
    "            if (!audioBlob) {\n",
    "              console.error(\"No audio to upload\");\n",
    "              return;\n",
    "            }\n",
    "\n",
    "            const formData = new FormData();\n",
    "            formData.append(\"file\", audioBlob, \"recorded_audio.wav\");\n",
    "\n",
    "            const speaker = h_speakerSelect.value;\n",
    "            formData.append(\"speaker\", speaker);\n",
    "\n",
    "            fetch(\"/streaming\", {\n",
    "              method: \"POST\",\n",
    "              body: formData,\n",
    "            })\n",
    "              .then((response) => response.json())\n",
    "              .then((data) => {\n",
    "                console.log(data);\n",
    "                // ボタン状態の初期化\n",
    "                setBtnonRestart();\n",
    "              })\n",
    "              .catch((error) => {\n",
    "                console.error(\"Upload failed:\");\n",
    "                // ボタン状態の初期化\n",
    "                setBtnonRestart();\n",
    "              });\n",
    "          });\n",
    "        });\n",
    "\n",
    "        // ボタン状態の初期化\n",
    "        setBtnonRestart();\n",
    "      });\n",
    "    </script>\n",
    "  </body>\n",
    "</html>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "まだストリーム再生はできていない．\n",
    "けど，ソケット通信でPush型で動くようにした．\n",
    "プログラム的にはだいぶ綺麗にはなったとは思う．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## その１３　改めて音声をストリームで受け取る形にする\n",
    "GPTの出力を文ごとに切り分けて，1文ずつVoice Voxに送り，合成された音声を順次mp３でクライアントに送る．\n",
    "クライアントでは受け取った音声を順次キューに入れていく．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile voicechatapp13.py\n",
    "\n",
    "from flask import Flask, request, Response, jsonify, send_from_directory, send_file, stream_with_context\n",
    "from flask_cors import CORS\n",
    "from flask_socketio import SocketIO, emit\n",
    "import os\n",
    "import speech_recognition as sr\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "import requests\n",
    "import time\n",
    "import logging\n",
    "import re\n",
    "from pydub import AudioSegment\n",
    "from io import BytesIO\n",
    "\n",
    "#　環境変数の読み込み\n",
    "load_dotenv()\n",
    "\n",
    "# VoiceVox APIのエンドポイント\n",
    "VOICEVOX_API_URL = \"http://localhost:50021\"\n",
    "\n",
    "\n",
    "# Flaskアプリケーションの作成\n",
    "app = Flask(__name__, static_folder=\"static\")  \n",
    "\n",
    "# CORSの設定\n",
    "CORS(app)\n",
    "\n",
    "# Socket.IOの設定\n",
    "socketio = SocketIO(app)\n",
    "\n",
    "# 会話ログを保持する変数\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}\n",
    "]\n",
    "\n",
    "#loggingの設定\n",
    "logging.basicConfig(\n",
    "    level=logging.DEBUG,\n",
    "    format=\"%(asctime)s [%(levelname)s] %(message)s\",\n",
    "    filename=\"app.log\",\n",
    "    encoding=\"utf-8\"\n",
    ")\n",
    "\n",
    "#--------------------------------------------------\n",
    "# Flaskのエンドポイントの作成\n",
    "#--------------------------------------------------\n",
    "\n",
    "# ルートパスへのリクエストを処理する\n",
    "@app.route(\"/\")\n",
    "def index():\n",
    "    logging.info(\"index.html を返します。\")\n",
    "    return send_from_directory(\"static\", \"index13.html\")\n",
    "\n",
    "# VoiceVoxのSpeakerIDリストを取得するエンドポイント\n",
    "@app.route(\"/speaker_ids\")\n",
    "def get_speaker_ids():\n",
    "    url = f\"{VOICEVOX_API_URL}/speakers\"  # VOICEVOX APIのエンドポイント\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error: {e}\")\n",
    "        return jsonify([])\n",
    "\n",
    "    voicevox_speakers = []\n",
    "    if response.status_code == 200:\n",
    "        speakers = response.json()\n",
    "        for speaker in speakers:\n",
    "            name = speaker['name']\n",
    "            style_names = [style['name'] for style in speaker['styles']]\n",
    "            style_ids = [style['id'] for style in speaker['styles']]\n",
    "            for style_id, style_name in zip(style_ids, style_names):\n",
    "                voicevox_speakers.append(f\"<option value={style_id}>Speaker: {name}, {style_name} </option>\")\n",
    "        logging.info(\"speaker_ids を取得しました。\")\n",
    "        return jsonify(voicevox_speakers)\n",
    "    else:\n",
    "        logging.error(f\"Error: {response.status_code}\")\n",
    "        return jsonify([])    \n",
    "\n",
    "# VoiceVoxの音声テストを行うエンドポイント\n",
    "@app.route(\"/speaker_test\" , methods=[\"POST\"])\n",
    "def speaker_test():\n",
    "    speaker = request.json[\"speaker\"]\n",
    "    text = \"こんにちは．初めまして．何かお手伝いできることはありますか？\"\n",
    "    synthesize_response = synthesize_voice(ai_response, speaker)\n",
    "\n",
    "    # 合成した音声をmp3化\n",
    "    if synthesize_response is None: return jsonify({\"error\": \"Failed to synthesize voice\"}), 400\n",
    "    audio = AudioSegment.from_file(BytesIO(synthesize_response.content), format=\"wav\")\n",
    "    mp3_data  = BytesIO()\n",
    "    audio.export(mp3_data , format=\"mp3\")\n",
    "    mp3_data .seek(0)  \n",
    "\n",
    "    # mp3データをWebSocketを通じてクライアントに通知\n",
    "    socketio.emit('play_audio', {'audio': mp3_data.getvalue()})\n",
    "    return jsonify({\"info\": \"Speaker Test Process Succeeded\"}), 200\n",
    "\n",
    "\n",
    "# /upload へのリクエストを処理する\n",
    "@app.route(\"/upload\", methods=[\"POST\"])\n",
    "def upload_audio():\n",
    "    # uploads ディレクトリがなければ作成\n",
    "    if not os.path.exists(\"uploads\"):\n",
    "        os.makedirs(\"uploads\")\n",
    "    # uploads ディレクトリにファイルがあれば削除\n",
    "    else:\n",
    "        for file in os.listdir(\"uploads\"):\n",
    "            os.remove(os.path.join(\"uploads\", file))\n",
    "\n",
    "    # 音声ファイルをアップロード\n",
    "    if \"file\" not in request.files:\n",
    "        logging.error(\"No audio file provided\")\n",
    "        return jsonify({\"error\": \"No audio file provided\"}), 400\n",
    "    \n",
    "    audio_file = request.files[\"file\"]\n",
    "    audio_path = os.path.join(\"uploads\", f\"input_{len(messages)}.wav\") #Uploadされたファイルを残すならこっちをOn\n",
    "    audio_file.save(audio_path)\n",
    "\n",
    "    # 音声認識\n",
    "    text = recognize_speech(audio_path)\n",
    "    ## 音声認識の結果をWebSocketを通じてクライアントに通知\n",
    "    if text:\n",
    "        socketio.emit(\"SpeechRecognition\",{\"text\": text})\n",
    "    else:\n",
    "        return jsonify({\"error\": \"Failed to recognize speech\"}), 400    \n",
    "    \n",
    "    # AIの応答を取得\n",
    "    ai_response = get_ai_response(text)\n",
    "    ## WebSocketを通じてクライアントに通知\n",
    "    if ai_response:\n",
    "        socketio.emit('ai_response', {'ai_response': ai_response}) #, 'audio': filename})\n",
    "    else:\n",
    "        return jsonify({\"error\": \"Failed to get AI response\"}), 400\n",
    "\n",
    "    # AIの応答から音声合成\n",
    "    speaker = request.form[\"speaker\"]\n",
    "    synthesize_response = synthesize_voice(ai_response, speaker)\n",
    "\n",
    "\n",
    "    # 合成した音声をmp3化\n",
    "    if synthesize_response is None: return jsonify({\"error\": \"Failed to synthesize voice\"}), 400\n",
    "    audio = AudioSegment.from_file(BytesIO(synthesize_response.content), format=\"wav\")\n",
    "    mp3_data  = BytesIO()\n",
    "    audio.export(mp3_data , format=\"mp3\")\n",
    "    mp3_data .seek(0)  \n",
    "\n",
    "    # mp3データをWebSocketを通じてクライアントに通知\n",
    "    socketio.emit('play_audio', {'audio': mp3_data.getvalue()})\n",
    "    return jsonify({\"info\": \"Uploard Process Succeeded\"}), 200\n",
    "\n",
    "\n",
    "\n",
    "# streaming処理するエンドポイント\n",
    "@app.route(\"/streaming\", methods=[\"POST\"])\n",
    "def streaming():\n",
    "    # uploads ディレクトリがなければ作成\n",
    "    if not os.path.exists(\"uploads\"):\n",
    "        os.makedirs(\"uploads\")\n",
    "    # uploads ディレクトリにファイルがあれば削除\n",
    "    else:\n",
    "        for file in os.listdir(\"uploads\"):\n",
    "            os.remove(os.path.join(\"uploads\", file))\n",
    "\n",
    "    # 音声ファイルをアップロード\n",
    "    if \"file\" not in request.files:\n",
    "        logging.error(\"No audio file provided\")\n",
    "        return jsonify({\"error\": \"No audio file provided\"}), 400\n",
    "    \n",
    "    audio_file = request.files[\"file\"]\n",
    "    audio_path = os.path.join(\"uploads\", \"input.wav\") #Uploadされたファイルを残さないならこっちをOn\n",
    "    audio_file.save(audio_path)\n",
    "\n",
    "    # 音声認識\n",
    "    text = recognize_speech(audio_path)\n",
    "    ## 音声認識の結果をWebSocketを通じてクライアントに通知\n",
    "    if text:\n",
    "        socketio.emit(\"SpeechRecognition\",{\"text\": text})\n",
    "    else:\n",
    "        return jsonify({\"error\": \"Failed to recognize speech\"}), 400    \n",
    "    \n",
    "    # AIの応答を取得\n",
    "    ai_response = get_ai_response(text)\n",
    "    ## WebSocketを通じてクライアントに通知\n",
    "    if ai_response:\n",
    "        socketio.emit('ai_response', {'ai_response': ai_response}) #, 'audio': filename})\n",
    "    else:\n",
    "        return jsonify({\"error\": \"Failed to get AI response\"}), 400\n",
    "    \n",
    "\n",
    "    ## 音声合成\n",
    "    speaker = request.form[\"speaker\"]\n",
    "\n",
    "    # テキストを句単位に区切る\n",
    "    logging.debug(\"テキストを句単位に区切ります。\")\n",
    "    text = preprocess_text(ai_response)\n",
    "    sentences = text.split(\"\\n\")\n",
    "\n",
    "    # 句ごとに音声合成\n",
    "    for sentence in sentences:\n",
    "        if sentence == \"\": continue\n",
    "        \n",
    "        ## 音声合成\n",
    "        synthesize_response=synthesize_voice(sentence, speaker)\n",
    "        if synthesize_response is None: return jsonify({\"error\": \"Failed to synthesize voice\"}), 400\n",
    "\n",
    "        ## 合成した音声をmp3化\n",
    "        audio = AudioSegment.from_file(BytesIO(synthesize_response.content), format=\"wav\")\n",
    "        mp3_data  = BytesIO()\n",
    "        audio.export(mp3_data , format=\"mp3\")\n",
    "        mp3_data .seek(0)\n",
    "\n",
    "        ## mp3データをWebSocketを通じてクライアントに通知 ここでうまくキューに入れて連続再生させたい\n",
    "        socketio.emit('play_audio', {'audio': mp3_data.getvalue()})\n",
    "        time.sleep(0.5)\n",
    "\n",
    "    \n",
    "    return jsonify({\"info\": \"Process Succeeded\"}), 200\n",
    "\n",
    "\n",
    "#--------------------------------------------------\n",
    "# Flaskの各エンドポイント内の処理関数\n",
    "#--------------------------------------------------\n",
    "# 音声認識を行う関数\n",
    "def recognize_speech(audio_path):\n",
    "    r = sr.Recognizer()\n",
    "    with sr.AudioFile(audio_path) as source:\n",
    "        audio = r.record(source)\n",
    "        text = r.recognize_google(audio, language=\"ja-JP\")\n",
    "    return text\n",
    "\n",
    "# OpenAIのAPIを呼び出してAIの応答を取得する関数\n",
    "def get_ai_response(text):\n",
    "    client = OpenAI()\n",
    "    messages.append({\"role\": \"user\", \"content\": text})\n",
    "    completion = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=messages\n",
    "    )\n",
    "    ai_response = completion.choices[0].message.content\n",
    "    messages.append({\"role\": \"assistant\", \"content\": ai_response})\n",
    "    logging.info(f\"AIの応答: {ai_response}\")\n",
    "    return ai_response\n",
    "\n",
    "\n",
    "# VoiceVox APIで音声合成を行なう関数\n",
    "def synthesize_voice(text, speaker):\n",
    "    # 1. テキストから音声合成のためのクエリを作成\n",
    "    query_payload = {'text': text, 'speaker': speaker}\n",
    "    query_response = requests.post(f'{VOICEVOX_API_URL}/audio_query', params=query_payload)\n",
    "\n",
    "    if query_response.status_code != 200:\n",
    "        logging.error(f\"Error in audio_query: {query_response.text}\")\n",
    "        print(f\"Error in audio_query: {query_response.text}\")\n",
    "        return\n",
    "\n",
    "    query = query_response.json()\n",
    "\n",
    "    # 2. クエリを元に音声データを生成\n",
    "    synthesis_payload = {'speaker': speaker}\n",
    "    synthesis_response = requests.post(f'{VOICEVOX_API_URL}/synthesis', params=synthesis_payload, json=query)\n",
    "\n",
    "    if synthesis_response.status_code == 200:\n",
    "        logging.info(\"音声データを生成しました。\")\n",
    "        return synthesis_response\n",
    "    else:\n",
    "        logging.error(f\"Error in synthesis: {synthesis_response.text}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "# テキストを句単位に区切る\n",
    "def preprocess_text(text):\n",
    "    # テキストの前処理\n",
    "    text = re.sub(r\"[。．.]\", \"。\\n\", text)\n",
    "    text = re.sub(r\"[？?]\", \"？\\n\", text)\n",
    "    text = re.sub(r\"[！!]\", \"！\\n\", text)\n",
    "    return text\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    logging.info(\"#####アプリケーションを起動します。#####\")\n",
    "    socketio.run(app, debug=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "html"
    }
   },
   "outputs": [],
   "source": [
    "%%witefile /static/index13.html\n",
    "<html lang=\"ja\">\n",
    "  <head>\n",
    "    <meta charset=\"UTF-8\" />\n",
    "    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\" />\n",
    "    <title>WAV録音アップロード</title>\n",
    "    <!-- Recorder.js を読み込む -->\n",
    "    <script src=\"https://cdn.jsdelivr.net/gh/mattdiamond/Recorderjs@master/dist/recorder.js\"></script>\n",
    "\n",
    "    <!-- Socket.IO を読み込む -->\n",
    "    <script src=\"https://cdn.socket.io/4.0.0/socket.io.min.js\"></script>\n",
    "\n",
    "    <!-- marked.js を読み込む -->\n",
    "    <script src=\"https://cdn.jsdelivr.net/npm/marked/marked.min.js\"></script>\n",
    "\n",
    "    <!-- cssの適用-->\n",
    "    <link rel=\"stylesheet\" href=\"/static/voicechatapp.css\" />\n",
    "  </head>\n",
    "\n",
    "  <body>\n",
    "    <h1>WAV録音アップロード</h1>\n",
    "    <button id=\"startRecording\">録音開始</button>\n",
    "    <button id=\"stopRecording\">録音停止</button>\n",
    "    <input type=\"radio\" id=\"streaming\" name=\"Method\" value=\"/upload\" checked>まとめて再生(基本)</radio>\n",
    "    <input type=\"radio\" id=\"streaming\" name=\"Method\" value=\"/streaming\" >ストリーミング</radio>\n",
    "    <select id=\"h_speakerSelect\"></select>\n",
    "    <button id=\"speakerTest\">音声テスト</button>\n",
    "    <div id=\"h_chatlog\"></div>\n",
    "\n",
    "    <script>\n",
    "      navigator.mediaDevices\n",
    "        .getUserMedia({ audio: true })\n",
    "        .then((stream) => {\n",
    "          window.stream = stream;\n",
    "        })\n",
    "        .catch((error) => {\n",
    "          console.error(\"Error accessing the microphone: \" + error);\n",
    "        });\n",
    "\n",
    "      let audioContext;\n",
    "      let recorder;\n",
    "      let audioBlob;\n",
    "      let audioQueue = [];\n",
    "      let isPlaying = false;\n",
    "\n",
    "      // フォーム要素取得\n",
    "      const h_startRecButton = document.getElementById(\"startRecording\");\n",
    "      const h_stopRecButton = document.getElementById(\"stopRecording\");\n",
    "      const h_speakerSelect = document.getElementById(\"h_speakerSelect\");\n",
    "      const h_speakerTestButton = document.getElementById(\"speakerTest\");\n",
    "\n",
    "      // 録音開始時のボタンを無効化\n",
    "      function setBtnonStart() {\n",
    "        h_startRecButton.disabled = true;\n",
    "        h_stopRecButton.disabled = false;\n",
    "        h_speakerSelect.disabled = true;\n",
    "        h_speakerTestButton.disabled = true;\n",
    "      }\n",
    "\n",
    "      // 処理中のボタン無効化\n",
    "      function setBtnunderProcessing() {\n",
    "        h_startRecButton.disabled = true;\n",
    "        h_stopRecButton.disabled = true;\n",
    "        h_speakerSelect.disabled = true;\n",
    "        h_speakerTestButton.disabled = true;\n",
    "      }\n",
    "\n",
    "      // 復帰時のボタン有効化\n",
    "      function setBtnonRestart() {\n",
    "        h_startRecButton.disabled = false;\n",
    "        h_stopRecButton.disabled = true;\n",
    "        h_speakerSelect.disabled = false;\n",
    "        h_speakerTestButton.disabled = false;\n",
    "      }\n",
    "\n",
    "\n",
    "      document.addEventListener(\"DOMContentLoaded\", () => {\n",
    "        const socket = io();\n",
    "\n",
    "        // SpeakerIDリストを取得\n",
    "        fetch(\"/speaker_ids\")\n",
    "          .then((response) => response.json())\n",
    "          .then((data) => {\n",
    "            h_speakerSelect.innerHTML = data.join(\"\");\n",
    "          });\n",
    "\n",
    "        // 音声認識の結果を受信\n",
    "        socket.on(\"SpeechRecognition\", (data) => {\n",
    "          const markdownText = data.text;\n",
    "          const htmlContent = marked.parse(markdownText);\n",
    "          document.getElementById(\n",
    "            \"h_chatlog\"\n",
    "          ).innerHTML += `<div class=\"user\">${htmlContent}</div>`;\n",
    "        });\n",
    "\n",
    "        // AIの応答を受信したときの処理\n",
    "        socket.on(\"ai_response\", (data) => {\n",
    "          const markdownText = data.ai_response;\n",
    "          const htmlContent = marked.parse(markdownText);\n",
    "          document.getElementById(\n",
    "            \"h_chatlog\"\n",
    "          ).innerHTML += `<div class=\"assistant\">${htmlContent}</div>`;\n",
    "        });\n",
    "\n",
    "        // 音声ファイルを再生する処理\n",
    "        socket.on(\"play_audio\", async(data) => {\n",
    "            const audioBlob = new Blob([data.audio], { type: \"audio/mp3\" });\n",
    "            const audioUrl = URL.createObjectURL(audioBlob);\n",
    "\n",
    "            // キューに登録\n",
    "            audioQueue.push(audioUrl);\n",
    "\n",
    "            // 再生中でなければ再生\n",
    "            if (!isPlaying) {\n",
    "                playAudio();\n",
    "            }\n",
    "            // const audio = new Audio(audioUrl);\n",
    "            // audio.play();\n",
    "        });\n",
    "\n",
    "        // Queueに登録された音声ファイルを再生する処理\n",
    "        async function playAudio() {\n",
    "            // 再生する音声ファイルがなければ終了\n",
    "            if (audioQueue.length === 0) {\n",
    "                isPlaying = false;\n",
    "                return;\n",
    "            }\n",
    "\n",
    "            isPlaying = true;\n",
    "            const audioUrl = audioQueue.shift();\n",
    "            const audio = new Audio(audioUrl);\n",
    "            audio.play();\n",
    "\n",
    "            // 再生が終了したら次の音声ファイルを再生\n",
    "            audio.onended = () => {\n",
    "                playAudio();\n",
    "            };\n",
    "        }\n",
    "\n",
    "        // //Stremingで音声合成の結果を受信\n",
    "        // socket.on(\"Streaming\", async (data) => {\n",
    "\n",
    "        // });\n",
    "\n",
    "        // Spaceキーが押されたときにstartRecordingボタンをクリック\n",
    "        document.addEventListener(\"keydown\", (event) => {\n",
    "          if (h_startRecButton.disabled) {\n",
    "            console.log(\"処理中のため入力はできません\");\n",
    "            return;\n",
    "          }\n",
    "          if (event.code === \"Space\" && !event.repeat) {\n",
    "            h_startRecButton.click();\n",
    "          }\n",
    "        });\n",
    "\n",
    "        // Spaceキーから指が離されたときにstopRecordingボタンをクリック\n",
    "        document.addEventListener(\"keyup\", (event) => {\n",
    "          if (h_stopRecButton.disabled) {\n",
    "            console.log(\"不正な録音停止操作です\");\n",
    "            return;\n",
    "          }\n",
    "          if (event.code === \"Space\" && !event.repeat) {\n",
    "            h_stopRecButton.click();\n",
    "          }\n",
    "        });\n",
    "\n",
    "        //Speakerの音声確認テスト\n",
    "        h_speakerTestButton.addEventListener(\"click\", () => {\n",
    "          const speaker = h_speakerSelect.value;\n",
    "          fetch(\"/speaker_test\", {\n",
    "            method: \"POST\",\n",
    "            headers: {\n",
    "              \"Content-Type\": \"application/json\",\n",
    "            },\n",
    "            body: JSON.stringify({ speaker }),\n",
    "          })\n",
    "            .then((response) => response.json())\n",
    "            .then((data) => {\n",
    "              console.log(data);\n",
    "            });\n",
    "        });\n",
    "\n",
    "        // 録音開始ボタンがクリックされたときの処理\n",
    "        h_startRecButton.addEventListener(\"click\", () => {\n",
    "          audioContext = new AudioContext();\n",
    "          const source = audioContext.createMediaStreamSource(window.stream);\n",
    "          recorder = new Recorder(source, { numChannels: 1 }); // モノラル録音\n",
    "          recorder.record();\n",
    "\n",
    "          // ボタンを無効化\n",
    "          setBtnonStart();\n",
    "        });\n",
    "\n",
    "        // 録音停止ボタンがクリックされたときの処理\n",
    "        h_stopRecButton.addEventListener(\"click\", () => {\n",
    "          // ボタンを無効化\n",
    "          setBtnunderProcessing();\n",
    "\n",
    "          // 録音を停止\n",
    "          recorder.stop();\n",
    "\n",
    "          // 録音した音声をファイルに保存して送信\n",
    "          recorder.exportWAV((blob) => {\n",
    "            audioBlob = blob;\n",
    "            if (!audioBlob) {\n",
    "              console.error(\"No audio to upload\");\n",
    "              return;\n",
    "            }\n",
    "\n",
    "            const formData = new FormData();\n",
    "            formData.append(\"file\", audioBlob, \"recorded_audio.wav\");\n",
    "\n",
    "            const speaker = h_speakerSelect.value;\n",
    "            formData.append(\"speaker\", speaker);\n",
    "\n",
    "            const method = document.querySelector('input[name=\"Method\"]:checked').value;\n",
    "\n",
    "            fetch(method, {\n",
    "              method: \"POST\",\n",
    "              body: formData,\n",
    "            })\n",
    "              .then((response) => response.json())\n",
    "              .then((data) => {\n",
    "                console.log(data);\n",
    "                // ボタン状態の初期化\n",
    "                setBtnonRestart();\n",
    "              })\n",
    "              .catch((error) => {\n",
    "                console.error(\"Upload failed:\");\n",
    "                // ボタン状態の初期化\n",
    "                setBtnonRestart();\n",
    "              });\n",
    "          });\n",
    "        });\n",
    "\n",
    "        // ボタン状態の初期化\n",
    "        setBtnonRestart();\n",
    "      });\n",
    "\n",
    "      // ページを離れるときにストリームを停止\n",
    "      window.addEventListener(\"beforeunload\", () => {\n",
    "        if (window.stream) {\n",
    "          window.stream.getTracks().forEach((track) => {\n",
    "            track.stop();\n",
    "          });\n",
    "        }\n",
    "      });\n",
    "    </script>\n",
    "  </body>\n",
    "</html>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "だいぶ苦労したけど，少しずつ形になってきた．\n",
    "このあとはGPTからの返答もストリーミングで受け取れるようにする．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## その１４　GPTからの返答をストリームで受け取る．\n",
    "これの場合，GPTからのストリーム出力を一旦キープして文末に来たところで一気に処理をかけるという処理になる．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing voicechatapp14.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile voicechatapp14.py\n",
    "\n",
    "from flask import Flask, request, Response, jsonify, send_from_directory, send_file, stream_with_context\n",
    "from flask_cors import CORS\n",
    "from flask_socketio import SocketIO, emit\n",
    "import os\n",
    "import speech_recognition as sr\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "import requests\n",
    "import time\n",
    "import logging\n",
    "import re\n",
    "from pydub import AudioSegment\n",
    "from io import BytesIO\n",
    "\n",
    "#　環境変数の読み込み\n",
    "load_dotenv()\n",
    "\n",
    "# VoiceVox APIのエンドポイント\n",
    "VOICEVOX_API_URL = \"http://localhost:50021\"\n",
    "\n",
    "\n",
    "# Flaskアプリケーションの作成\n",
    "app = Flask(__name__, static_folder=\"static\")  \n",
    "\n",
    "# CORSの設定\n",
    "CORS(app)\n",
    "\n",
    "# Socket.IOの設定\n",
    "socketio = SocketIO(app)\n",
    "\n",
    "# 会話ログを保持する変数\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}\n",
    "]\n",
    "\n",
    "#loggingの設定\n",
    "logging.basicConfig(\n",
    "    level=logging.DEBUG,\n",
    "    format=\"%(asctime)s [%(levelname)s] %(message)s\",\n",
    "    filename=\"app.log\",\n",
    "    encoding=\"utf-8\"\n",
    ")\n",
    "\n",
    "#--------------------------------------------------\n",
    "# Flaskのエンドポイントの作成\n",
    "#--------------------------------------------------\n",
    "\n",
    "# ルートパスへのリクエストを処理する\n",
    "@app.route(\"/\")\n",
    "def index():\n",
    "    logging.info(\"index.html を返します。\")\n",
    "    return send_from_directory(\"static\", \"index14.html\")\n",
    "\n",
    "# VoiceVoxのSpeakerIDリストを取得するエンドポイント\n",
    "@app.route(\"/speaker_ids\")\n",
    "def get_speaker_ids():\n",
    "    url = f\"{VOICEVOX_API_URL}/speakers\"  # VOICEVOX APIのエンドポイント\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error: {e}\")\n",
    "        return jsonify([])\n",
    "\n",
    "    voicevox_speakers = []\n",
    "    if response.status_code == 200:\n",
    "        speakers = response.json()\n",
    "        for speaker in speakers:\n",
    "            name = speaker['name']\n",
    "            style_names = [style['name'] for style in speaker['styles']]\n",
    "            style_ids = [style['id'] for style in speaker['styles']]\n",
    "            for style_id, style_name in zip(style_ids, style_names):\n",
    "                voicevox_speakers.append(f\"<option value={style_id}>Speaker: {name}, {style_name} </option>\")\n",
    "        logging.info(\"speaker_ids を取得しました。\")\n",
    "        return jsonify(voicevox_speakers)\n",
    "    else:\n",
    "        logging.error(f\"Error: {response.status_code}\")\n",
    "        return jsonify([])    \n",
    "\n",
    "# VoiceVoxの音声テストを行うエンドポイント\n",
    "@app.route(\"/speaker_test\" , methods=[\"POST\"])\n",
    "def speaker_test():\n",
    "    speaker = request.json[\"speaker\"]\n",
    "    text = \"こんにちは．初めまして．何かお手伝いできることはありますか？\"\n",
    "    synthesize_response = synthesize_voice(ai_response, speaker)\n",
    "\n",
    "    # 合成した音声をmp3化\n",
    "    if synthesize_response is None: return jsonify({\"error\": \"Failed to synthesize voice\"}), 400\n",
    "    audio = AudioSegment.from_file(BytesIO(synthesize_response.content), format=\"wav\")\n",
    "    mp3_data  = BytesIO()\n",
    "    audio.export(mp3_data , format=\"mp3\")\n",
    "    mp3_data .seek(0)  \n",
    "\n",
    "    # mp3データをWebSocketを通じてクライアントに通知\n",
    "    socketio.emit('play_audio', {'audio': mp3_data.getvalue()})\n",
    "    return jsonify({\"info\": \"Speaker Test Process Succeeded\"}), 200\n",
    "\n",
    "\n",
    "# /upload へのリクエストを処理する\n",
    "@app.route(\"/upload\", methods=[\"POST\"])\n",
    "def upload_audio():\n",
    "    # uploads ディレクトリがなければ作成\n",
    "    if not os.path.exists(\"uploads\"):\n",
    "        os.makedirs(\"uploads\")\n",
    "    # uploads ディレクトリにファイルがあれば削除\n",
    "    else:\n",
    "        for file in os.listdir(\"uploads\"):\n",
    "            os.remove(os.path.join(\"uploads\", file))\n",
    "\n",
    "    # 音声ファイルをアップロード\n",
    "    if \"file\" not in request.files:\n",
    "        logging.error(\"No audio file provided\")\n",
    "        return jsonify({\"error\": \"No audio file provided\"}), 400\n",
    "    \n",
    "    audio_file = request.files[\"file\"]\n",
    "    audio_path = os.path.join(\"uploads\", f\"input_{len(messages)}.wav\") #Uploadされたファイルを残すならこっちをOn\n",
    "    audio_file.save(audio_path)\n",
    "\n",
    "    # 音声認識\n",
    "    text = recognize_speech(audio_path)\n",
    "    ## 音声認識の結果をWebSocketを通じてクライアントに通知\n",
    "    if text:\n",
    "        socketio.emit(\"SpeechRecognition\",{\"text\": text})\n",
    "    else:\n",
    "        return jsonify({\"error\": \"Failed to recognize speech\"}), 400    \n",
    "    \n",
    "    # # AIの応答をストリームで生成\n",
    "    # socketio.emit('ai_response', {'ai_response': \"---Start---\"}) # 開始を通知\n",
    "    # for ai_response in generate_ai_response(text):\n",
    "    #     ## WebSocketを通じてクライアントに通知\n",
    "    #     if ai_response:\n",
    "    #         socketio.emit('ai_response', {'ai_response': ai_response}) \n",
    "    #     else:\n",
    "    #         return jsonify({\"error\": \"Failed to get AI response\"}), 400\n",
    "    # socketio.emit('ai_response', {'ai_response': \"---End---\"}) # 終了を通知\n",
    "\n",
    "    # AIの応答を取得\n",
    "    ai_response = get_ai_response(text)\n",
    "    ## WebSocketを通じてクライアントに通知\n",
    "    if ai_response:\n",
    "        socketio.emit('ai_response', {'ai_response': ai_response}) \n",
    "    else:\n",
    "        return jsonify({\"error\": \"Failed to get AI response\"}), 400\n",
    "    \n",
    "    # AIの応答から音声合成\n",
    "    speaker = request.form[\"speaker\"]\n",
    "    synthesize_response = synthesize_voice(ai_response, speaker)\n",
    "\n",
    "\n",
    "    # 合成した音声をmp3化\n",
    "    if synthesize_response is None: return jsonify({\"error\": \"Failed to synthesize voice\"}), 400\n",
    "    audio = AudioSegment.from_file(BytesIO(synthesize_response.content), format=\"wav\")\n",
    "    mp3_data  = BytesIO()\n",
    "    audio.export(mp3_data , format=\"mp3\")\n",
    "    mp3_data .seek(0)  \n",
    "\n",
    "    # mp3データをWebSocketを通じてクライアントに通知\n",
    "    socketio.emit('play_audio', {'audio': mp3_data.getvalue()})\n",
    "\n",
    "    return jsonify({\"info\": \"Uploard Process Succeeded\"}), 200\n",
    "\n",
    "\n",
    "\n",
    "# streaming処理するエンドポイント\n",
    "@app.route(\"/streaming\", methods=[\"POST\"])\n",
    "def streaming():\n",
    "    # uploads ディレクトリがなければ作成\n",
    "    if not os.path.exists(\"uploads\"):\n",
    "        os.makedirs(\"uploads\")\n",
    "    # uploads ディレクトリにファイルがあれば削除\n",
    "    else:\n",
    "        for file in os.listdir(\"uploads\"):\n",
    "            os.remove(os.path.join(\"uploads\", file))\n",
    "\n",
    "    # 音声ファイルをアップロード\n",
    "    if \"file\" not in request.files:\n",
    "        logging.error(\"No audio file provided\")\n",
    "        return jsonify({\"error\": \"No audio file provided\"}), 400\n",
    "    \n",
    "    audio_file = request.files[\"file\"]\n",
    "    audio_path = os.path.join(\"uploads\", \"input.wav\") #Uploadされたファイルを残さないならこっちをOn\n",
    "    audio_file.save(audio_path)\n",
    "\n",
    "    # 音声認識\n",
    "    text = recognize_speech(audio_path)\n",
    "    ## 音声認識の結果をWebSocketを通じてクライアントに通知\n",
    "    if text:\n",
    "        socketio.emit(\"SpeechRecognition\",{\"text\": text})\n",
    "    else:\n",
    "        return jsonify({\"error\": \"Failed to recognize speech\"}), 400    \n",
    "    \n",
    "\n",
    "    # AIの応答を句単位でストリームするとともに．句単位で音声合成もしていく\n",
    "    \"\"\"\n",
    "    現状では，多分句の表示と音声が同期しない．句は順に表示されていくけど，音声はキューに入って順に再生されるので．\n",
    "    これを同期させようと思うと，Javascriptに句を送ったものも一旦キューに入れて，音声と句を同時に処理するようにしないといけな\n",
    "    い．\n",
    "    できなくはないか・・・\n",
    "    \"\"\"\n",
    "    speaker = request.form[\"speaker\"]\n",
    "    socketio.emit('ai_stream', {'ai_stream': \"---Start---\"}) # 開始を通知\n",
    "    for sentence in generate_ai_response(text):\n",
    "        ## WebSocketを通じてクライアントに通知\n",
    "        if sentence:\n",
    "            # 音声合成\n",
    "            synthesize_response=synthesize_voice(sentence, speaker)\n",
    "            if synthesize_response is None: return jsonify({\"error\": \"Failed to synthesize voice\"}), 400\n",
    "            ## 合成した音声をmp3化\n",
    "            audio = AudioSegment.from_file(BytesIO(synthesize_response.content), format=\"wav\")\n",
    "            mp3_data  = BytesIO()\n",
    "            audio.export(mp3_data , format=\"mp3\")\n",
    "            mp3_data .seek(0)\n",
    "            ## mp3データをWebSocketを通じてクライアントに通知 ここでうまくキューに入れて連続再生させたい\n",
    "            socketio.emit('play_audio', {'audio': mp3_data.getvalue()})\n",
    "            socketio.emit('ai_stream', {'ai_stream': sentence})\n",
    "            ## 0.5秒の無音を入れる．これで句の切り分けが聞きやすくなると思う．\n",
    "            silent_audio = AudioSegment.silent(duration=500)\n",
    "            mp3_data  = BytesIO()\n",
    "            silent_audio.export(mp3_data , format=\"mp3\")\n",
    "            mp3_data .seek(0)\n",
    "            socketio.emit('play_audio', {'audio': mp3_data.getvalue()}) \n",
    "        else:\n",
    "            return jsonify({\"error\": \"Failed to get AI response\"}), 400\n",
    "    socketio.emit('ai_stream', {'ai_stream': \"---End---\"}) # 終了を通知\n",
    "\n",
    "    \n",
    "    return jsonify({\"info\": \"Process Succeeded\"}), 200\n",
    "\n",
    "\n",
    "#--------------------------------------------------\n",
    "# Flaskの各エンドポイント内の処理関数\n",
    "#--------------------------------------------------\n",
    "# 音声認識を行う関数\n",
    "def recognize_speech(audio_path):\n",
    "    r = sr.Recognizer()\n",
    "    with sr.AudioFile(audio_path) as source:\n",
    "        audio = r.record(source)\n",
    "        text = r.recognize_google(audio, language=\"ja-JP\")\n",
    "    return text\n",
    "\n",
    "# OpenAIのAPIを呼び出してAIの応答を取得する関数\n",
    "def get_ai_response(text):\n",
    "    client = OpenAI()\n",
    "    messages.append({\"role\": \"user\", \"content\": text})\n",
    "    completion = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=messages\n",
    "    )\n",
    "    ai_response = completion.choices[0].message.content\n",
    "    messages.append({\"role\": \"assistant\", \"content\": ai_response})\n",
    "    logging.info(f\"AIの応答: {ai_response}\")\n",
    "    return ai_response\n",
    "\n",
    "# OpenAIのAPIを呼び出してAIの応答をストリームで生成する関数\n",
    "def generate_ai_response(text):\n",
    "    client = OpenAI()\n",
    "    messages.append({\"role\": \"user\", \"content\": text})\n",
    "    completion = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=messages,\n",
    "        stream = True\n",
    "    )\n",
    "\n",
    "    sentens = \"\" # 句を構成するためのバッファ\n",
    "    message = \"\" # プロンプトに含めるためにチャンクを結合させるためのためのバッファ\n",
    "    for chunk in completion:\n",
    "        # きちんとしたチャンクが帰ってきているかのチェック\n",
    "        if \"choices\" in chunk.to_dict() and len(chunk.choices) > 0: #to_dict：辞書型に変えないと”choices”が見つからないようなので\n",
    "            content  = chunk.choices[0].delta.content\n",
    "            if content:\n",
    "                message += content\n",
    "                # 1文字ずつ取り出してチェックする\n",
    "                for i in range(len(content)):\n",
    "                    char = content[i]\n",
    "                    sentens += char\n",
    "                    if char in \"。．.？?！!\\n\": #今見ているのが区切り文字だった場合\n",
    "                        if i < len(content)-1: # i が最後の文字でないなら，次の文字をチェック\n",
    "                            if content[i+1] not in \"。．.？?！!\\n\": #次の文字が区切り文字でないならyield\n",
    "                                logging.debug(f\"句: {sentens}\")\n",
    "                                yield sentens\n",
    "                                sentens = \"\"\n",
    "                            else: #もし次の文字が区切り文字なら，現時点の区切り文字はスルー\n",
    "                                continue\n",
    "                        else: #iが最後の文字の場合，現時点でyield\n",
    "                            logging.debug(f\"句: {sentens}\")\n",
    "                            yield sentens\n",
    "                            sentens = \"\"\n",
    "    # 最後の句を返す\n",
    "    if sentens:\n",
    "        yield sentens\n",
    "    \n",
    "    # message をmessagesに追加\n",
    "    messages.append({\"role\": \"assistant\", \"content\": message})\n",
    "    logging.info(f\"AIの応答: {message}\")\n",
    "\n",
    "\n",
    "\n",
    "# VoiceVox APIで音声合成を行なう関数\n",
    "def synthesize_voice(text, speaker):\n",
    "    # 1. テキストから音声合成のためのクエリを作成\n",
    "    query_payload = {'text': text, 'speaker': speaker}\n",
    "    query_response = requests.post(f'{VOICEVOX_API_URL}/audio_query', params=query_payload)\n",
    "\n",
    "    if query_response.status_code != 200:\n",
    "        logging.error(f\"Error in audio_query: {query_response.text}\")\n",
    "        print(f\"Error in audio_query: {query_response.text}\")\n",
    "        return\n",
    "\n",
    "    query = query_response.json()\n",
    "\n",
    "    # 2. クエリを元に音声データを生成\n",
    "    synthesis_payload = {'speaker': speaker}\n",
    "    synthesis_response = requests.post(f'{VOICEVOX_API_URL}/synthesis', params=synthesis_payload, json=query)\n",
    "\n",
    "    if synthesis_response.status_code == 200:\n",
    "        logging.info(\"音声データを生成しました。\")\n",
    "        return synthesis_response\n",
    "    else:\n",
    "        logging.error(f\"Error in synthesis: {synthesis_response.text}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "# テキストを句単位に区切る\n",
    "def preprocess_text(text):\n",
    "    # テキストの前処理\n",
    "    text = re.sub(r\"[。．.]\", \"。\\n\", text)\n",
    "    text = re.sub(r\"[？?]\", \"？\\n\", text)\n",
    "    text = re.sub(r\"[！!]\", \"！\\n\", text)\n",
    "    return text\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    logging.info(\"#####アプリケーションを起動します。#####\")\n",
    "    socketio.run(app, debug=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "html"
    }
   },
   "outputs": [],
   "source": [
    "%%writefile /static/index14.html\n",
    "<html lang=\"ja\">\n",
    "  <head>\n",
    "    <meta charset=\"UTF-8\" />\n",
    "    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\" />\n",
    "    <title>WAV録音アップロード</title>\n",
    "    <!-- Recorder.js を読み込む -->\n",
    "    <script src=\"https://cdn.jsdelivr.net/gh/mattdiamond/Recorderjs@master/dist/recorder.js\"></script>\n",
    "\n",
    "    <!-- Socket.IO を読み込む -->\n",
    "    <script src=\"https://cdn.socket.io/4.0.0/socket.io.min.js\"></script>\n",
    "\n",
    "    <!-- marked.js を読み込む -->\n",
    "    <script src=\"https://cdn.jsdelivr.net/npm/marked/marked.min.js\"></script>\n",
    "\n",
    "    <!-- cssの適用-->\n",
    "    <link rel=\"stylesheet\" href=\"/static/voicechatapp.css\" />\n",
    "  </head>\n",
    "\n",
    "  <body>\n",
    "    <h1>WAV録音アップロード</h1>\n",
    "    <button id=\"startRecording\">録音開始</button>\n",
    "    <button id=\"stopRecording\">録音停止</button>\n",
    "    <input type=\"radio\" id=\"streaming\" name=\"Method\" value=\"/upload\" checked>まとめて再生(基本)</radio>\n",
    "    <input type=\"radio\" id=\"streaming\" name=\"Method\" value=\"/streaming\" >ストリーミング</radio>\n",
    "    <select id=\"h_speakerSelect\"></select>\n",
    "    <button id=\"speakerTest\">音声テスト</button>\n",
    "    <div id=\"h_chatlog\"></div>\n",
    "\n",
    "    <script>\n",
    "      navigator.mediaDevices\n",
    "        .getUserMedia({ audio: true })\n",
    "        .then((stream) => {\n",
    "          window.stream = stream;\n",
    "        })\n",
    "        .catch((error) => {\n",
    "          console.error(\"Error accessing the microphone: \" + error);\n",
    "        });\n",
    "\n",
    "      let audioContext;\n",
    "      let recorder;\n",
    "      let audioBlob;\n",
    "      let audioQueue = [];\n",
    "      let isPlaying = false;\n",
    "\n",
    "      // html要素取得\n",
    "      const h_startRecButton = document.getElementById(\"startRecording\");\n",
    "      const h_stopRecButton = document.getElementById(\"stopRecording\");\n",
    "      const h_speakerSelect = document.getElementById(\"h_speakerSelect\");\n",
    "      const h_speakerTestButton = document.getElementById(\"speakerTest\");\n",
    "      const h_chatlog = document.getElementById(\"h_chatlog\");\n",
    "\n",
    "      // 録音開始時のボタンを無効化\n",
    "      function setBtnonStart() {\n",
    "        h_startRecButton.disabled = true;\n",
    "        h_stopRecButton.disabled = false;\n",
    "        h_speakerSelect.disabled = true;\n",
    "        h_speakerTestButton.disabled = true;\n",
    "      }\n",
    "\n",
    "      // 処理中のボタン無効化\n",
    "      function setBtnunderProcessing() {\n",
    "        h_startRecButton.disabled = true;\n",
    "        h_stopRecButton.disabled = true;\n",
    "        h_speakerSelect.disabled = true;\n",
    "        h_speakerTestButton.disabled = true;\n",
    "      }\n",
    "\n",
    "      // 復帰時のボタン有効化\n",
    "      function setBtnonRestart() {\n",
    "        h_startRecButton.disabled = false;\n",
    "        h_stopRecButton.disabled = true;\n",
    "        h_speakerSelect.disabled = false;\n",
    "        h_speakerTestButton.disabled = false;\n",
    "      }\n",
    "\n",
    "\n",
    "      document.addEventListener(\"DOMContentLoaded\", () => {\n",
    "        const socket = io();\n",
    "\n",
    "        // SpeakerIDリストを取得\n",
    "        fetch(\"/speaker_ids\")\n",
    "          .then((response) => response.json())\n",
    "          .then((data) => {\n",
    "            h_speakerSelect.innerHTML = data.join(\"\");\n",
    "          });\n",
    "\n",
    "        // 音声認識の結果を受信\n",
    "        socket.on(\"SpeechRecognition\", (data) => {\n",
    "          const markdownText = data.text;\n",
    "          const htmlContent = marked.parse(markdownText);\n",
    "          h_chatlog.innerHTML += `<div class=\"user\">${htmlContent}</div>`;\n",
    "        });\n",
    "\n",
    "        // AIの応答を受信したときの処理\n",
    "        socket.on(\"ai_response\", (data) => {\n",
    "          const markdownText = data.ai_response;\n",
    "          const htmlContent = marked.parse(markdownText);\n",
    "          h_chatlog.innerHTML += `<div class=\"assistant\">${htmlContent}</div>`;\n",
    "        });\n",
    "\n",
    "        // AIの応答ストリームを受信したときの処理\n",
    "        let currentDiv=\"\";\n",
    "        socket.on(\"ai_stream\", (data) => {\n",
    "          if (data.ai_stream.includes(\"---Start---\")) { \n",
    "            // 最初はdivを作成\n",
    "            h_chatlog.innerHTML += `<div class=\"assistant\"></div>`;\n",
    "            const assistantDivs = h_chatlog.getElementsByClassName(\"assistant\");\n",
    "            currentDiv = assistantDivs[assistantDivs.length - 1];//作ったdivを取得\n",
    "            return;\n",
    "          }\n",
    "          else if (data.ai_stream.includes(\"---End---\") ){ \n",
    "            // 終了時は改めて中身をマークダウンで書き直す．\n",
    "            currentDiv.innerHTML= marked.parse(currentDiv.innerHTML);\n",
    "            currentDiv = \"\"; //初期化\n",
    "            return;\n",
    "          }\n",
    "          else{\n",
    "            // 途中の場合はnakedなテキストを追加\n",
    "            currentDiv.innerHTML += data.ai_stream;\n",
    "          }\n",
    "        });\n",
    "\n",
    "        // 音声ファイルを再生する処理\n",
    "        socket.on(\"play_audio\", async(data) => {\n",
    "            const audioBlob = new Blob([data.audio], { type: \"audio/mp3\" });\n",
    "            const audioUrl = URL.createObjectURL(audioBlob);\n",
    "\n",
    "            // キューに登録\n",
    "            audioQueue.push(audioUrl);\n",
    "\n",
    "            // 再生中でなければ再生\n",
    "            if (!isPlaying) {\n",
    "                playAudio();\n",
    "            }\n",
    "        });\n",
    "\n",
    "        // Queueに登録された音声ファイルを再生する処理\n",
    "        async function playAudio() {\n",
    "            // 再生する音声ファイルがなければ終了\n",
    "            if (audioQueue.length === 0) {\n",
    "                isPlaying = false;\n",
    "                return;\n",
    "            }\n",
    "\n",
    "            isPlaying = true;\n",
    "            const audioUrl = audioQueue.shift();\n",
    "            const audio = new Audio(audioUrl);\n",
    "            audio.play();\n",
    "\n",
    "            // 再生が終了したら次の音声ファイルを再生\n",
    "            audio.onended = () => {\n",
    "                playAudio();\n",
    "            };\n",
    "        }\n",
    "\n",
    "\n",
    "        // Spaceキーが押されたときにstartRecordingボタンをクリック\n",
    "        document.addEventListener(\"keydown\", (event) => {\n",
    "          if (h_startRecButton.disabled) {\n",
    "            console.log(\"処理中のため入力はできません\");\n",
    "            return;\n",
    "          }\n",
    "          if (event.code === \"Space\" && !event.repeat) {\n",
    "            h_startRecButton.click();\n",
    "          }\n",
    "        });\n",
    "\n",
    "        // Spaceキーから指が離されたときにstopRecordingボタンをクリック\n",
    "        document.addEventListener(\"keyup\", (event) => {\n",
    "          if (h_stopRecButton.disabled) {\n",
    "            console.log(\"不正な録音停止操作です\");\n",
    "            return;\n",
    "          }\n",
    "          if (event.code === \"Space\" && !event.repeat) {\n",
    "            h_stopRecButton.click();\n",
    "          }\n",
    "        });\n",
    "\n",
    "        //Speakerの音声確認テスト\n",
    "        h_speakerTestButton.addEventListener(\"click\", () => {\n",
    "          const speaker = h_speakerSelect.value;\n",
    "          fetch(\"/speaker_test\", {\n",
    "            method: \"POST\",\n",
    "            headers: {\n",
    "              \"Content-Type\": \"application/json\",\n",
    "            },\n",
    "            body: JSON.stringify({ speaker }),\n",
    "          })\n",
    "            .then((response) => response.json())\n",
    "            .then((data) => {\n",
    "              console.log(data);\n",
    "            });\n",
    "        });\n",
    "\n",
    "        // 録音開始ボタンがクリックされたときの処理\n",
    "        h_startRecButton.addEventListener(\"click\", () => {\n",
    "          audioContext = new AudioContext();\n",
    "          const source = audioContext.createMediaStreamSource(window.stream);\n",
    "          recorder = new Recorder(source, { numChannels: 1 }); // モノラル録音\n",
    "          recorder.record();\n",
    "\n",
    "          // ボタンを無効化\n",
    "          setBtnonStart();\n",
    "        });\n",
    "\n",
    "        // 録音停止ボタンがクリックされたときの処理\n",
    "        h_stopRecButton.addEventListener(\"click\", () => {\n",
    "          // ボタンを無効化\n",
    "          setBtnunderProcessing();\n",
    "\n",
    "          // 録音を停止\n",
    "          recorder.stop();\n",
    "\n",
    "          // 録音した音声をファイルに保存して送信\n",
    "          recorder.exportWAV((blob) => {\n",
    "            audioBlob = blob;\n",
    "            if (!audioBlob) {\n",
    "              console.error(\"No audio to upload\");\n",
    "              return;\n",
    "            }\n",
    "\n",
    "            const formData = new FormData();\n",
    "            formData.append(\"file\", audioBlob, \"recorded_audio.wav\");\n",
    "\n",
    "            const speaker = h_speakerSelect.value;\n",
    "            formData.append(\"speaker\", speaker);\n",
    "\n",
    "            const method = document.querySelector('input[name=\"Method\"]:checked').value;\n",
    "\n",
    "            fetch(method, {\n",
    "              method: \"POST\",\n",
    "              body: formData,\n",
    "            })\n",
    "              .then((response) => response.json())\n",
    "              .then((data) => {\n",
    "                console.log(data);\n",
    "                // ボタン状態の初期化\n",
    "                setBtnonRestart();\n",
    "              })\n",
    "              .catch((error) => {\n",
    "                console.error(\"Upload failed:\");\n",
    "                // ボタン状態の初期化\n",
    "                setBtnonRestart();\n",
    "              });\n",
    "          });\n",
    "        });\n",
    "\n",
    "        // ボタン状態の初期化\n",
    "        setBtnonRestart();\n",
    "      });\n",
    "\n",
    "      // ページを離れるときにストリームを停止\n",
    "      window.addEventListener(\"beforeunload\", () => {\n",
    "        if (window.stream) {\n",
    "          window.stream.getTracks().forEach((track) => {\n",
    "            track.stop();\n",
    "          });\n",
    "        }\n",
    "      });\n",
    "\n",
    "    </script>\n",
    "  </body>\n",
    "</html>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "なんとかできた．\n",
    "\n",
    "Chunkの文章への再構築と，一方で文末かどうかの区切りがめちゃくちゃ面倒やった（苦笑）\n",
    "\n",
    "現状では，音声生成のスピードがそこまでではないので，そこまでキュニー蓄積させるということもないが，もしもっと合成のスピードが上がれば，先に文章がバーっと表示されていく，ということが起こり得ると思う．\n",
    "まあ，それも悪くはないが・・・\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## その１５　音声とセンテンス表示の同期をとる．\n",
    "チャレンジしてみるか"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing voicechatapp15.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile voicechatapp15.py\n",
    "\n",
    "from flask import Flask, request, Response, jsonify, send_from_directory, send_file, stream_with_context\n",
    "from flask_cors import CORS\n",
    "from flask_socketio import SocketIO, emit\n",
    "import os\n",
    "import speech_recognition as sr\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "import requests\n",
    "import time\n",
    "import logging\n",
    "import re\n",
    "from pydub import AudioSegment\n",
    "from io import BytesIO\n",
    "\n",
    "#　環境変数の読み込み\n",
    "load_dotenv()\n",
    "\n",
    "# VoiceVox APIのエンドポイント\n",
    "VOICEVOX_API_URL = \"http://localhost:50021\"\n",
    "\n",
    "\n",
    "# Flaskアプリケーションの作成\n",
    "app = Flask(__name__, static_folder=\"static\")  \n",
    "\n",
    "# CORSの設定\n",
    "CORS(app)\n",
    "\n",
    "# Socket.IOの設定\n",
    "socketio = SocketIO(app)\n",
    "\n",
    "# 会話ログを保持する変数\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}\n",
    "]\n",
    "\n",
    "#loggingの設定\n",
    "logging.basicConfig(\n",
    "    level=logging.DEBUG,\n",
    "    format=\"%(asctime)s [%(levelname)s] %(message)s\",\n",
    "    filename=\"app.log\",\n",
    "    encoding=\"utf-8\"\n",
    ")\n",
    "\n",
    "#--------------------------------------------------\n",
    "# Flaskのエンドポイントの作成\n",
    "#--------------------------------------------------\n",
    "\n",
    "# ルートパスへのリクエストを処理する\n",
    "@app.route(\"/\")\n",
    "def index():\n",
    "    logging.info(\"index.html を返します。\")\n",
    "    return send_from_directory(\"static\", \"index15.html\")\n",
    "\n",
    "# VoiceVoxのSpeakerIDリストを取得するエンドポイント\n",
    "@app.route(\"/speaker_ids\")\n",
    "def get_speaker_ids():\n",
    "    url = f\"{VOICEVOX_API_URL}/speakers\"  # VOICEVOX APIのエンドポイント\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error: {e}\")\n",
    "        return jsonify([])\n",
    "\n",
    "    voicevox_speakers = []\n",
    "    if response.status_code == 200:\n",
    "        speakers = response.json()\n",
    "        for speaker in speakers:\n",
    "            name = speaker['name']\n",
    "            style_names = [style['name'] for style in speaker['styles']]\n",
    "            style_ids = [style['id'] for style in speaker['styles']]\n",
    "            for style_id, style_name in zip(style_ids, style_names):\n",
    "                voicevox_speakers.append(f\"<option value={style_id}>Speaker: {name}, {style_name} </option>\")\n",
    "        logging.info(\"speaker_ids を取得しました。\")\n",
    "        return jsonify(voicevox_speakers)\n",
    "    else:\n",
    "        logging.error(f\"Error: {response.status_code}\")\n",
    "        return jsonify([])    \n",
    "\n",
    "# VoiceVoxの音声テストを行うエンドポイント\n",
    "@app.route(\"/speaker_test\" , methods=[\"POST\"])\n",
    "def speaker_test():\n",
    "    speaker = request.json[\"speaker\"]\n",
    "    text = \"こんにちは．初めまして．何かお手伝いできることはありますか？\"\n",
    "    synthesize_response = synthesize_voice(text, speaker)\n",
    "\n",
    "    # 合成した音声をmp3化\n",
    "    if synthesize_response is None: return jsonify({\"error\": \"Failed to synthesize voice\"}), 400\n",
    "    audio = AudioSegment.from_file(BytesIO(synthesize_response.content), format=\"wav\")\n",
    "    mp3_data  = BytesIO()\n",
    "    audio.export(mp3_data , format=\"mp3\")\n",
    "    mp3_data .seek(0)  \n",
    "\n",
    "    # mp3データをWebSocketを通じてクライアントに通知\n",
    "    socketio.emit('play_audio', {'audio': mp3_data.getvalue()})\n",
    "    return jsonify({\"info\": \"Speaker Test Process Succeeded\"}), 200\n",
    "\n",
    "\n",
    "# /upload へのリクエストを処理する\n",
    "@app.route(\"/upload\", methods=[\"POST\"])\n",
    "def upload_audio():\n",
    "    # uploads ディレクトリがなければ作成\n",
    "    if not os.path.exists(\"uploads\"):\n",
    "        os.makedirs(\"uploads\")\n",
    "    # uploads ディレクトリにファイルがあれば削除\n",
    "    else:\n",
    "        for file in os.listdir(\"uploads\"):\n",
    "            os.remove(os.path.join(\"uploads\", file))\n",
    "\n",
    "    # 音声ファイルをアップロード\n",
    "    if \"file\" not in request.files:\n",
    "        logging.error(\"No audio file provided\")\n",
    "        return jsonify({\"error\": \"No audio file provided\"}), 400\n",
    "    \n",
    "    audio_file = request.files[\"file\"]\n",
    "    audio_path = os.path.join(\"uploads\", f\"input_{len(messages)}.wav\") #Uploadされたファイルを残すならこっちをOn\n",
    "    audio_file.save(audio_path)\n",
    "\n",
    "    # 音声認識\n",
    "    text = recognize_speech(audio_path)\n",
    "    ## 音声認識の結果をWebSocketを通じてクライアントに通知\n",
    "    if text:\n",
    "        socketio.emit(\"SpeechRecognition\",{\"text\": text})\n",
    "    else:\n",
    "        return jsonify({\"error\": \"Failed to recognize speech\"}), 400    \n",
    "    \n",
    "    # AIの応答を取得\n",
    "    ai_response = get_ai_response(text)\n",
    "    ## WebSocketを通じてクライアントに通知\n",
    "    if ai_response:\n",
    "        socketio.emit('ai_response', {'ai_response': ai_response}) \n",
    "    else:\n",
    "        return jsonify({\"error\": \"Failed to get AI response\"}), 400\n",
    "    \n",
    "    # AIの応答から音声合成\n",
    "    speaker = request.form[\"speaker\"]\n",
    "    synthesize_response = synthesize_voice(ai_response, speaker)\n",
    "\n",
    "\n",
    "    # 合成した音声をmp3化\n",
    "    if synthesize_response is None: return jsonify({\"error\": \"Failed to synthesize voice\"}), 400\n",
    "    audio = AudioSegment.from_file(BytesIO(synthesize_response.content), format=\"wav\")\n",
    "    mp3_data  = BytesIO()\n",
    "    audio.export(mp3_data , format=\"mp3\")\n",
    "    mp3_data .seek(0)  \n",
    "\n",
    "    # mp3データをWebSocketを通じてクライアントに通知\n",
    "    socketio.emit('play_audio', {'audio': mp3_data.getvalue()})\n",
    "\n",
    "    return jsonify({\"info\": \"Uploard Process Succeeded\"}), 200\n",
    "\n",
    "\n",
    "\n",
    "# streaming処理するエンドポイント\n",
    "@app.route(\"/streaming\", methods=[\"POST\"])\n",
    "def streaming():\n",
    "    # uploads ディレクトリがなければ作成\n",
    "    if not os.path.exists(\"uploads\"):\n",
    "        os.makedirs(\"uploads\")\n",
    "    # uploads ディレクトリにファイルがあれば削除\n",
    "    else:\n",
    "        for file in os.listdir(\"uploads\"):\n",
    "            os.remove(os.path.join(\"uploads\", file))\n",
    "\n",
    "    # 音声ファイルをアップロード\n",
    "    if \"file\" not in request.files:\n",
    "        logging.error(\"No audio file provided\")\n",
    "        return jsonify({\"error\": \"No audio file provided\"}), 400\n",
    "    \n",
    "    audio_file = request.files[\"file\"]\n",
    "    audio_path = os.path.join(\"uploads\", \"input.wav\") #Uploadされたファイルを残さないならこっちをOn\n",
    "    audio_file.save(audio_path)\n",
    "\n",
    "    # 音声認識\n",
    "    text = recognize_speech(audio_path)\n",
    "    ## 音声認識の結果をWebSocketを通じてクライアントに通知\n",
    "    if text:\n",
    "        socketio.emit(\"SpeechRecognition\",{\"text\": text})\n",
    "    else:\n",
    "        return jsonify({\"error\": \"Failed to recognize speech\"}), 400    \n",
    "    \n",
    "\n",
    "    # AIの応答を句単位でストリームするとともに．句単位で音声合成もしていく\n",
    "    speaker = request.form[\"speaker\"]\n",
    "    socketio.emit('ai_stream', {'sentens': \"---Start---\"}) # 開始を通知\n",
    "    for sentence in generate_ai_response(text):\n",
    "        ## WebSocketを通じてクライアントに通知\n",
    "        if sentence:\n",
    "            # 音声合成\n",
    "            synthesize_response=synthesize_voice(sentence, speaker)\n",
    "            if synthesize_response is None: return jsonify({\"error\": \"Failed to synthesize voice\"}), 400\n",
    "            ## 合成した音声をmp3化\n",
    "            audio = AudioSegment.from_file(BytesIO(synthesize_response.content), format=\"wav\")\n",
    "            mp3_data  = BytesIO()\n",
    "            audio.export(mp3_data , format=\"mp3\")\n",
    "            mp3_data .seek(0)\n",
    "            ## mp3データをWebSocketを通じてクライアントに通知 ここでうまくキューに入れて連続再生させたい\n",
    "            socketio.emit('ai_stream', {'audio': mp3_data.getvalue(), 'sentens': sentence})\n",
    "            # sentensの区切り文字が読点だったら，0.2秒の無音を入れる\n",
    "            if sentence[-1] in \",，、\":\n",
    "                silent_audio = AudioSegment.silent(duration=10)\n",
    "                mp3_data  = BytesIO()\n",
    "                silent_audio.export(mp3_data , format=\"mp3\")\n",
    "                mp3_data .seek(0)\n",
    "            # sentensの区切り文字が読点でなかったら，0.5秒の無音を入れる\n",
    "            else:\n",
    "                silent_audio = AudioSegment.silent(duration=500)\n",
    "                mp3_data  = BytesIO()\n",
    "                silent_audio.export(mp3_data , format=\"mp3\")\n",
    "                mp3_data .seek(0)\n",
    "            # 無音を送信\n",
    "            socketio.emit('ai_stream', {'audio': mp3_data.getvalue(), 'sentens': \"---silent---\"})\n",
    "        else:\n",
    "            return jsonify({\"error\": \"Failed to get AI response\"}), 400\n",
    "    socketio.emit('ai_stream', {'sentens': \"---End---\"}) # 終了を通知\n",
    "\n",
    "    \n",
    "    return jsonify({\"info\": \"Process Succeeded\"}), 200\n",
    "\n",
    "\n",
    "#--------------------------------------------------\n",
    "# Flaskの各エンドポイント内の処理関数\n",
    "#--------------------------------------------------\n",
    "# 音声認識を行う関数\n",
    "def recognize_speech(audio_path):\n",
    "    r = sr.Recognizer()\n",
    "    with sr.AudioFile(audio_path) as source:\n",
    "        audio = r.record(source)\n",
    "        text = r.recognize_google(audio, language=\"ja-JP\")\n",
    "    return text\n",
    "\n",
    "# OpenAIのAPIを呼び出してAIの応答を取得する関数\n",
    "def get_ai_response(text):\n",
    "    client = OpenAI()\n",
    "    messages.append({\"role\": \"user\", \"content\": text})\n",
    "    completion = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=messages\n",
    "    )\n",
    "    ai_response = completion.choices[0].message.content\n",
    "    messages.append({\"role\": \"assistant\", \"content\": ai_response})\n",
    "    logging.info(f\"AIの応答: {ai_response}\")\n",
    "    return ai_response\n",
    "\n",
    "# OpenAIのAPIを呼び出してAIの応答をストリームで生成する関数\n",
    "def generate_ai_response(text):\n",
    "    client = OpenAI()\n",
    "    messages.append({\"role\": \"user\", \"content\": text})\n",
    "    completion = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=messages,\n",
    "        stream = True\n",
    "    )\n",
    "\n",
    "    sentens = \"\" # 句を構成するためのバッファ　\n",
    "    message = \"\" # プロンプトに含めるためにチャンクを結合させるためのためのバッファ\n",
    "    for chunk in completion:\n",
    "        # きちんとしたチャンクが帰ってきているかのチェック\n",
    "        if \"choices\" in chunk.to_dict() and len(chunk.choices) > 0: #to_dict：辞書型に変えないと”choices”が見つからないようなので\n",
    "            content  = chunk.choices[0].delta.content\n",
    "            if content:\n",
    "                message += content\n",
    "                # 1文字ずつ取り出してチェックする\n",
    "                for i in range(len(content)):\n",
    "                    char = content[i]\n",
    "                    sentens += char\n",
    "                    if char in \",，、。．.？?！!\\n\": #今見ているのが区切り文字だった場合（読点も区切りに含める）\n",
    "                        if i < len(content)-1: # i が最後の文字でないなら，次の文字をチェック\n",
    "                            if content[i+1] not in \",，、。．.？?！!\\n\": #次の文字が区切り文字でないならyield\n",
    "                                logging.debug(f\"句: {sentens}\")\n",
    "                                yield sentens\n",
    "                                sentens = \"\"\n",
    "                            else: #もし次の文字が区切り文字なら，現時点の区切り文字はスルー\n",
    "                                continue\n",
    "                        else: #iが最後の文字の場合，現時点でyield\n",
    "                            logging.debug(f\"句: {sentens}\")\n",
    "                            yield sentens\n",
    "                            sentens = \"\"\n",
    "    # 最後の句を返す\n",
    "    if sentens:\n",
    "        yield sentens\n",
    "    \n",
    "    # message をmessagesに追加\n",
    "    messages.append({\"role\": \"assistant\", \"content\": message})\n",
    "    logging.info(f\"AIの応答: {message}\")\n",
    "\n",
    "\n",
    "\n",
    "# VoiceVox APIで音声合成を行なう関数\n",
    "def synthesize_voice(text, speaker):\n",
    "    # 1. テキストから音声合成のためのクエリを作成\n",
    "    query_payload = {'text': text, 'speaker': speaker}\n",
    "    query_response = requests.post(f'{VOICEVOX_API_URL}/audio_query', params=query_payload)\n",
    "\n",
    "    if query_response.status_code != 200:\n",
    "        logging.error(f\"Error in audio_query: {query_response.text}\")\n",
    "        print(f\"Error in audio_query: {query_response.text}\")\n",
    "        return\n",
    "\n",
    "    query = query_response.json()\n",
    "\n",
    "    # 2. クエリを元に音声データを生成\n",
    "    synthesis_payload = {'speaker': speaker}\n",
    "    synthesis_response = requests.post(f'{VOICEVOX_API_URL}/synthesis', params=synthesis_payload, json=query)\n",
    "\n",
    "    if synthesis_response.status_code == 200:\n",
    "        logging.info(\"音声データを生成しました。\")\n",
    "        return synthesis_response\n",
    "    else:\n",
    "        logging.error(f\"Error in synthesis: {synthesis_response.text}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "# テキストを句単位に区切る\n",
    "def preprocess_text(text):\n",
    "    # テキストの前処理\n",
    "    text = re.sub(r\"[。．.]\", \"。\\n\", text)\n",
    "    text = re.sub(r\"[？?]\", \"？\\n\", text)\n",
    "    text = re.sub(r\"[！!]\", \"！\\n\", text)\n",
    "    return text\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    logging.info(\"#####アプリケーションを起動します。#####\")\n",
    "    socketio.run(app, debug=True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "html"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing static/index15.html\n"
     ]
    }
   ],
   "source": [
    "%%writefile static/index15.html\n",
    "<html lang=\"ja\">\n",
    "\n",
    "<head>\n",
    "    <meta charset=\"UTF-8\" />\n",
    "    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\" />\n",
    "    <title>WAV録音アップロード</title>\n",
    "    <!-- Recorder.js を読み込む -->\n",
    "    <script src=\"https://cdn.jsdelivr.net/gh/mattdiamond/Recorderjs@master/dist/recorder.js\"></script>\n",
    "\n",
    "    <!-- Socket.IO を読み込む -->\n",
    "    <script src=\"https://cdn.socket.io/4.0.0/socket.io.min.js\"></script>\n",
    "\n",
    "    <!-- marked.js を読み込む -->\n",
    "    <script src=\"https://cdn.jsdelivr.net/npm/marked/marked.min.js\"></script>\n",
    "\n",
    "    <!-- cssの適用-->\n",
    "    <link rel=\"stylesheet\" href=\"/static/voicechatapp.css\" />\n",
    "</head>\n",
    "\n",
    "<body>\n",
    "    <h1>WAV録音アップロード</h1>\n",
    "    <button id=\"startRecording\">録音開始</button>\n",
    "    <button id=\"stopRecording\">録音停止</button>\n",
    "    <input type=\"radio\" id=\"streaming\" name=\"Method\" value=\"/upload\" checked>まとめて再生(基本)</radio>\n",
    "    <input type=\"radio\" id=\"streaming\" name=\"Method\" value=\"/streaming\">ストリーミング</radio>\n",
    "    <select id=\"h_speakerSelect\"></select>\n",
    "    <button id=\"speakerTest\">音声テスト</button>\n",
    "    <div id=\"h_chatlog\"></div>\n",
    "\n",
    "    <script>\n",
    "        navigator.mediaDevices\n",
    "            .getUserMedia({ audio: true })\n",
    "            .then((stream) => {\n",
    "                window.stream = stream;\n",
    "            })\n",
    "            .catch((error) => {\n",
    "                console.error(\"Error accessing the microphone: \" + error);\n",
    "            });\n",
    "\n",
    "        // 音声処理用の変数\n",
    "        let audioContext; // 音声処理用のコンテキスト\n",
    "        let recorder;   // 録音用のオブジェクト\n",
    "        let audioBlob;  // 録音した音声データ\n",
    "        let audioQueue = [];    // 音声ファイルのキュー\n",
    "        let sentensQueue = [];  // センテンスのキュー\n",
    "        let isPlaying = false;  // 音声ファイル再生中かどうか\n",
    "        let currentDiv = \"\";    // 現在のdiv要素\n",
    "\n",
    "\n",
    "        // html要素取得\n",
    "        const h_startRecButton = document.getElementById(\"startRecording\");\n",
    "        const h_stopRecButton = document.getElementById(\"stopRecording\");\n",
    "        const h_speakerSelect = document.getElementById(\"h_speakerSelect\");\n",
    "        const h_speakerTestButton = document.getElementById(\"speakerTest\");\n",
    "        const h_chatlog = document.getElementById(\"h_chatlog\");\n",
    "\n",
    "        // 録音開始時のボタンを無効化\n",
    "        function setBtnonStart() {\n",
    "            h_startRecButton.disabled = true;\n",
    "            h_stopRecButton.disabled = false;\n",
    "            h_speakerSelect.disabled = true;\n",
    "            h_speakerTestButton.disabled = true;\n",
    "        }\n",
    "\n",
    "        // 処理中のボタン無効化\n",
    "        function setBtnunderProcessing() {\n",
    "            h_startRecButton.disabled = true;\n",
    "            h_stopRecButton.disabled = true;\n",
    "            h_speakerSelect.disabled = true;\n",
    "            h_speakerTestButton.disabled = true;\n",
    "        }\n",
    "\n",
    "        // 復帰時のボタン有効化\n",
    "        function setBtnonRestart() {\n",
    "            h_startRecButton.disabled = false;\n",
    "            h_stopRecButton.disabled = true;\n",
    "            h_speakerSelect.disabled = false;\n",
    "            h_speakerTestButton.disabled = false;\n",
    "        }\n",
    "\n",
    "\n",
    "        document.addEventListener(\"DOMContentLoaded\", () => {\n",
    "            const socket = io();\n",
    "\n",
    "            // SpeakerIDリストを取得\n",
    "            fetch(\"/speaker_ids\")\n",
    "                .then((response) => response.json())\n",
    "                .then((data) => {\n",
    "                    h_speakerSelect.innerHTML = data.join(\"\");\n",
    "                });\n",
    "\n",
    "            // 音声認識の結果を受信\n",
    "            socket.on(\"SpeechRecognition\", (data) => {\n",
    "                const markdownText = data.text;\n",
    "                const htmlContent = marked.parse(markdownText);\n",
    "                h_chatlog.innerHTML += `<div class=\"user\">${htmlContent}</div>`;\n",
    "            });\n",
    "\n",
    "            // AIの応答を受信したときの処理\n",
    "            socket.on(\"ai_response\", (data) => {\n",
    "                const markdownText = data.ai_response;\n",
    "                const htmlContent = marked.parse(markdownText);\n",
    "                h_chatlog.innerHTML += `<div class=\"assistant\">${htmlContent}</div>`;\n",
    "            });\n",
    "\n",
    "            // 音声ファイルを再生する処理\n",
    "            socket.on(\"play_audio\", async (data) => {\n",
    "                const audioBlob = new Blob([data.audio], { type: \"audio/mp3\" });\n",
    "                const audioUrl = URL.createObjectURL(audioBlob);\n",
    "\n",
    "                // キューに登録\n",
    "                audioQueue.push(audioUrl);\n",
    "\n",
    "                // 再生中でなければ再生\n",
    "                if (!isPlaying) {\n",
    "                    playAudio();\n",
    "                }\n",
    "                // const audio = new Audio(audioUrl);\n",
    "                // audio.play();\n",
    "            });\n",
    "\n",
    "            // Queueに登録された音声ファイルを再生する処理\n",
    "            async function playAudio() {\n",
    "                // 再生する音声ファイルがなければ終了\n",
    "                if (audioQueue.length === 0) {\n",
    "                    isPlaying = false;\n",
    "                    return;\n",
    "                }\n",
    "\n",
    "                isPlaying = true;\n",
    "                const audioUrl = audioQueue.shift();\n",
    "                const audio = new Audio(audioUrl);\n",
    "                audio.play();\n",
    "\n",
    "                // 再生が終了したら次の音声ファイルを再生\n",
    "                audio.onended = () => {\n",
    "                    playAudio();\n",
    "                };\n",
    "            }\n",
    "\n",
    "            // AIの応答ストリームを受信したときの処理\n",
    "            socket.on(\"ai_stream\", (data) => {\n",
    "                if(data.sentens){\n",
    "                    if (data.sentens.includes(\"---Start---\")) { \n",
    "                        // 最初はdivを作成\n",
    "                        h_chatlog.innerHTML += `<div class=\"assistant\"></div>`;\n",
    "                        const assistantDivs = h_chatlog.getElementsByClassName(\"assistant\");\n",
    "                        currentDiv = assistantDivs[assistantDivs.length - 1];//作ったdivを取得\n",
    "                        return;\n",
    "                    }\n",
    "                    else if (data.sentens.includes(\"---End---\") ){ \n",
    "                        // 終了時はmarkedを適用\n",
    "                        currentDiv.innerHTML= marked.parse(currentDiv.innerHTML);\n",
    "                        currentDiv = \"\"; //初期化\n",
    "                        return;\n",
    "                    }\n",
    "                    else{\n",
    "                        // sentensをセンテンスキューに登録\n",
    "                        sentensQueue.push(data.sentens);\n",
    "                    }\n",
    "                }\n",
    "\n",
    "                if(data.audio){\n",
    "                    // 音声ファイルをキューにと登録\n",
    "                    const audioBlob = new Blob([data.audio], { type: \"audio/mp3\" });\n",
    "                    const audioUrl = URL.createObjectURL(audioBlob);\n",
    "                    audioQueue.push(audioUrl); // オーディオキューに登録\n",
    "\n",
    "\n",
    "                    if (!isPlaying) {\n",
    "                        playAudioWithSentens();\n",
    "                    }\n",
    "                }\n",
    "\n",
    "            });\n",
    "\n",
    "            // Queueに登録された音声ファイルを再生する処理\n",
    "            async function playAudioWithSentens() {\n",
    "                // 再生する音声ファイルがなければ終了\n",
    "                if (audioQueue.length === 0) {\n",
    "                    isPlaying = false;\n",
    "                    return;\n",
    "                }\n",
    "                // 再生中フラグを立てる\n",
    "                isPlaying = true;\n",
    "\n",
    "                //SentensQueueからセンテンスを取り出して表示\n",
    "                //ただし、---silent---が含まれている場合は表示しない\n",
    "                const sentens = sentensQueue.shift();\n",
    "                if (!sentens.includes(\"---silent---\")){\n",
    "                    currentDiv.innerHTML += sentens;\n",
    "                }\n",
    "\n",
    "                //AudioQueueから音声ファイルを取り出して再生\n",
    "                const audioUrl = audioQueue.shift();\n",
    "                const audio = new Audio(audioUrl);\n",
    "                audio.play();\n",
    "\n",
    "                // 再生が終了したら次の音声ファイルを再生\n",
    "                audio.onended = () => {\n",
    "                    playAudioWithSentens();\n",
    "                };\n",
    "            }\n",
    "\n",
    "            // Spaceキーが押されたときにstartRecordingボタンをクリック\n",
    "            document.addEventListener(\"keydown\", (event) => {\n",
    "                if (h_startRecButton.disabled) {\n",
    "                    console.log(\"処理中のため入力はできません\");\n",
    "                    return;\n",
    "                }\n",
    "                if (event.code === \"Space\" && !event.repeat) {\n",
    "                    h_startRecButton.click();\n",
    "                }\n",
    "            });\n",
    "\n",
    "            // Spaceキーから指が離されたときにstopRecordingボタンをクリック\n",
    "            document.addEventListener(\"keyup\", (event) => {\n",
    "                if (h_stopRecButton.disabled) {\n",
    "                    console.log(\"不正な録音停止操作です\");\n",
    "                    return;\n",
    "                }\n",
    "                if (event.code === \"Space\" && !event.repeat) {\n",
    "                    h_stopRecButton.click();\n",
    "                }\n",
    "            });\n",
    "\n",
    "            //Speakerの音声確認テスト\n",
    "            h_speakerTestButton.addEventListener(\"click\", () => {\n",
    "                const speaker = h_speakerSelect.value;\n",
    "                fetch(\"/speaker_test\", {\n",
    "                    method: \"POST\",\n",
    "                    headers: {\n",
    "                        \"Content-Type\": \"application/json\",\n",
    "                    },\n",
    "                    body: JSON.stringify({ speaker }),\n",
    "                })\n",
    "                    .then((response) => response.json())\n",
    "                    .then((data) => {\n",
    "                        console.log(data);\n",
    "                    });\n",
    "            });\n",
    "\n",
    "            // 録音開始ボタンがクリックされたときの処理\n",
    "            h_startRecButton.addEventListener(\"click\", () => {\n",
    "                audioContext = new AudioContext();\n",
    "                const source = audioContext.createMediaStreamSource(window.stream);\n",
    "                recorder = new Recorder(source, { numChannels: 1 }); // モノラル録音\n",
    "                recorder.record();\n",
    "\n",
    "                // ボタンを無効化\n",
    "                setBtnonStart();\n",
    "            });\n",
    "\n",
    "            // 録音停止ボタンがクリックされたときの処理\n",
    "            h_stopRecButton.addEventListener(\"click\", () => {\n",
    "                // ボタンを無効化\n",
    "                setBtnunderProcessing();\n",
    "\n",
    "                // 録音を停止\n",
    "                recorder.stop();\n",
    "\n",
    "                // 録音した音声をファイルに保存して送信\n",
    "                recorder.exportWAV((blob) => {\n",
    "                    audioBlob = blob;\n",
    "                    if (!audioBlob) {\n",
    "                        console.error(\"No audio to upload\");\n",
    "                        return;\n",
    "                    }\n",
    "\n",
    "                    const formData = new FormData();\n",
    "                    formData.append(\"file\", audioBlob, \"recorded_audio.wav\");\n",
    "\n",
    "                    const speaker = h_speakerSelect.value;\n",
    "                    formData.append(\"speaker\", speaker);\n",
    "\n",
    "                    const method = document.querySelector('input[name=\"Method\"]:checked').value;\n",
    "\n",
    "                    fetch(method, {\n",
    "                        method: \"POST\",\n",
    "                        body: formData,\n",
    "                    })\n",
    "                        .then((response) => response.json())\n",
    "                        .then((data) => {\n",
    "                            console.log(data);\n",
    "                            // ボタン状態の初期化\n",
    "                            setBtnonRestart();\n",
    "                        })\n",
    "                        .catch((error) => {\n",
    "                            console.error(\"Upload failed:\");\n",
    "                            // ボタン状態の初期化\n",
    "                            setBtnonRestart();\n",
    "                        });\n",
    "                });\n",
    "            });\n",
    "\n",
    "            // ボタン状態の初期化\n",
    "            setBtnonRestart();\n",
    "        });\n",
    "\n",
    "        // ページを離れるときにストリームを停止\n",
    "        window.addEventListener(\"beforeunload\", () => {\n",
    "            if (window.stream) {\n",
    "                window.stream.getTracks().forEach((track) => {\n",
    "                    track.stop();\n",
    "                });\n",
    "            }\n",
    "        });\n",
    "\n",
    "    </script>\n",
    "</body>\n",
    "\n",
    "</html>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "昨日，最後の一文が音声が流れたけど，センテンスが表示されなかった原因が多分分かった！\n",
    "原因は，現在のコードだと，たとえセンテンスQueにセンテンスが残っていたとしても，ソケット通信でEndが送られるとHTMLを閉じてしまうコードになっているから．\n",
    "対策として，\n",
    "StartとEnd以外はかならずAudioとSentensがセットで送られるから，Endが発信されたということは，必要なAudioもSentensも全てQueに入っている．なので，EndもセンテンスQueに入れておいて，オーディオQueの長さが0になって時点で，センテンスQueを吐き出させればよい（通常はオーディオQueとSentensQueはセットで動くのでEndがQueに入った時だけこれが発動するはず！！\n",
    "\n",
    "ということで，改めて以下の通り．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "html"
    }
   },
   "outputs": [],
   "source": [
    "%%writefile static/index15.html\n",
    "\n",
    "<html lang=\"ja\">\n",
    "\n",
    "<head>\n",
    "    <meta charset=\"UTF-8\" />\n",
    "    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\" />\n",
    "    <title>WAV録音アップロード</title>\n",
    "    <!-- Recorder.js を読み込む -->\n",
    "    <script src=\"https://cdn.jsdelivr.net/gh/mattdiamond/Recorderjs@master/dist/recorder.js\"></script>\n",
    "\n",
    "    <!-- Socket.IO を読み込む -->\n",
    "    <script src=\"https://cdn.socket.io/4.0.0/socket.io.min.js\"></script>\n",
    "\n",
    "    <!-- marked.js を読み込む -->\n",
    "    <script src=\"https://cdn.jsdelivr.net/npm/marked/marked.min.js\"></script>\n",
    "\n",
    "    <!-- cssの適用-->\n",
    "    <link rel=\"stylesheet\" href=\"/static/voicechatapp.css\" />\n",
    "</head>\n",
    "\n",
    "<body>\n",
    "    <h1>WAV録音アップロード</h1>\n",
    "    <button id=\"startRecording\">録音開始</button>\n",
    "    <button id=\"stopRecording\">録音停止</button>\n",
    "    <input type=\"radio\" id=\"streaming\" name=\"Method\" value=\"/upload\" checked>まとめて再生(基本)</radio>\n",
    "    <input type=\"radio\" id=\"streaming\" name=\"Method\" value=\"/streaming\">ストリーミング</radio>\n",
    "    <select id=\"h_speakerSelect\"></select>\n",
    "    <button id=\"speakerTest\">音声テスト</button>\n",
    "    <div id=\"h_chatlog\"></div>\n",
    "\n",
    "    <script>\n",
    "        navigator.mediaDevices\n",
    "            .getUserMedia({ audio: true })\n",
    "            .then((stream) => {\n",
    "                window.stream = stream;\n",
    "            })\n",
    "            .catch((error) => {\n",
    "                console.error(\"Error accessing the microphone: \" + error);\n",
    "            });\n",
    "\n",
    "        // 音声処理用の変数\n",
    "        let audioContext; // 音声処理用のコンテキスト\n",
    "        let recorder;   // 録音用のオブジェクト\n",
    "        let audioBlob;  // 録音した音声データ\n",
    "        let audioQueue = [];    // 音声ファイルのキュー\n",
    "        let sentensQueue = [];  // センテンスのキュー\n",
    "        let isPlaying = false;  // 音声ファイル再生中かどうか\n",
    "        let currentDiv = \"\";    // 現在のdiv要素\n",
    "\n",
    "\n",
    "        // html要素取得\n",
    "        const h_startRecButton = document.getElementById(\"startRecording\");\n",
    "        const h_stopRecButton = document.getElementById(\"stopRecording\");\n",
    "        const h_speakerSelect = document.getElementById(\"h_speakerSelect\");\n",
    "        const h_speakerTestButton = document.getElementById(\"speakerTest\");\n",
    "        const h_chatlog = document.getElementById(\"h_chatlog\");\n",
    "\n",
    "        // 録音開始時のボタンを無効化\n",
    "        function setBtnonStart() {\n",
    "            h_startRecButton.disabled = true;\n",
    "            h_stopRecButton.disabled = false;\n",
    "            h_speakerSelect.disabled = true;\n",
    "            h_speakerTestButton.disabled = true;\n",
    "        }\n",
    "\n",
    "        // 処理中のボタン無効化\n",
    "        function setBtnunderProcessing() {\n",
    "            h_startRecButton.disabled = true;\n",
    "            h_stopRecButton.disabled = true;\n",
    "            h_speakerSelect.disabled = true;\n",
    "            h_speakerTestButton.disabled = true;\n",
    "        }\n",
    "\n",
    "        // 復帰時のボタン有効化\n",
    "        function setBtnonRestart() {\n",
    "            h_startRecButton.disabled = false;\n",
    "            h_stopRecButton.disabled = true;\n",
    "            h_speakerSelect.disabled = false;\n",
    "            h_speakerTestButton.disabled = false;\n",
    "        }\n",
    "\n",
    "\n",
    "        document.addEventListener(\"DOMContentLoaded\", () => {\n",
    "            const socket = io();\n",
    "\n",
    "            // SpeakerIDリストを取得\n",
    "            fetch(\"/speaker_ids\")\n",
    "                .then((response) => response.json())\n",
    "                .then((data) => {\n",
    "                    h_speakerSelect.innerHTML = data.join(\"\");\n",
    "                });\n",
    "\n",
    "            // 音声認識の結果を受信\n",
    "            socket.on(\"SpeechRecognition\", (data) => {\n",
    "                const markdownText = data.text;\n",
    "                const htmlContent = marked.parse(markdownText);\n",
    "                h_chatlog.innerHTML += `<div class=\"user\">${htmlContent}</div>`;\n",
    "            });\n",
    "\n",
    "            // AIの応答を受信したときの処理\n",
    "            socket.on(\"ai_response\", (data) => {\n",
    "                const markdownText = data.ai_response;\n",
    "                const htmlContent = marked.parse(markdownText);\n",
    "                h_chatlog.innerHTML += `<div class=\"assistant\">${htmlContent}</div>`;\n",
    "            });\n",
    "\n",
    "            // 音声ファイルを再生する処理\n",
    "            socket.on(\"play_audio\", async (data) => {\n",
    "                const audioBlob = new Blob([data.audio], { type: \"audio/mp3\" });\n",
    "                const audioUrl = URL.createObjectURL(audioBlob);\n",
    "\n",
    "                // キューに登録\n",
    "                audioQueue.push(audioUrl);\n",
    "\n",
    "                // 再生中でなければ再生\n",
    "                if (!isPlaying) {\n",
    "                    playAudio();\n",
    "                }\n",
    "                // const audio = new Audio(audioUrl);\n",
    "                // audio.play();\n",
    "            });\n",
    "\n",
    "            // Queueに登録された音声ファイルを再生する処理\n",
    "            async function playAudio() {\n",
    "                // 再生する音声ファイルがなければ終了\n",
    "                if (audioQueue.length === 0) {\n",
    "                    isPlaying = false;\n",
    "                    return;\n",
    "                }\n",
    "\n",
    "                isPlaying = true;\n",
    "                const audioUrl = audioQueue.shift();\n",
    "                const audio = new Audio(audioUrl);\n",
    "                audio.play();\n",
    "\n",
    "                // 再生が終了したら次の音声ファイルを再生\n",
    "                audio.onended = () => {\n",
    "                    playAudio();\n",
    "                };\n",
    "            }\n",
    "\n",
    "            // AIの応答ストリームを受信したときの処理\n",
    "            socket.on(\"ai_stream\", (data) => {\n",
    "                if(data.sentens){\n",
    "                    if (data.sentens.includes(\"---Start---\")) { \n",
    "                        // 最初はdivを作成\n",
    "                        h_chatlog.innerHTML += `<div class=\"assistant\"></div>`;\n",
    "                        const assistantDivs = h_chatlog.getElementsByClassName(\"assistant\");\n",
    "                        currentDiv = assistantDivs[assistantDivs.length - 1];//作ったdivを取得\n",
    "                        return;\n",
    "                    }\n",
    "                    // else if (data.sentens.includes(\"---End---\") ){ \n",
    "                    //     // 終了時はmarkedを適用\n",
    "                    //     currentDiv.innerHTML= marked.parse(currentDiv.innerHTML);\n",
    "                    //     currentDiv = \"\"; //初期化\n",
    "                    //     return;\n",
    "                    // }\n",
    "                    else{\n",
    "                        // sentensをセンテンスキューに登録\n",
    "                        sentensQueue.push(data.sentens);\n",
    "                    }\n",
    "                }\n",
    "\n",
    "                if(data.audio){\n",
    "                    // 音声ファイルをキューにと登録\n",
    "                    const audioBlob = new Blob([data.audio], { type: \"audio/mp3\" });\n",
    "                    const audioUrl = URL.createObjectURL(audioBlob);\n",
    "                    audioQueue.push(audioUrl); // オーディオキューに登録\n",
    "\n",
    "                    if (!isPlaying) {\n",
    "                        playAudioWithSentens();\n",
    "                    }\n",
    "                }\n",
    "            });\n",
    "\n",
    "            // Queueに登録された音声ファイルを再生する処理\n",
    "            async function playAudioWithSentens() {\n",
    "                // 再生する音声ファイルがなければ終了\n",
    "                if (audioQueue.length === 0) {\n",
    "                    isPlaying = false;\n",
    "                    //もしセンテンスQueにデータがあれば全部吐き出す\n",
    "                    while (sentensQueue.length)  {\n",
    "                        const sentens = sentensQueue.shift();\n",
    "                        if (sentens.includes(\"---End---\")){\n",
    "                            currentDiv.innerHTML= marked.parse(currentDiv.innerHTML);\n",
    "                            currentDiv = \"\"; //初期化\n",
    "                        }else{\n",
    "                            currentDiv.innerHTML += sentens;\n",
    "                        }\n",
    "                    }\n",
    "                    return;\n",
    "                }\n",
    "                // 再生中フラグを立てる\n",
    "                isPlaying = true;\n",
    "\n",
    "                //SentensQueueからセンテンスを取り出して表示\n",
    "                //ただし、---silent---が含まれている場合は表示しない\n",
    "                const sentens = sentensQueue.shift();\n",
    "                if (!sentens.includes(\"---silent---\")){\n",
    "                    currentDiv.innerHTML += sentens;\n",
    "                }\n",
    "\n",
    "                //AudioQueueから音声ファイルを取り出して再生\n",
    "                const audioUrl = audioQueue.shift();\n",
    "                const audio = new Audio(audioUrl);\n",
    "                audio.play();\n",
    "\n",
    "                // 再生が終了したら次の音声ファイルを再生\n",
    "                audio.onended = () => {\n",
    "                    playAudioWithSentens();\n",
    "                };\n",
    "            }\n",
    "\n",
    "            // Spaceキーが押されたときにstartRecordingボタンをクリック\n",
    "            document.addEventListener(\"keydown\", (event) => {\n",
    "                if (h_startRecButton.disabled) {\n",
    "                    console.log(\"処理中のため入力はできません\");\n",
    "                    return;\n",
    "                }\n",
    "                if (event.code === \"Space\" && !event.repeat) {\n",
    "                    h_startRecButton.click();\n",
    "                }\n",
    "            });\n",
    "\n",
    "            // Spaceキーから指が離されたときにstopRecordingボタンをクリック\n",
    "            document.addEventListener(\"keyup\", (event) => {\n",
    "                if (h_stopRecButton.disabled) {\n",
    "                    console.log(\"不正な録音停止操作です\");\n",
    "                    return;\n",
    "                }\n",
    "                if (event.code === \"Space\" && !event.repeat) {\n",
    "                    h_stopRecButton.click();\n",
    "                }\n",
    "            });\n",
    "\n",
    "            //Speakerの音声確認テスト\n",
    "            h_speakerTestButton.addEventListener(\"click\", () => {\n",
    "                const speaker = h_speakerSelect.value;\n",
    "                fetch(\"/speaker_test\", {\n",
    "                    method: \"POST\",\n",
    "                    headers: {\n",
    "                        \"Content-Type\": \"application/json\",\n",
    "                    },\n",
    "                    body: JSON.stringify({ speaker }),\n",
    "                })\n",
    "                    .then((response) => response.json())\n",
    "                    .then((data) => {\n",
    "                        console.log(data);\n",
    "                    });\n",
    "            });\n",
    "\n",
    "            // 録音開始ボタンがクリックされたときの処理\n",
    "            h_startRecButton.addEventListener(\"click\", () => {\n",
    "                audioContext = new AudioContext();\n",
    "                const source = audioContext.createMediaStreamSource(window.stream);\n",
    "                recorder = new Recorder(source, { numChannels: 1 }); // モノラル録音\n",
    "                recorder.record();\n",
    "\n",
    "                // ボタンを無効化\n",
    "                setBtnonStart();\n",
    "            });\n",
    "\n",
    "            // 録音停止ボタンがクリックされたときの処理\n",
    "            h_stopRecButton.addEventListener(\"click\", () => {\n",
    "                // ボタンを無効化\n",
    "                setBtnunderProcessing();\n",
    "\n",
    "                // 録音を停止\n",
    "                recorder.stop();\n",
    "\n",
    "                // 録音した音声をファイルに保存して送信\n",
    "                recorder.exportWAV((blob) => {\n",
    "                    audioBlob = blob;\n",
    "                    if (!audioBlob) {\n",
    "                        console.error(\"No audio to upload\");\n",
    "                        return;\n",
    "                    }\n",
    "\n",
    "                    const formData = new FormData();\n",
    "                    formData.append(\"file\", audioBlob, \"recorded_audio.wav\");\n",
    "\n",
    "                    const speaker = h_speakerSelect.value;\n",
    "                    formData.append(\"speaker\", speaker);\n",
    "\n",
    "                    const method = document.querySelector('input[name=\"Method\"]:checked').value;\n",
    "\n",
    "                    fetch(method, {\n",
    "                        method: \"POST\",\n",
    "                        body: formData,\n",
    "                    })\n",
    "                        .then((response) => response.json())\n",
    "                        .then((data) => {\n",
    "                            console.log(data);\n",
    "                            // ボタン状態の初期化\n",
    "                            setBtnonRestart();\n",
    "                        })\n",
    "                        .catch((error) => {\n",
    "                            console.error(\"Upload failed:\");\n",
    "                            // ボタン状態の初期化\n",
    "                            setBtnonRestart();\n",
    "                        });\n",
    "                });\n",
    "            });\n",
    "\n",
    "            // ボタン状態の初期化\n",
    "            setBtnonRestart();\n",
    "        });\n",
    "\n",
    "        // ページを離れるときにストリームを停止\n",
    "        window.addEventListener(\"beforeunload\", () => {\n",
    "            if (window.stream) {\n",
    "                window.stream.getTracks().forEach((track) => {\n",
    "                    track.stop();\n",
    "                });\n",
    "            }\n",
    "        });\n",
    "\n",
    "    </script>\n",
    "</body>\n",
    "\n",
    "</html>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## その16　他のTTSに対応する "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "とりあえずGoogle Cloud TTSを使ってみるか.\n",
    "ついでにvoicevoxについてmp3出力を端から出来るようにしておく．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### とりあえずGoogle Clout TTSを使えるようにする"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input #0, wav, from '/var/folders/vb/63y84xfs4g3cw7_09_s2_yd40000gn/T/tmp_vhok6s3.wav':\n",
      "  Duration: 00:00:09.02, bitrate: 384 kb/s\n",
      "  Stream #0:0: Audio: pcm_s16le ([1][0][0][0] / 0x0001), 24000 Hz, 1 channels, s16, 384 kb/s\n",
      "   8.92 M-A:  0.000 fd=   0 aq=    0KB vq=    0KB sq=    0B \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "import io\n",
    "from pydub import AudioSegment\n",
    "from pydub.playback import play\n",
    "import base64\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# 環境変数の読み込み\n",
    "load_dotenv()\n",
    "\n",
    "# API_KEYの取得\n",
    "API_KEY = os.environ.get(\"GOOGLE_TTS_API_KEY\")\n",
    "\n",
    "# APIエンドポイント\n",
    "url = f\"https://texttospeech.googleapis.com/v1/text:synthesize?key={API_KEY}\"\n",
    "\n",
    "# 音声合成のリクエストデータ\n",
    "data = {\n",
    "    \"input\": {\"text\": \"Hello Nice to meet you. How can I help you? I am a helpful assistant. Please tell me your request. I will do my best to help you.\"},\n",
    "    \"voice\": {\n",
    "        \"languageCode\": \"en-US\",\n",
    "        \"name\": \"en-US-Journey-F\",  \n",
    "        #\"ssmlGender\": \"FEMALE\"\n",
    "    },\n",
    "    \"audioConfig\": {\n",
    "        \"audioEncoding\": \"MP3\"\n",
    "    }\n",
    "}\n",
    "\n",
    "# リクエスト送信\n",
    "response = requests.post(url, headers={\"Content-Type\": \"application/json\"}, data=json.dumps(data))\n",
    "\n",
    "# 結果を取得\n",
    "if response.status_code == 200:\n",
    "    # Base64エンコードされた音声データをデコード\n",
    "    audio_content = json.loads(response.text)[\"audioContent\"]\n",
    "    audio_data = base64.b64decode(audio_content)\n",
    "    \n",
    "    # バイナリデータを pydub の AudioSegment に変換\n",
    "    audio = AudioSegment.from_file(io.BytesIO(audio_data), format=\"mp3\")\n",
    "\n",
    "    # Python 上で直接再生\n",
    "    play(audio)\n",
    "\n",
    "else:\n",
    "    print(\"エラー:\", response.text)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ふむ．PYTHONライブラリを使うより，API KEYをつかうのが簡単だよね（苦笑）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ということでGoogle TTSを組み込んだバージョン．\n",
    "VoiceVoxよりレスポンスはやいからスムーズにきこえるね．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing voicechatapp16.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile voicechatapp16.py\n",
    "\n",
    "from flask import Flask, request, Response, jsonify, send_from_directory, send_file, stream_with_context\n",
    "from flask_cors import CORS\n",
    "from flask_socketio import SocketIO, emit\n",
    "import os\n",
    "import speech_recognition as sr\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "import requests\n",
    "import time\n",
    "import logging\n",
    "import json\n",
    "from pydub import AudioSegment\n",
    "from io import BytesIO\n",
    "import base64\n",
    "\n",
    "\n",
    "#　環境変数の読み込み\n",
    "load_dotenv()\n",
    "\n",
    "# VoiceVox APIのエンドポイント\n",
    "VOICEVOX_API_URL = \"http://localhost:50021\"\n",
    "\n",
    "\n",
    "# Flaskアプリケーションの作成\n",
    "app = Flask(__name__, static_folder=\"static\")  \n",
    "\n",
    "# CORSの設定\n",
    "CORS(app)\n",
    "\n",
    "# Socket.IOの設定\n",
    "socketio = SocketIO(app)\n",
    "\n",
    "# 会話ログを保持する変数\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}\n",
    "]\n",
    "\n",
    "# 区切り文字の設定．AI出力をストリームで受け取るときに句切りをどの文字で行なうかの指定\n",
    "# この文字が来たら，その前までを一つの句として扱う\n",
    "SegmentingChars=\",，、。．.？?！!\\n\"\n",
    "\n",
    "#loggingの設定\n",
    "logging.basicConfig(\n",
    "    level=logging.DEBUG,\n",
    "    format=\"%(asctime)s [%(levelname)s] %(message)s\",\n",
    "    filename=\"app.log\",\n",
    "    encoding=\"utf-8\"\n",
    ")\n",
    "\n",
    "#--------------------------------------------------\n",
    "# Flaskのエンドポイントの作成\n",
    "#--------------------------------------------------\n",
    "\n",
    "# ルートパスへのリクエストを処理する\n",
    "@app.route(\"/\")\n",
    "def index():\n",
    "    logging.info(\"index.html を返します。\")\n",
    "    return send_from_directory(\"static\", \"index15.html\")\n",
    "\n",
    "# VoiceVoxのSpeakerIDリストを取得するエンドポイント\n",
    "@app.route(\"/speaker_ids\")\n",
    "def get_speaker_ids():\n",
    "    url = f\"{VOICEVOX_API_URL}/speakers\"  # VOICEVOX APIのエンドポイント\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error: {e}\")\n",
    "        return jsonify([])\n",
    "\n",
    "    voicevox_speakers = []\n",
    "    if response.status_code == 200:\n",
    "        speakers = response.json()\n",
    "        for speaker in speakers:\n",
    "            name = speaker['name']\n",
    "            style_names = [style['name'] for style in speaker['styles']]\n",
    "            style_ids = [style['id'] for style in speaker['styles']]\n",
    "            for style_id, style_name in zip(style_ids, style_names):\n",
    "                voicevox_speakers.append(f\"<option value={style_id}>Speaker: {name}, {style_name} </option>\")\n",
    "        logging.info(\"speaker_ids を取得しました。\")\n",
    "        return jsonify(voicevox_speakers)\n",
    "    else:\n",
    "        logging.error(f\"Error: {response.status_code}\")\n",
    "        return jsonify([])    \n",
    "\n",
    "# VoiceVoxの音声テストを行うエンドポイント\n",
    "@app.route(\"/speaker_test\" , methods=[\"POST\"])\n",
    "def speaker_test():\n",
    "    speaker = request.json[\"speaker\"]\n",
    "    text = \"こんにちは．初めまして．何かお手伝いできることはありますか？\"\n",
    "    mp3_data = synthesize_voice_mp3(text, speaker)\n",
    "    if mp3_data is None: return jsonify({\"error\": \"Failed to synthesize voice\"}), 400\n",
    "    # mp3データをWebSocketを通じてクライアントに通知\n",
    "    socketio.emit('play_audio', {'audio': mp3_data.getvalue()})\n",
    "    return jsonify({\"info\": \"Speaker Test Process Succeeded\"}), 200\n",
    "\n",
    "\n",
    "# /upload へのリクエストを処理する\n",
    "@app.route(\"/upload\", methods=[\"POST\"])\n",
    "def upload_audio():\n",
    "    # uploads ディレクトリがなければ作成\n",
    "    if not os.path.exists(\"uploads\"):\n",
    "        os.makedirs(\"uploads\")\n",
    "    # uploads ディレクトリにファイルがあれば削除\n",
    "    else:\n",
    "        for file in os.listdir(\"uploads\"):\n",
    "            os.remove(os.path.join(\"uploads\", file))\n",
    "\n",
    "    # 音声ファイルをアップロード\n",
    "    if \"file\" not in request.files:\n",
    "        logging.error(\"No audio file provided\")\n",
    "        return jsonify({\"error\": \"No audio file provided\"}), 400\n",
    "    \n",
    "    audio_file = request.files[\"file\"]\n",
    "    audio_path = os.path.join(\"uploads\", f\"input_{len(messages)}.wav\") #Uploadされたファイルを残すならこっちをOn\n",
    "    audio_file.save(audio_path)\n",
    "\n",
    "    # 音声認識\n",
    "    text = recognize_speech(audio_path)\n",
    "    ## 音声認識の結果をWebSocketを通じてクライアントに通知\n",
    "    if text:\n",
    "        socketio.emit(\"SpeechRecognition\",{\"text\": text})\n",
    "    else:\n",
    "        return jsonify({\"error\": \"Failed to recognize speech\"}), 400    \n",
    "    \n",
    "    # AIの応答を取得\n",
    "    ai_response = get_ai_response(text)\n",
    "    ## WebSocketを通じてクライアントに通知\n",
    "    if ai_response:\n",
    "        socketio.emit('ai_response', {'ai_response': ai_response}) \n",
    "    else:\n",
    "        return jsonify({\"error\": \"Failed to get AI response\"}), 400\n",
    "    \n",
    "    # AIの応答から音声合成してmp3で返す\n",
    "    speaker = request.form[\"speaker\"]\n",
    "    mp3_data = synthesize_voice_mp3(ai_response, speaker)\n",
    "    if mp3_data is None: return jsonify({\"error\": \"Failed to synthesize voice\"}), 400\n",
    "\n",
    "    # mp3データをWebSocketを通じてクライアントに通知\n",
    "    socketio.emit('play_audio', {'audio': mp3_data.getvalue()})\n",
    "\n",
    "    return jsonify({\"info\": \"Uploard Process Succeeded\"}), 200\n",
    "\n",
    "\n",
    "\n",
    "# streaming処理するエンドポイント\n",
    "@app.route(\"/streaming\", methods=[\"POST\"])\n",
    "def streaming():\n",
    "    # uploads ディレクトリがなければ作成\n",
    "    if not os.path.exists(\"uploads\"):\n",
    "        os.makedirs(\"uploads\")\n",
    "    # uploads ディレクトリにファイルがあれば削除\n",
    "    else:\n",
    "        for file in os.listdir(\"uploads\"):\n",
    "            os.remove(os.path.join(\"uploads\", file))\n",
    "\n",
    "    # 音声ファイルをアップロード\n",
    "    if \"file\" not in request.files:\n",
    "        logging.error(\"No audio file provided\")\n",
    "        return jsonify({\"error\": \"No audio file provided\"}), 400\n",
    "    \n",
    "    audio_file = request.files[\"file\"]\n",
    "    audio_path = os.path.join(\"uploads\", \"input.wav\") #Uploadされたファイルを残さないならこっちをOn\n",
    "    audio_file.save(audio_path)\n",
    "\n",
    "    # 音声認識\n",
    "    text = recognize_speech(audio_path)\n",
    "    ## 音声認識の結果をWebSocketを通じてクライアントに通知\n",
    "    if text:\n",
    "        socketio.emit(\"SpeechRecognition\",{\"text\": text})\n",
    "    else:\n",
    "        return jsonify({\"error\": \"Failed to recognize speech\"}), 400    \n",
    "    \n",
    "\n",
    "    # AIの応答を句単位でストリームするとともに．句単位で音声合成もしていく\n",
    "    speaker = request.form[\"speaker\"]\n",
    "    socketio.emit('ai_stream', {'sentens': \"---Start---\"}) # 開始を通知\n",
    "    for sentence in generate_ai_response(text):\n",
    "        ## WebSocketを通じてクライアントに通知\n",
    "        if sentence:\n",
    "            #　音声合成（mp3出力）\n",
    "            #mp3_data = synthesize_voice_mp3(sentence, speaker) # VoiceVox APIを使う場合\n",
    "            mp3_data = synthesize_voice_google(sentence) # Google Cloud TTS APIを使う場合\n",
    "            if mp3_data is None: return jsonify({\"error\": \"Failed to synthesize voice\"}), 400\n",
    "            ## mp3データをWebSocketを通じてクライアントに通知 ここでうまくキューに入れて連続再生させたい\n",
    "            socketio.emit('ai_stream', {'audio': mp3_data.getvalue(), 'sentens': sentence})\n",
    "            \n",
    "            # sentensの区切り文字が読点だったら，0.2秒の無音を入れる\n",
    "            if sentence[-1] in \",，、\":\n",
    "                silent_audio = AudioSegment.silent(duration=10)\n",
    "                mp3_data  = BytesIO()\n",
    "                silent_audio.export(mp3_data , format=\"mp3\")\n",
    "                mp3_data .seek(0)\n",
    "            # sentensの区切り文字が読点でなかったら，0.5秒の無音を入れる\n",
    "            else:\n",
    "                silent_audio = AudioSegment.silent(duration=500)\n",
    "                mp3_data  = BytesIO()\n",
    "                silent_audio.export(mp3_data , format=\"mp3\")\n",
    "                mp3_data .seek(0)\n",
    "            # 無音を送信\n",
    "            socketio.emit('ai_stream', {'audio': mp3_data.getvalue(), 'sentens': \"---silent---\"})\n",
    "        else:\n",
    "            return jsonify({\"error\": \"Failed to get AI response\"}), 400\n",
    "    socketio.emit('ai_stream', {'sentens': \"---End---\"}) # 終了を通知\n",
    "\n",
    "    \n",
    "    return jsonify({\"info\": \"Process Succeeded\"}), 200\n",
    "\n",
    "\n",
    "#--------------------------------------------------\n",
    "# Flaskの各エンドポイント内の処理関数\n",
    "#--------------------------------------------------\n",
    "# 音声認識を行う関数\n",
    "def recognize_speech(audio_path):\n",
    "    r = sr.Recognizer()\n",
    "    with sr.AudioFile(audio_path) as source:\n",
    "        audio = r.record(source)\n",
    "        text = r.recognize_google(audio, language=\"ja-JP\")\n",
    "    return text\n",
    "\n",
    "# OpenAIのAPIを呼び出してAIの応答を取得する関数\n",
    "def get_ai_response(text):\n",
    "    client = OpenAI()\n",
    "    messages.append({\"role\": \"user\", \"content\": text})\n",
    "    completion = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=messages\n",
    "    )\n",
    "    ai_response = completion.choices[0].message.content\n",
    "    messages.append({\"role\": \"assistant\", \"content\": ai_response})\n",
    "    logging.info(f\"AIの応答: {ai_response}\")\n",
    "    return ai_response\n",
    "\n",
    "# OpenAIのAPIを呼び出してAIの応答をストリームで生成する関数\n",
    "def generate_ai_response(text):\n",
    "    client = OpenAI()\n",
    "    messages.append({\"role\": \"user\", \"content\": text})\n",
    "    completion = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=messages,\n",
    "        stream = True\n",
    "    )\n",
    "\n",
    "    sentens = \"\" # 句を構成するためのバッファ　\n",
    "    message = \"\" # プロンプトに含めるためにチャンクを結合させるためのためのバッファ\n",
    "    for chunk in completion:\n",
    "        # きちんとしたチャンクが帰ってきているかのチェック\n",
    "        if \"choices\" in chunk.to_dict() and len(chunk.choices) > 0: #to_dict：辞書型に変えないと”choices”が見つからないようなので\n",
    "            content  = chunk.choices[0].delta.content\n",
    "            if content:\n",
    "                message += content\n",
    "                # 1文字ずつ取り出してチェックする\n",
    "                for i in range(len(content)):\n",
    "                    char = content[i]\n",
    "                    sentens += char\n",
    "                    if char in SegmentingChars: #今見ているのが区切り文字だった場合（読点も区切りに含める）\n",
    "                        if i < len(content)-1: # i が最後の文字でないなら，次の文字をチェック\n",
    "                            if content[i+1] not in SegmentingChars: #次の文字が区切り文字でないならyield\n",
    "                                logging.debug(f\"句: {sentens}\")\n",
    "                                yield sentens\n",
    "                                sentens = \"\"\n",
    "                            else: #もし次の文字が区切り文字なら，現時点の区切り文字はスルー\n",
    "                                continue\n",
    "                        else: #iが最後の文字の場合，現時点でyield\n",
    "                            logging.debug(f\"句: {sentens}\")\n",
    "                            yield sentens\n",
    "                            sentens = \"\"\n",
    "    # 最後の句を返す\n",
    "    if sentens:\n",
    "        yield sentens\n",
    "    \n",
    "    # message をmessagesに追加\n",
    "    messages.append({\"role\": \"assistant\", \"content\": message})\n",
    "    logging.info(f\"AIの応答: {message}\")\n",
    "\n",
    "\n",
    "\n",
    "# VoiceVox APIで音声合成を行なう関数\n",
    "def synthesize_voice(text, speaker):\n",
    "    # 1. テキストから音声合成のためのクエリを作成\n",
    "    query_payload = {'text': text, 'speaker': speaker}\n",
    "    query_response = requests.post(f'{VOICEVOX_API_URL}/audio_query', params=query_payload)\n",
    "\n",
    "    if query_response.status_code != 200:\n",
    "        logging.error(f\"Error in audio_query: {query_response.text}\")\n",
    "        print(f\"Error in audio_query: {query_response.text}\")\n",
    "        return\n",
    "\n",
    "    query = query_response.json()\n",
    "\n",
    "    # 2. クエリを元に音声データを生成\n",
    "    synthesis_payload = {'speaker': speaker}\n",
    "    synthesis_response = requests.post(f'{VOICEVOX_API_URL}/synthesis', params=synthesis_payload, json=query)\n",
    "\n",
    "    if synthesis_response.status_code == 200:\n",
    "        logging.info(\"音声データを生成しました。\")\n",
    "        return synthesis_response\n",
    "    else:\n",
    "        logging.error(f\"Error in synthesis: {synthesis_response.text}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "# voicevox apiで音声合成を行う関数（mp3出力）\n",
    "def synthesize_voice_mp3(text, speaker):\n",
    "    # 1. テキストから音声合成のためのクエリを作成\n",
    "    query_payload = {'text': text, 'speaker': speaker}\n",
    "    query_response = requests.post(f'{VOICEVOX_API_URL}/audio_query', params=query_payload)\n",
    "\n",
    "    if query_response.status_code != 200:\n",
    "        logging.error(f\"Error in audio_query: {query_response.text}\")\n",
    "        print(f\"Error in audio_query: {query_response.text}\")\n",
    "        return\n",
    "\n",
    "    query = query_response.json()\n",
    "\n",
    "    # 2. クエリを元に音声データを生成\n",
    "    synthesis_payload = {'speaker': speaker}\n",
    "    synthesis_response = requests.post(f'{VOICEVOX_API_URL}/synthesis', params=synthesis_payload, json=query)\n",
    "\n",
    "    if synthesis_response.status_code == 200:\n",
    "        logging.info(\"音声データを生成しました。\")\n",
    "        audio = AudioSegment.from_file(BytesIO(synthesis_response.content), format=\"wav\")\n",
    "        mp3_data  = BytesIO()\n",
    "        audio.export(mp3_data , format=\"mp3\")\n",
    "        mp3_data .seek(0)  \n",
    "        return mp3_data\n",
    "    else:\n",
    "        logging.error(f\"Error in synthesis: {synthesis_response.text}\")\n",
    "        return None\n",
    "\n",
    "# Google Clout TTS APIで音声合成を行う関数\n",
    "def synthesize_voice_google(text):\n",
    "    # APIキーの取得\n",
    "    API_KEY = os.getenv(\"GOOGLE_TTS_API_KEY\")\n",
    "\n",
    "    # APIエンドポイント\n",
    "    url = f\"https://texttospeech.googleapis.com/v1/text:synthesize?key={API_KEY}\"\n",
    "\n",
    "    # 音声合成のリクエストデータ\n",
    "    data = {\n",
    "        \"input\": {\"text\": text},\n",
    "        \"voice\": {\n",
    "            \"languageCode\": \"ja-JP\",\n",
    "            \"name\": \"ja-JP-Wavenet-D\",  # 男性の自然な声\n",
    "            \"ssmlGender\": \"MALE\"\n",
    "        },\n",
    "        \"audioConfig\": {\n",
    "            \"audioEncoding\": \"MP3\"\n",
    "        }\n",
    "    }\n",
    "\n",
    "    # リクエスト送信\n",
    "    response = requests.post(url, headers={\"Content-Type\": \"application/json\"}, data=json.dumps(data))\n",
    "\n",
    "    # 結果を取得\n",
    "    if response.status_code == 200:\n",
    "        # Base64エンコードされた音声データをデコード\n",
    "        audio_content = json.loads(response.text)[\"audioContent\"]\n",
    "        audio_data = base64.b64decode(audio_content)\n",
    "        \n",
    "        # バイナリデータを pydub の AudioSegment に変換\n",
    "        mp3_data  =BytesIO(audio_data)\n",
    "        mp3_data .seek(0)  \n",
    "        return mp3_data\n",
    "    else:\n",
    "        logging.error(f\"Error in synthesis: {response.text}\")\n",
    "        return None\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    logging.info(\"#####アプリケーションを起動します。#####\")\n",
    "    socketio.run(app, debug=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HTMLも弄っておいて，Googleの場合とVoiceVoxの場合の選択が出来るようにしておくか．\n",
    "\n",
    "### HTMLで色々と触れるようにする\n",
    "\n",
    "https://cloud.google.com/text-to-speech/docs/voices?hl=ja\n",
    "\n",
    "https://cloud.google.com/text-to-speech/docs/voice-types?hl=ja\n",
    "\n",
    "声の種類が思いのほか多い😂\n",
    "日本語はともかく，英語はめちゃくちゃ多い😂"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "html"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing static/index16.html\n"
     ]
    }
   ],
   "source": [
    "%%writefile static/index16.html\n",
    "\n",
    "<html lang=\"ja\">\n",
    "\n",
    "<head>\n",
    "    <meta charset=\"UTF-8\" />\n",
    "    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\" />\n",
    "    <title>WAV録音アップロード</title>\n",
    "    <!-- Recorder.js を読み込む -->\n",
    "    <script src=\"https://cdn.jsdelivr.net/gh/mattdiamond/Recorderjs@master/dist/recorder.js\"></script>\n",
    "\n",
    "    <!-- Socket.IO を読み込む -->\n",
    "    <script src=\"https://cdn.socket.io/4.0.0/socket.io.min.js\"></script>\n",
    "\n",
    "    <!-- marked.js を読み込む -->\n",
    "    <script src=\"https://cdn.jsdelivr.net/npm/marked/marked.min.js\"></script>\n",
    "\n",
    "    <!-- cssの適用-->\n",
    "    <link rel=\"stylesheet\" href=\"/static/voicechatapp.css\" />\n",
    "</head>\n",
    "\n",
    "<body>\n",
    "    <h1>WAV録音アップロード</h1>\n",
    "    <button type=\"button\" id=\"startRecording\">録音開始</button>\n",
    "    <button type=\"button\" id=\"stopRecording\">録音停止</button>\n",
    "    <form id = \"myForm\">\n",
    "        <div id =\"divProcessType\">\n",
    "            <input type=\"radio\" id=\"onetime\" name=\"Method\" value=\"/upload\" checked>まとめて再生(基本)</radio>\n",
    "            <input type=\"radio\" id=\"streaming\" name=\"Method\" value=\"/streaming\">ストリーミング</radio>\n",
    "        </div>\n",
    "        <div id =\"divTTSselect\">\n",
    "            <input type=\"radio\" id=\"radioVoicevoxTTS\" name=\"TTS\" value=\"VoiceVox\" checked>VoiceVox</radio>\n",
    "            <input type=\"radio\" id=\"radioGoogleTTS\" name=\"TTS\" value=\"Google\">Google TTS</radio>\n",
    "        </div>\n",
    "        <div id =\"divVoiceVoxSpeaker\">\n",
    "            <select id=\"speakerSelect\" name=\"speaker\"></select>\n",
    "        </div>\n",
    "        <div id =\"divGoogleSpeaker\" hidden>\n",
    "            <p>言語\n",
    "                <input type=\"radio\"  id = \"langCode_jp\" name=\"languageCode\" value=\"ja-JP\" checked>日本語</radio>\n",
    "                <input type=\"radio\"  id = \"langCode_en\" name=\"languageCode\" value=\"en-US\">英語</radio>\n",
    "            </p>\n",
    "            <p id=\"JPvoiceSelect\">日本語の声質\n",
    "                <select id=\"JPvoicetype\" name=\"JPvoicetype\">\n",
    "                    <option value=\"ja-JP-Neural2-B\">ニューラル・女性</option>\n",
    "                    <option value=\"ja-JP-Neural2-C\">ニューラル・男性1</option>\n",
    "                    <option value=\"ja-JP-Neural2-D\">ニューラル男性2</option>\n",
    "                    <option value=\"ja-JP-Wavenet-A\">ウェーブネット・女性1</option>\n",
    "                    <option value=\"ja-JP-Wavenet-B\">ウェーブネット・女性2</option>\n",
    "                    <option value=\"ja-JP-Wavenet-C\">ウェーブネット・男性1</option>\n",
    "                    <option value=\"ja-JP-Wavenet-D\">ウェーブネット・男性2</option>\n",
    "                </select>\n",
    "            </p>\n",
    "            <p id=\"ENvoiceSelect\" hidden>英語の声質\n",
    "                <select id=\"ENvoicetype\" name=\"ENvoicetype\">\n",
    "                    <option value=\"en-US-Journey-F\">ジャーニー/女性1</option>\n",
    "                    <option value=\"en-US-Journey-O\">ジャーニー/女性2</option>\n",
    "                    <option value=\"en-US-Journey-D\">ジャーニー/男性1</option>\n",
    "                    <option value=\"en-US-Wavenet-C\">ウェーブネット/女性1</option>\n",
    "                    <option value=\"en-US-Wavenet-E\">ウェーブネット/女性2</option>\n",
    "                    <option value=\"en-US-Wavenet-F\">ウェーブネット/女性3</option>\n",
    "                    <option value=\"en-US-Wavenet-G\">ウェーブネット/女性4</option>\n",
    "                    <option value=\"en-US-Wavenet-H\">ウェーブネット/女性5</option>\n",
    "                    <option value=\"en-US-Wavenet-A\">ウェーブネット/男性1</option>\n",
    "                    <option value=\"en-US-Wavenet-B\">ウェーブネット/男性2</option>\n",
    "                    <option value=\"en-US-Wavenet-D\">ウェーブネット/男性3</option>\n",
    "                    <option value=\"en-US-Wavenet-I\">ウェーブネット/男性4</option>\n",
    "                    <option value=\"en-US-Wavenet-J\">ウェーブネット/男性5</option>\n",
    "                </select>\n",
    "            </p>\n",
    "        </div>\n",
    "        <button type=\"button\" id=\"speakerTest\">音声テスト</button>\n",
    "    </form>\n",
    "    <div id=\"chatlog\"></div>\n",
    "\n",
    "    <script>\n",
    "        navigator.mediaDevices\n",
    "            .getUserMedia({ audio: true })\n",
    "            .then((stream) => {\n",
    "                window.stream = stream;\n",
    "            })\n",
    "            .catch((error) => {\n",
    "                console.error(\"Error accessing the microphone: \" + error);\n",
    "            });\n",
    "\n",
    "        // 音声処理用の変数\n",
    "        let audioContext; // 音声処理用のコンテキスト\n",
    "        let recorder;   // 録音用のオブジェクト\n",
    "        let audioBlob;  // 録音した音声データ\n",
    "        let audioQueue = [];    // 音声ファイルのキュー\n",
    "        let sentensQueue = [];  // センテンスのキュー\n",
    "        let isPlaying = false;  // 音声ファイル再生中かどうか\n",
    "        let currentDiv = \"\";    // 現在のdiv要素\n",
    "\n",
    "\n",
    "        // html要素取得\n",
    "        const h_startRecButton = document.getElementById(\"startRecording\");\n",
    "        const h_stopRecButton = document.getElementById(\"stopRecording\");\n",
    "        const h_speakerSelect = document.getElementById(\"speakerSelect\");\n",
    "        const h_speakerTestButton = document.getElementById(\"speakerTest\");\n",
    "        const h_chatlog = document.getElementById(\"chatlog\");\n",
    "        const h_languageCode = document.querySelector('input[name=\"languageCode\"]:checked');\n",
    "        const h_jpname = document.getElementById(\"jpname\");\n",
    "        const h_enname = document.getElementById(\"enname\");\n",
    "        const h_radioVoicevoxTTS = document.getElementById(\"radioVoicevoxTTS\");\n",
    "        const h_radioGoogleTTS = document.getElementById(\"radioGoogleTTS\");   \n",
    "        const h_TTS = document.querySelector('input[name=\"TTS\"]:checked');        \n",
    "\n",
    "        // 録音開始時のボタンを無効化\n",
    "        function setBtnonStart() {\n",
    "            h_startRecButton.disabled = true;\n",
    "            h_stopRecButton.disabled = false;\n",
    "            h_speakerSelect.disabled = true;\n",
    "            h_speakerTestButton.disabled = true;\n",
    "        }\n",
    "\n",
    "        // 処理中のボタン無効化\n",
    "        function setBtnunderProcessing() {\n",
    "            h_startRecButton.disabled = true;\n",
    "            h_stopRecButton.disabled = true;\n",
    "            h_speakerSelect.disabled = true;\n",
    "            h_speakerTestButton.disabled = true;\n",
    "        }\n",
    "\n",
    "        // 復帰時のボタン有効化\n",
    "        function setBtnonRestart() {\n",
    "            h_startRecButton.disabled = false;\n",
    "            h_stopRecButton.disabled = true;\n",
    "            h_speakerSelect.disabled = false;\n",
    "            h_speakerTestButton.disabled = false;\n",
    "        }\n",
    "\n",
    "        // formsの値を取得してJSON形式で返す\n",
    "        function getFormValues(){\n",
    "            const data = new FormData(document.getElementById(\"myForm\"));\n",
    "            const obj = {};\n",
    "            data.forEach((value, key) => {\n",
    "                obj[key] = value;\n",
    "            });\n",
    "            console.log(obj);\n",
    "            return obj;\n",
    "        }\n",
    "     \n",
    "\n",
    "        document.addEventListener(\"DOMContentLoaded\", () => {\n",
    "            // Socket.IO サーバーに接続\n",
    "            const socket = io();\n",
    "\n",
    "            // VoiceVoxの話者リストを取得\n",
    "            fetch(\"/speaker_ids\")\n",
    "                .then((response) => response.json())\n",
    "                .then((data) => {\n",
    "                    h_speakerSelect.innerHTML = data.join(\"\");\n",
    "                });\n",
    "\n",
    "\n",
    "/****** socket.ioの処理 *****/\n",
    "            // 音声認識の結果を受信\n",
    "            socket.on(\"SpeechRecognition\", (data) => {\n",
    "                const markdownText = data.text;\n",
    "                const htmlContent = marked.parse(markdownText);\n",
    "                h_chatlog.innerHTML += `<div class=\"user\">${htmlContent}</div>`;\n",
    "            });\n",
    "\n",
    "            // AIの応答を受信したときの処理\n",
    "            socket.on(\"ai_response\", (data) => {\n",
    "                const markdownText = data.ai_response;\n",
    "                const htmlContent = marked.parse(markdownText);\n",
    "                h_chatlog.innerHTML += `<div class=\"assistant\">${htmlContent}</div>`;\n",
    "            });\n",
    "\n",
    "            // 音声を再生する処理\n",
    "            socket.on(\"play_audio\", async (data) => {\n",
    "                const audioBlob = new Blob([data.audio], { type: \"audio/mp3\" });\n",
    "                const audioUrl = URL.createObjectURL(audioBlob);\n",
    "\n",
    "                // キューに登録\n",
    "                audioQueue.push(audioUrl);\n",
    "\n",
    "                // 再生中でなければ再生\n",
    "                if (!isPlaying) {\n",
    "                    playAudio();\n",
    "                }\n",
    "                // const audio = new Audio(audioUrl);\n",
    "                // audio.play();\n",
    "            });\n",
    "\n",
    "            // Queueに登録された音声ファイルを再生する処理\n",
    "            async function playAudio() {\n",
    "                // 再生する音声ファイルがなければ終了\n",
    "                if (audioQueue.length === 0) {\n",
    "                    isPlaying = false;\n",
    "                    return;\n",
    "                }\n",
    "\n",
    "                isPlaying = true;\n",
    "                const audioUrl = audioQueue.shift();\n",
    "                const audio = new Audio(audioUrl);\n",
    "                audio.play();\n",
    "\n",
    "                // 再生が終了したら次の音声ファイルを再生\n",
    "                audio.onended = () => {\n",
    "                    playAudio();\n",
    "                };\n",
    "            }\n",
    "\n",
    "            // AIの応答ストリームを受信したときの処理\n",
    "            socket.on(\"ai_stream\", (data) => {\n",
    "                if(data.sentens){\n",
    "                    if (data.sentens.includes(\"---Start---\")) { \n",
    "                        // 最初はdivを作成\n",
    "                        h_chatlog.innerHTML += `<div class=\"assistant\"></div>`;\n",
    "                        const assistantDivs = h_chatlog.getElementsByClassName(\"assistant\");\n",
    "                        currentDiv = assistantDivs[assistantDivs.length - 1];//作ったdivを取得\n",
    "                        return;\n",
    "                    }\n",
    "                    // else if (data.sentens.includes(\"---End---\") ){ \n",
    "                    //     // 終了時はmarkedを適用\n",
    "                    //     currentDiv.innerHTML= marked.parse(currentDiv.innerHTML);\n",
    "                    //     currentDiv = \"\"; //初期化\n",
    "                    //     return;\n",
    "                    // }\n",
    "                    else{\n",
    "                        // sentensをセンテンスキューに登録\n",
    "                        sentensQueue.push(data.sentens);\n",
    "                    }\n",
    "                }\n",
    "\n",
    "                if(data.audio){\n",
    "                    // 音声ファイルをキューに登録\n",
    "                    const audioBlob = new Blob([data.audio], { type: \"audio/mp3\" });\n",
    "                    const audioUrl = URL.createObjectURL(audioBlob);\n",
    "                    audioQueue.push(audioUrl); // オーディオキューに登録\n",
    "\n",
    "                    if (!isPlaying) {\n",
    "                        playAudioWithSentens();\n",
    "                    }\n",
    "                }\n",
    "            });\n",
    "\n",
    "            // Queueに登録された音声ファイルを再生する処理\n",
    "            async function playAudioWithSentens() {\n",
    "                // 再生する音声ファイルがなければ終了\n",
    "                if (audioQueue.length === 0) {\n",
    "                    isPlaying = false;\n",
    "                    //もしセンテンスQueにデータがあれば全部吐き出す\n",
    "                    while (sentensQueue.length)  {\n",
    "                        const sentens = sentensQueue.shift();\n",
    "                        if (sentens.includes(\"---End---\")){\n",
    "                            currentDiv.innerHTML= marked.parse(currentDiv.innerHTML);\n",
    "                            currentDiv = \"\"; //初期化\n",
    "                        }else{\n",
    "                            currentDiv.innerHTML += sentens;\n",
    "                        }\n",
    "                    }\n",
    "                    return;\n",
    "                }\n",
    "                // 再生中フラグを立てる\n",
    "                isPlaying = true;\n",
    "\n",
    "                //SentensQueueからセンテンスを取り出して表示\n",
    "                //ただし---silent---が含まれている場合は表示しない\n",
    "                const sentens = sentensQueue.shift();\n",
    "                if (!sentens.includes(\"---silent---\")){\n",
    "                    currentDiv.innerHTML += sentens;\n",
    "                }\n",
    "\n",
    "                //AudioQueueから音声ファイルを取り出して再生\n",
    "                const audioUrl = audioQueue.shift();\n",
    "                const audio = new Audio(audioUrl);\n",
    "                audio.play();\n",
    "\n",
    "                // 再生が終了したら次の音声ファイルを再生\n",
    "                audio.onended = () => {\n",
    "                    playAudioWithSentens();\n",
    "                };\n",
    "            }\n",
    "\n",
    "\n",
    "/***** Event listener *****/\n",
    "            // TTSselectの選択による表示切り替え\n",
    "            //// もしVoiceVoxが選択されていたら，divVoiceVoxSpeakerを表示し，divGoogleSpeakerを非表示にする\n",
    "            h_radioVoicevoxTTS.addEventListener(\"click\", () => {\n",
    "                document.getElementById(\"divVoiceVoxSpeaker\").hidden = false;\n",
    "                document.getElementById(\"divGoogleSpeaker\").hidden = true;\n",
    "            });\n",
    "            //// もしGoogleTTSが選択されていたら，divVoiceVoxSpeakerを非表示し，divGoogleSpeakerを表示する\n",
    "            h_radioGoogleTTS.addEventListener(\"click\", () => {\n",
    "                document.getElementById(\"divVoiceVoxSpeaker\").hidden = true;\n",
    "                document.getElementById(\"divGoogleSpeaker\").hidden = false;\n",
    "            });\n",
    "\n",
    "            // GoogleTTSの言語選択による表示切り替え\n",
    "            //// もし日本語が選択されていたら，JPvoiceSelectを表示し，ENvoiceSelectを非表示にする\n",
    "            document.getElementById(\"langCode_jp\").addEventListener(\"click\", () => {\n",
    "                document.getElementById(\"JPvoiceSelect\").hidden = false;\n",
    "                document.getElementById(\"ENvoiceSelect\").hidden = true;\n",
    "            });\n",
    "            //// もし英語が選択されていたら，JPvoiceSelectを非表示し，ENvoiceSelectを表示する   \n",
    "            document.getElementById(\"langCode_en\").addEventListener(\"click\", () => {\n",
    "                document.getElementById(\"JPvoiceSelect\").hidden = true;\n",
    "                document.getElementById(\"ENvoiceSelect\").hidden = false;\n",
    "            });\n",
    "\n",
    "            // Spaceキーが押されたときにstartRecordingボタンをクリック\n",
    "            document.addEventListener(\"keydown\", (event) => {\n",
    "                if (h_startRecButton.disabled) {\n",
    "                    console.log(\"処理中のため入力はできません\");\n",
    "                    return;\n",
    "                }\n",
    "                if (event.code === \"Space\" && !event.repeat) {\n",
    "                    h_startRecButton.click();\n",
    "                }\n",
    "            });\n",
    "\n",
    "            // Spaceキーから指が離されたときにstopRecordingボタンをクリック\n",
    "            document.addEventListener(\"keyup\", (event) => {\n",
    "                if (h_stopRecButton.disabled) {\n",
    "                    console.log(\"不正な録音停止操作です\");\n",
    "                    return;\n",
    "                }\n",
    "                if (event.code === \"Space\" && !event.repeat) {\n",
    "                    h_stopRecButton.click();\n",
    "                }\n",
    "            });\n",
    "\n",
    "            //Speakerの音声確認テスト\n",
    "            h_speakerTestButton.addEventListener(\"click\", () => {\n",
    "                const speaker = speakerSelect.value;\n",
    "                //Formの値を取得\n",
    "                const data = getFormValues();\n",
    "                fetch(\"/speaker_test\", {\n",
    "                    method: \"POST\",\n",
    "                    headers: {\n",
    "                        \"Content-Type\": \"application/json\",\n",
    "                    },\n",
    "                    body: JSON.stringify(data),\n",
    "                })\n",
    "                    .then((response) => response.json())\n",
    "                    .then((data) => {\n",
    "                        console.log(data);\n",
    "                    });\n",
    "            });\n",
    "\n",
    "            // 録音開始ボタンがクリックされたときの処理\n",
    "            h_startRecButton.addEventListener(\"click\", () => {\n",
    "                audioContext = new AudioContext();\n",
    "                const source = audioContext.createMediaStreamSource(window.stream);\n",
    "                recorder = new Recorder(source, { numChannels: 1 }); // モノラル録音\n",
    "                recorder.record();\n",
    "\n",
    "                // ボタンを無効化\n",
    "                setBtnonStart();\n",
    "            });\n",
    "\n",
    "            // 録音停止ボタンがクリックされたときの処理\n",
    "            h_stopRecButton.addEventListener(\"click\", () => {\n",
    "                // ボタンを無効化\n",
    "                setBtnunderProcessing();\n",
    "\n",
    "                // 録音を停止\n",
    "                recorder.stop();\n",
    "\n",
    "                // 録音した音声をファイルに保存して送信\n",
    "                recorder.exportWAV((blob) => {\n",
    "                    audioBlob = blob;\n",
    "                    if (!audioBlob) {\n",
    "                        console.error(\"No audio to upload\");\n",
    "                        return;\n",
    "                    }\n",
    "\n",
    "                    const formData = new FormData();\n",
    "                    formData.append(\"file\", audioBlob, \"recorded_audio.wav\");\n",
    "\n",
    "                    const speaker = h_speakerSelect.value;\n",
    "                    formData.append(\"speaker\", speaker);\n",
    "\n",
    "                    const method = document.querySelector('input[name=\"Method\"]:checked').value;\n",
    "\n",
    "                    fetch(method, {\n",
    "                        method: \"POST\",\n",
    "                        body: formData,\n",
    "                    })\n",
    "                        .then((response) => response.json())\n",
    "                        .then((data) => {\n",
    "                            console.log(data);\n",
    "                            // ボタン状態の初期化\n",
    "                            setBtnonRestart();\n",
    "                        })\n",
    "                        .catch((error) => {\n",
    "                            console.error(\"Upload failed:\");\n",
    "                            // ボタン状態の初期化\n",
    "                            setBtnonRestart();\n",
    "                        });\n",
    "                });\n",
    "            });\n",
    "\n",
    "            // ボタン状態の初期化\n",
    "            setBtnonRestart();\n",
    "        });\n",
    "\n",
    "        // ページを離れるときにストリームを停止\n",
    "        window.addEventListener(\"beforeunload\", () => {\n",
    "            if (window.stream) {\n",
    "                window.stream.getTracks().forEach((track) => {\n",
    "                    track.stop();\n",
    "                });\n",
    "            }\n",
    "        });\n",
    "\n",
    "    </script>\n",
    "</body>\n",
    "\n",
    "</html>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile voicechatapp16.py\n",
    "\n",
    "\n",
    "from flask import Flask, request, Response, jsonify, send_from_directory, send_file, stream_with_context\n",
    "from flask_cors import CORS\n",
    "from flask_socketio import SocketIO, emit\n",
    "import os\n",
    "import speech_recognition as sr\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "import requests\n",
    "import time\n",
    "import logging\n",
    "import json\n",
    "from pydub import AudioSegment\n",
    "from io import BytesIO\n",
    "import base64\n",
    "\n",
    "\n",
    "#　環境変数の読み込み\n",
    "load_dotenv()\n",
    "\n",
    "# VoiceVox APIのエンドポイント\n",
    "VOICEVOX_API_URL = \"http://localhost:50021\"\n",
    "\n",
    "\n",
    "# Flaskアプリケーションの作成\n",
    "app = Flask(__name__, static_folder=\"static\")  \n",
    "\n",
    "# CORSの設定\n",
    "CORS(app)\n",
    "\n",
    "# Socket.IOの設定\n",
    "socketio = SocketIO(app)\n",
    "\n",
    "# 会話ログを保持する変数\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}\n",
    "]\n",
    "\n",
    "# 区切り文字の設定．AI出力をストリームで受け取るときに句切りをどの文字で行なうかの指定\n",
    "# この文字が来たら，その前までを一つの句として扱う\n",
    "SegmentingChars=\",，、。．.？?！!\\n\"\n",
    "\n",
    "#loggingの設定\n",
    "logging.basicConfig(\n",
    "    level=logging.DEBUG,\n",
    "    format=\"%(asctime)s [%(levelname)s] %(message)s\",\n",
    "    filename=\"app.log\",\n",
    "    encoding=\"utf-8\"\n",
    ")\n",
    "\n",
    "#--------------------------------------------------\n",
    "# Flaskのエンドポイントの作成\n",
    "#--------------------------------------------------\n",
    "\n",
    "# ルートパスへのリクエストを処理する\n",
    "@app.route(\"/\")\n",
    "def index():\n",
    "    logging.info(\"index.html を返します。\")\n",
    "    return send_from_directory(\"static\", \"index16.html\")\n",
    "\n",
    "# VoiceVoxのSpeakerIDリストを取得するエンドポイント\n",
    "@app.route(\"/speaker_ids\")\n",
    "def get_speaker_ids():\n",
    "    url = f\"{VOICEVOX_API_URL}/speakers\"  # VOICEVOX APIのエンドポイント\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error: {e}\")\n",
    "        return jsonify([])\n",
    "\n",
    "    voicevox_speakers = []\n",
    "    if response.status_code == 200:\n",
    "        speakers = response.json()\n",
    "        for speaker in speakers:\n",
    "            name = speaker['name']\n",
    "            style_names = [style['name'] for style in speaker['styles']]\n",
    "            style_ids = [style['id'] for style in speaker['styles']]\n",
    "            for style_id, style_name in zip(style_ids, style_names):\n",
    "                voicevox_speakers.append(f\"<option value={style_id}>Speaker: {name}, {style_name} </option>\")\n",
    "        logging.info(\"speaker_ids を取得しました。\")\n",
    "        return jsonify(voicevox_speakers)\n",
    "    else:\n",
    "        logging.error(f\"Error: {response.status_code}\")\n",
    "        return jsonify([])    \n",
    "\n",
    "# 音声テストを行うエンドポイント\n",
    "@app.route(\"/speaker_test\" , methods=[\"POST\"])\n",
    "def speaker_test():\n",
    "    TTS = request.json[\"TTS\"]\n",
    "    speaker = request.json[\"speaker\"]\n",
    "    languageCode = request.json[\"languageCode\"]\n",
    "    JPvoicetype = request.json[\"JPvoicetype\"]\n",
    "    ENvoicetype = request.json[\"ENvoicetype\"]\n",
    "    print(f\"speaker_test: TTS={TTS}, speaker={speaker}, languageCode={languageCode}, JPvoicetype={JPvoicetype}, ENvoicetype={ENvoicetype}\")\n",
    "\n",
    "    if TTS == \"VoiceVox\":\n",
    "        text = \"こんにちは．初めまして．何かお手伝いできることはありますか？\"\n",
    "        mp3_data = synthesize_voicevox_mp3(text, speaker)\n",
    "    elif TTS == \"Google\":\n",
    "        # 日本語と英語で分岐\n",
    "        if languageCode == \"ja-JP\":\n",
    "            text = f\"こんにちは．初めまして．何かお手伝いできることはありますか？\"\n",
    "            voicetype = JPvoicetype\n",
    "        elif languageCode == \"en-US\":\n",
    "            text = f\"Hello. Nice to meet you. How can I help you?\"\n",
    "            voicetype = ENvoicetype\n",
    "        else:\n",
    "            return jsonify({\"error\": \"Failed to synthesize voice_Test. Input languageCode is irregal\"}), 400\n",
    "        # Google Cloud TTS APIで音声合成\n",
    "        mp3_data = synthesize_voice_google(text,languageCode, voicetype)\n",
    "    else:\n",
    "        return jsonify({\"error\": \"Failed to synthesize voice_Test. Input TTS is irregal\"}), 400\n",
    "    if mp3_data is None: return jsonify({\"error\": \"Failed to synthesize voice\"}), 400\n",
    "    # mp3データをWebSocketを通じてクライアントに通知\n",
    "    socketio.emit('play_audio', {'audio': mp3_data.getvalue()})\n",
    "    return jsonify({\"info\": \"Speaker Test Process Succeeded\"}), 200\n",
    "\n",
    "\n",
    "# /upload へのリクエストを処理する\n",
    "@app.route(\"/upload\", methods=[\"POST\"])\n",
    "def upload_audio():\n",
    "    # uploads ディレクトリがなければ作成\n",
    "    if not os.path.exists(\"uploads\"):\n",
    "        os.makedirs(\"uploads\")\n",
    "    # uploads ディレクトリにファイルがあれば削除\n",
    "    else:\n",
    "        for file in os.listdir(\"uploads\"):\n",
    "            os.remove(os.path.join(\"uploads\", file))\n",
    "\n",
    "    # 音声ファイルをアップロード\n",
    "    if \"file\" not in request.files:\n",
    "        logging.error(\"No audio file provided\")\n",
    "        return jsonify({\"error\": \"No audio file provided\"}), 400\n",
    "    \n",
    "    audio_file = request.files[\"file\"]\n",
    "    audio_path = os.path.join(\"uploads\", f\"input_{len(messages)}.wav\") #Uploadされたファイルを残すならこっちをOn\n",
    "    audio_file.save(audio_path)\n",
    "\n",
    "    # 音声認識\n",
    "    text = recognize_speech(audio_path)\n",
    "    ## 音声認識の結果をWebSocketを通じてクライアントに通知\n",
    "    if text:\n",
    "        socketio.emit(\"SpeechRecognition\",{\"text\": text})\n",
    "    else:\n",
    "        return jsonify({\"error\": \"Failed to recognize speech\"}), 400    \n",
    "    \n",
    "    # AIの応答を取得\n",
    "    ai_response = get_ai_response(text)\n",
    "    ## WebSocketを通じてクライアントに通知\n",
    "    if ai_response:\n",
    "        socketio.emit('ai_response', {'ai_response': ai_response}) \n",
    "    else:\n",
    "        return jsonify({\"error\": \"Failed to get AI response\"}), 400\n",
    "    \n",
    "    # AIの応答から音声合成してmp3で返す\n",
    "    speaker = request.form[\"speaker\"]\n",
    "    mp3_data = synthesize_voicevox_mp3(ai_response, speaker)\n",
    "    if mp3_data is None: return jsonify({\"error\": \"Failed to synthesize voice\"}), 400\n",
    "\n",
    "    # mp3データをWebSocketを通じてクライアントに通知\n",
    "    socketio.emit('play_audio', {'audio': mp3_data.getvalue()})\n",
    "\n",
    "    return jsonify({\"info\": \"Uploard Process Succeeded\"}), 200\n",
    "\n",
    "\n",
    "\n",
    "# streaming処理するエンドポイント\n",
    "@app.route(\"/streaming\", methods=[\"POST\"])\n",
    "def streaming():\n",
    "    # uploads ディレクトリがなければ作成\n",
    "    if not os.path.exists(\"uploads\"):\n",
    "        os.makedirs(\"uploads\")\n",
    "    # uploads ディレクトリにファイルがあれば削除\n",
    "    else:\n",
    "        for file in os.listdir(\"uploads\"):\n",
    "            os.remove(os.path.join(\"uploads\", file))\n",
    "\n",
    "    # 音声ファイルをアップロード\n",
    "    if \"file\" not in request.files:\n",
    "        logging.error(\"No audio file provided\")\n",
    "        return jsonify({\"error\": \"No audio file provided\"}), 400\n",
    "    \n",
    "    audio_file = request.files[\"file\"]\n",
    "    audio_path = os.path.join(\"uploads\", \"input.wav\") #Uploadされたファイルを残さないならこっちをOn\n",
    "    audio_file.save(audio_path)\n",
    "\n",
    "    # 音声認識\n",
    "    text = recognize_speech(audio_path)\n",
    "    ## 音声認識の結果をWebSocketを通じてクライアントに通知\n",
    "    if text:\n",
    "        socketio.emit(\"SpeechRecognition\",{\"text\": text})\n",
    "    else:\n",
    "        return jsonify({\"error\": \"Failed to recognize speech\"}), 400    \n",
    "    \n",
    "\n",
    "    # AIの応答を句単位でストリームするとともに．句単位で音声合成もしていく\n",
    "    speaker = request.form[\"speaker\"]\n",
    "    socketio.emit('ai_stream', {'sentens': \"---Start---\"}) # 開始を通知\n",
    "    for sentence in generate_ai_response(text):\n",
    "        ## WebSocketを通じてクライアントに通知\n",
    "        if sentence:\n",
    "            #　音声合成（mp3出力）\n",
    "            #mp3_data = synthesize_voicevox_mp3(sentence, speaker) # VoiceVox APIを使う場合\n",
    "            mp3_data = synthesize_voice_google(sentence) # Google Cloud TTS APIを使う場合\n",
    "            if mp3_data is None: return jsonify({\"error\": \"Failed to synthesize voice\"}), 400\n",
    "            ## mp3データをWebSocketを通じてクライアントに通知 ここでうまくキューに入れて連続再生させたい\n",
    "            socketio.emit('ai_stream', {'audio': mp3_data.getvalue(), 'sentens': sentence})\n",
    "            \n",
    "            # sentensの区切り文字が読点だったら，0.2秒の無音を入れる\n",
    "            if sentence[-1] in \",，、\":\n",
    "                silent_audio = AudioSegment.silent(duration=10)\n",
    "                mp3_data  = BytesIO()\n",
    "                silent_audio.export(mp3_data , format=\"mp3\")\n",
    "                mp3_data .seek(0)\n",
    "            # sentensの区切り文字が読点でなかったら，0.5秒の無音を入れる\n",
    "            else:\n",
    "                silent_audio = AudioSegment.silent(duration=500)\n",
    "                mp3_data  = BytesIO()\n",
    "                silent_audio.export(mp3_data , format=\"mp3\")\n",
    "                mp3_data .seek(0)\n",
    "            # 無音を送信\n",
    "            socketio.emit('ai_stream', {'audio': mp3_data.getvalue(), 'sentens': \"---silent---\"})\n",
    "        else:\n",
    "            return jsonify({\"error\": \"Failed to get AI response\"}), 400\n",
    "    socketio.emit('ai_stream', {'sentens': \"---End---\"}) # 終了を通知\n",
    "\n",
    "    \n",
    "    return jsonify({\"info\": \"Process Succeeded\"}), 200\n",
    "\n",
    "\n",
    "#--------------------------------------------------\n",
    "# Flaskの各エンドポイント内の処理関数\n",
    "#--------------------------------------------------\n",
    "# 音声認識を行う関数\n",
    "def recognize_speech(audio_path):\n",
    "    r = sr.Recognizer()\n",
    "    with sr.AudioFile(audio_path) as source:\n",
    "        audio = r.record(source)\n",
    "        text = r.recognize_google(audio, language=\"ja-JP\")\n",
    "    return text\n",
    "\n",
    "# OpenAIのAPIを呼び出してAIの応答を取得する関数\n",
    "def get_ai_response(text):\n",
    "    client = OpenAI()\n",
    "    messages.append({\"role\": \"user\", \"content\": text})\n",
    "    completion = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=messages\n",
    "    )\n",
    "    ai_response = completion.choices[0].message.content\n",
    "    messages.append({\"role\": \"assistant\", \"content\": ai_response})\n",
    "    logging.info(f\"AIの応答: {ai_response}\")\n",
    "    return ai_response\n",
    "\n",
    "# OpenAIのAPIを呼び出してAIの応答をストリームで生成する関数\n",
    "def generate_ai_response(text):\n",
    "    client = OpenAI()\n",
    "    messages.append({\"role\": \"user\", \"content\": text})\n",
    "    completion = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=messages,\n",
    "        stream = True\n",
    "    )\n",
    "\n",
    "    sentens = \"\" # 句を構成するためのバッファ　\n",
    "    message = \"\" # プロンプトに含めるためにチャンクを結合させるためのためのバッファ\n",
    "    for chunk in completion:\n",
    "        # きちんとしたチャンクが帰ってきているかのチェック\n",
    "        if \"choices\" in chunk.to_dict() and len(chunk.choices) > 0: #to_dict：辞書型に変えないと”choices”が見つからないようなので\n",
    "            content  = chunk.choices[0].delta.content\n",
    "            if content:\n",
    "                message += content\n",
    "                # 1文字ずつ取り出してチェックする\n",
    "                for i in range(len(content)):\n",
    "                    char = content[i]\n",
    "                    sentens += char\n",
    "                    if char in SegmentingChars: #今見ているのが区切り文字だった場合（読点も区切りに含める）\n",
    "                        if i < len(content)-1: # i が最後の文字でないなら，次の文字をチェック\n",
    "                            if content[i+1] not in SegmentingChars: #次の文字が区切り文字でないならyield\n",
    "                                logging.debug(f\"句: {sentens}\")\n",
    "                                yield sentens\n",
    "                                sentens = \"\"\n",
    "                            else: #もし次の文字が区切り文字なら，現時点の区切り文字はスルー\n",
    "                                continue\n",
    "                        else: #iが最後の文字の場合，現時点でyield\n",
    "                            logging.debug(f\"句: {sentens}\")\n",
    "                            yield sentens\n",
    "                            sentens = \"\"\n",
    "    # 最後の句を返す\n",
    "    if sentens:\n",
    "        yield sentens\n",
    "    \n",
    "    # message をmessagesに追加\n",
    "    messages.append({\"role\": \"assistant\", \"content\": message})\n",
    "    logging.info(f\"AIの応答: {message}\")\n",
    "\n",
    "\n",
    "\n",
    "# VoiceVox APIで音声合成を行う関数 (wav出力)\n",
    "def synthesize_voicevox(text, speaker):\n",
    "    # 1. テキストから音声合成のためのクエリを作成\n",
    "    query_payload = {'text': text, 'speaker': speaker}\n",
    "    query_response = requests.post(f'{VOICEVOX_API_URL}/audio_query', params=query_payload)\n",
    "\n",
    "    if query_response.status_code != 200:\n",
    "        logging.error(f\"Error in audio_query: {query_response.text}\")\n",
    "        print(f\"Error in audio_query: {query_response.text}\")\n",
    "        return\n",
    "\n",
    "    query = query_response.json()\n",
    "\n",
    "    # 2. クエリを元に音声データを生成\n",
    "    synthesis_payload = {'speaker': speaker}\n",
    "    synthesis_response = requests.post(f'{VOICEVOX_API_URL}/synthesis', params=synthesis_payload, json=query)\n",
    "\n",
    "    if synthesis_response.status_code == 200:\n",
    "        logging.info(\"音声データを生成しました。\")\n",
    "        return synthesis_response\n",
    "    else:\n",
    "        logging.error(f\"Error in synthesis: {synthesis_response.text}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "# voicevox で音声合成を行う関数（mp3出力）\n",
    "def synthesize_voicevox_mp3(text, speaker):\n",
    "    # voicecvox apiでwavデータを生成\n",
    "    synthesis_response = synthesize_voicevox(text, speaker)\n",
    "\n",
    "    if synthesis_response.status_code == 200:\n",
    "        logging.info(\"音声データを生成しました。\")\n",
    "        audio = AudioSegment.from_file(BytesIO(synthesis_response.content), format=\"wav\")\n",
    "        mp3_data  = BytesIO()\n",
    "        audio.export(mp3_data , format=\"mp3\")\n",
    "        mp3_data .seek(0)  \n",
    "        return mp3_data\n",
    "    else:\n",
    "        logging.error(f\"Error in synthesis: {synthesis_response.text}\")\n",
    "        return None\n",
    "\n",
    "# Google Clout TTS APIで音声合成を行う関数\n",
    "def synthesize_voice_google(text,langcode=\"ja-JP\", voicetype=\"ja-JP-Wavenet-A\"):\n",
    "    # APIキーの取得\n",
    "    API_KEY = os.getenv(\"GOOGLE_TTS_API_KEY\")\n",
    "\n",
    "    # APIエンドポイント\n",
    "    url = f\"https://texttospeech.googleapis.com/v1/text:synthesize?key={API_KEY}\"\n",
    "\n",
    "    # 音声合成のリクエストデータ\n",
    "    data = {\n",
    "        \"input\": {\"text\": text},\n",
    "        \"voice\": {\n",
    "            \"languageCode\": langcode,\n",
    "            \"name\": voicetype,  \n",
    "#            \"ssmlGender\": \"MALE\"\n",
    "        },\n",
    "        \"audioConfig\": {\n",
    "            \"audioEncoding\": \"MP3\"\n",
    "        }\n",
    "    }\n",
    "\n",
    "    # リクエスト送信\n",
    "    response = requests.post(url, headers={\"Content-Type\": \"application/json\"}, data=json.dumps(data))\n",
    "\n",
    "    # 結果を取得\n",
    "    if response.status_code == 200:\n",
    "        # Base64エンコードされた音声データをデコード\n",
    "        audio_content = json.loads(response.text)[\"audioContent\"]\n",
    "        audio_data = base64.b64decode(audio_content)\n",
    "        \n",
    "        # バイナリデータを pydub の AudioSegment に変換\n",
    "        mp3_data  =BytesIO(audio_data)\n",
    "        mp3_data .seek(0)  \n",
    "        return mp3_data\n",
    "    else:\n",
    "        logging.error(f\"Error in synthesis: {response.text}\")\n",
    "        return None\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    logging.info(\"#####アプリケーションを起動します。#####\")\n",
    "    socketio.run(app, debug=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "よし，これでGoogle TTSかVoiceVoxかを選べるようになった！\n",
    "細かな点でハマったのは，\n",
    "\n",
    "- Formでデータを取るためには，Form要素にname属性を与えないといけないこと\n",
    "- 送信した後にページのリロードが行われてFormの状態がリセットされるのは，Button要素のデフォルトがtype=\"submit\"になっていて，結びつけられたFormを送信するようにできているから（実際にはFormに結びつけてないので送信なんてしない）．また，送信すると送信の状態はリロードされてしまう．なので，type=\"button\"を設定．\n",
    "\n",
    "現時点ではまだSpeakerTestの部分だけ．\n",
    "ラッパーをかましていくか．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## その17　AIとの対話もGoogle対応\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing voicechatapp17.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile voicechatapp17.py\n",
    "\n",
    "\n",
    "\n",
    "from flask import Flask, request, Response, jsonify, send_from_directory, send_file, stream_with_context\n",
    "from flask_cors import CORS\n",
    "from flask_socketio import SocketIO, emit\n",
    "import os\n",
    "import speech_recognition as sr\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "import requests\n",
    "import time\n",
    "import logging\n",
    "import json\n",
    "from pydub import AudioSegment\n",
    "from io import BytesIO\n",
    "import base64\n",
    "\n",
    "\n",
    "#　環境変数の読み込み\n",
    "load_dotenv()\n",
    "\n",
    "# VoiceVox APIのエンドポイント\n",
    "VOICEVOX_API_URL = \"http://localhost:50021\"\n",
    "\n",
    "\n",
    "# Flaskアプリケーションの作成\n",
    "app = Flask(__name__, static_folder=\"static\")  \n",
    "\n",
    "# CORSの設定\n",
    "CORS(app)\n",
    "\n",
    "# Socket.IOの設定\n",
    "socketio = SocketIO(app)\n",
    "\n",
    "# 会話ログを保持する変数\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}\n",
    "]\n",
    "\n",
    "# 区切り文字の設定．AI出力をストリームで受け取るときに句切りをどの文字で行なうかの指定\n",
    "# この文字が来たら，その前までを一つの句として扱う\n",
    "SegmentingChars=\",，、。．.:;？?！!\\n\"\n",
    "\n",
    "#loggingの設定\n",
    "logging.basicConfig(\n",
    "    level=logging.DEBUG,\n",
    "    format=\"%(asctime)s [%(levelname)s] %(message)s\",\n",
    "    filename=\"app.log\",\n",
    "    encoding=\"utf-8\"\n",
    ")\n",
    "\n",
    "#--------------------------------------------------\n",
    "# Flaskのエンドポイントの作成\n",
    "#--------------------------------------------------\n",
    "\n",
    "# ルートパスへのリクエストを処理する\n",
    "@app.route(\"/\")\n",
    "def index():\n",
    "    logging.info(\"index.html を返します。\")\n",
    "    return send_from_directory(\"static\", \"index17.html\")\n",
    "\n",
    "# VoiceVoxのSpeakerIDリストを取得するエンドポイント\n",
    "@app.route(\"/speaker_ids\")\n",
    "def get_speaker_ids():\n",
    "    url = f\"{VOICEVOX_API_URL}/speakers\"  # VOICEVOX APIのエンドポイント\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error: {e}\")\n",
    "        return jsonify([])\n",
    "\n",
    "    voicevox_speakers = []\n",
    "    if response.status_code == 200:\n",
    "        speakers = response.json()\n",
    "        for speaker in speakers:\n",
    "            name = speaker['name']\n",
    "            style_names = [style['name'] for style in speaker['styles']]\n",
    "            style_ids = [style['id'] for style in speaker['styles']]\n",
    "            for style_id, style_name in zip(style_ids, style_names):\n",
    "                voicevox_speakers.append(f\"<option value={style_id}>Speaker: {name}, {style_name} </option>\")\n",
    "        logging.info(\"speaker_ids を取得しました。\")\n",
    "        return jsonify(voicevox_speakers)\n",
    "    else:\n",
    "        logging.error(f\"Error: {response.status_code}\")\n",
    "        return jsonify([])    \n",
    "\n",
    "# 音声テストを行うエンドポイント\n",
    "@app.route(\"/speaker_test\" , methods=[\"POST\"])\n",
    "def speaker_test():\n",
    "    TTS = request.form[\"TTS\"]\n",
    "    speaker = request.form[\"speakerId\"]\n",
    "    languageCode = request.form[\"languageCode\"]\n",
    "    JPvoicetype = request.form[\"JPvoicetype\"]\n",
    "    ENvoicetype = request.form[\"ENvoicetype\"]\n",
    "    logging.debug(f\"speaker_test: TTS={TTS}, speaker={speaker}, languageCode={languageCode}, JPvoicetype={JPvoicetype}, ENvoicetype={ENvoicetype}\")\n",
    "\n",
    "    if TTS == \"VoiceVox\":\n",
    "        text = \"こんにちは．初めまして．何かお手伝いできることはありますか？\"\n",
    "    elif TTS == \"Google\":\n",
    "        # 日本語と英語で分岐\n",
    "        if languageCode == \"ja-JP\":\n",
    "            text = f\"こんにちは．初めまして．何かお手伝いできることはありますか？\"\n",
    "        elif languageCode == \"en-US\":\n",
    "            text = f\"Hello. Nice to meet you. How can I help you?\"\n",
    "        else:\n",
    "            return jsonify({\"error\": \"Failed to synthesize voice_Test. Input languageCode is irregal\"}), 400\n",
    "        # Google Cloud TTS APIで音声合成\n",
    "    else:\n",
    "        return jsonify({\"error\": \"Failed to synthesize voice_Test. Input TTS is irregal\"}), 400\n",
    "\n",
    "    # 音声合成\n",
    "    mp3_data = synthesize_voice(text, request.form)\n",
    "    if mp3_data is None: return jsonify({\"error\": \"Failed to synthesize voice\"}), 400\n",
    "    socketio.emit('play_audio', {'audio': mp3_data.getvalue()})\n",
    "    return jsonify({\"info\": \"Speaker Test Process Succeeded\"}), 200\n",
    "\n",
    "\n",
    "# /upload へのリクエストを処理する\n",
    "@app.route(\"/upload\", methods=[\"POST\"])\n",
    "def upload_audio():\n",
    "    logging.debug(\"request.form: %s\", request.form)\n",
    "    logging.debug(\"request.files: %s\", request.files)\n",
    "    # uploads ディレクトリがなければ作成\n",
    "    if not os.path.exists(\"uploads\"):\n",
    "        os.makedirs(\"uploads\")\n",
    "    # uploads ディレクトリにファイルがあれば削除\n",
    "    else:\n",
    "        for file in os.listdir(\"uploads\"):\n",
    "            os.remove(os.path.join(\"uploads\", file))\n",
    "\n",
    "    # 音声ファイルをアップロード\n",
    "    if \"file\" not in request.files:\n",
    "        logging.error(\"No audio file provided\")\n",
    "        return jsonify({\"error\": \"No audio file provided\"}), 400\n",
    "    \n",
    "    audio_file = request.files[\"file\"]\n",
    "    audio_path = os.path.join(\"uploads\", f\"input_{len(messages)}.wav\") #Uploadされたファイルを残すならこっちをOn\n",
    "    audio_file.save(audio_path)\n",
    "\n",
    "    # 音声認識\n",
    "    start_time_sr = time.time()\n",
    "    text = recognize_speech(audio_path, request.form)\n",
    "    logging.debug(f\"UPLOAD: 音声認識にかかった時間: {time.time() - start_time_sr :.2f}秒\")\n",
    "    ## 音声認識の結果をWebSocketを通じてクライアントに通知\n",
    "    if text:\n",
    "        socketio.emit(\"SpeechRecognition\",{\"text\": text})\n",
    "    else:\n",
    "        return jsonify({\"error\": \"Failed to recognize speech\"}), 400    \n",
    "    \n",
    "    # AIの応答を取得\n",
    "    start_time_ai = time.time()\n",
    "    ai_response = get_ai_response(text)\n",
    "    logging.debug(f\"UPLOAD: AIの応答にかかった時間: {time.time() - start_time_ai :.2f}秒\")\n",
    "    ## WebSocketを通じてクライアントに通知\n",
    "    if ai_response:\n",
    "        socketio.emit('ai_response', {'ai_response': ai_response}) \n",
    "    else:\n",
    "        return jsonify({\"error\": \"Failed to get AI response\"}), 400\n",
    "    \n",
    "    # AIの応答から音声合成してmp3で返す\n",
    "    start_time_sv = time.time()\n",
    "    mp3_data = synthesize_voice(ai_response, request.form)\n",
    "    logging.debug(f\"UPLOAD: 音声合成にかかった時間: {time.time() - start_time_sv :.2f}秒\")\n",
    "    logging.debug(f\"UPLOAD: 合計処理時間: {time.time() - start_time_ai :.2f}秒\")\n",
    "    if mp3_data is None: return jsonify({\"error\": \"Failed to synthesize voice\"}), 400\n",
    "\n",
    "    # mp3データをWebSocketを通じてクライアントに通知\n",
    "    socketio.emit('play_audio', {'audio': mp3_data.getvalue()})\n",
    "\n",
    "    return jsonify({\"info\": \"Uploard Process Succeeded\"}), 200\n",
    "\n",
    "\n",
    "\n",
    "# streaming処理するエンドポイント\n",
    "@app.route(\"/streaming\", methods=[\"POST\"])\n",
    "def streaming():\n",
    "    # uploads ディレクトリがなければ作成\n",
    "    if not os.path.exists(\"uploads\"):\n",
    "        os.makedirs(\"uploads\")\n",
    "    # uploads ディレクトリにファイルがあれば削除\n",
    "    else:\n",
    "        for file in os.listdir(\"uploads\"):\n",
    "            os.remove(os.path.join(\"uploads\", file))\n",
    "\n",
    "    # 音声ファイルをアップロード\n",
    "    if \"file\" not in request.files:\n",
    "        logging.error(\"No audio file provided\")\n",
    "        return jsonify({\"error\": \"No audio file provided\"}), 400\n",
    "    \n",
    "    audio_file = request.files[\"file\"]\n",
    "    audio_path = os.path.join(\"uploads\", \"input.wav\") #Uploadされたファイルを残さないならこっちをOn\n",
    "    audio_file.save(audio_path)\n",
    "\n",
    "    # 音声認識\n",
    "    start_time_sr = time.time()\n",
    "    text = recognize_speech(audio_path, request.form)\n",
    "    logging.debug(f\"STREAMING: 音声認識にかかった時間: {time.time() - start_time_sr :.2f}秒\")\n",
    "    ## 音声認識の結果をWebSocketを通じてクライアントに通知\n",
    "    if text:\n",
    "        socketio.emit(\"SpeechRecognition\",{\"text\": text})\n",
    "    else:\n",
    "        return jsonify({\"error\": \"Failed to recognize speech\"}), 400    \n",
    "    \n",
    "\n",
    "    # AIの応答を句単位でストリームするとともに．句単位で音声合成もしていく\n",
    "    socketio.emit('ai_stream', {'sentens': \"---Start---\"}) # 開始を通知\n",
    "    start_time_stream = time.time()\n",
    "    for sentence in generate_ai_response(text):\n",
    "        ## WebSocketを通じてクライアントに通知\n",
    "        if sentence:\n",
    "            #　音声合成（mp3出力）\n",
    "            mp3_data = synthesize_voice(sentence, request.form)\n",
    "            if mp3_data is None: return jsonify({\"error\": \"Failed to synthesize voice\"}), 400\n",
    "            ## mp3データをWebSocketを通じてクライアントに通知 ここでうまくキューに入れて連続再生させたい\n",
    "            socketio.emit('ai_stream', {'audio': mp3_data.getvalue(), 'sentens': sentence})\n",
    "            \n",
    "            # sentensの区切り文字が読点だったら，0.2秒の無音を入れる\n",
    "            if sentence[-1] in \",，、\":\n",
    "                silent_audio = AudioSegment.silent(duration=10)\n",
    "                mp3_data  = BytesIO()\n",
    "                silent_audio.export(mp3_data , format=\"mp3\")\n",
    "                mp3_data .seek(0)\n",
    "            # sentensの区切り文字が読点でなかったら，0.5秒の無音を入れる\n",
    "            else:\n",
    "                silent_audio = AudioSegment.silent(duration=500)\n",
    "                mp3_data  = BytesIO()\n",
    "                silent_audio.export(mp3_data , format=\"mp3\")\n",
    "                mp3_data .seek(0)\n",
    "            # 無音を送信\n",
    "            socketio.emit('ai_stream', {'audio': mp3_data.getvalue(), 'sentens': \"---silent---\"})\n",
    "        else:\n",
    "            return jsonify({\"error\": \"Failed to get AI response\"}), 400\n",
    "    logging.debug(f\"STREAMING: ストリーミング処理にかかった時間: {time.time() - start_time_stream :.2f}秒\")\n",
    "    socketio.emit('ai_stream', {'sentens': \"---End---\"}) # 終了を通知\n",
    "\n",
    "    \n",
    "    return jsonify({\"info\": \"Process Succeeded\"}), 200\n",
    "\n",
    "\n",
    "#--------------------------------------------------\n",
    "# Flaskの各エンドポイント内の処理関数\n",
    "#--------------------------------------------------\n",
    "# 音声認識を行う関数\n",
    "def recognize_speech(audio_path, form):\n",
    "    languageCode = form[\"languageCode\"]\n",
    "    r = sr.Recognizer()\n",
    "    with sr.AudioFile(audio_path) as source:\n",
    "        audio = r.record(source)\n",
    "        text = r.recognize_google(audio, language=languageCode)\n",
    "    return text\n",
    "\n",
    "# OpenAIのAPIを呼び出してAIの応答を取得する関数\n",
    "def get_ai_response(text):\n",
    "    client = OpenAI()\n",
    "    messages.append({\"role\": \"user\", \"content\": text})\n",
    "    completion = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=messages\n",
    "    )\n",
    "    ai_response = completion.choices[0].message.content\n",
    "    messages.append({\"role\": \"assistant\", \"content\": ai_response})\n",
    "    logging.info(f\"AIの応答: {ai_response}\")\n",
    "    return ai_response\n",
    "\n",
    "# OpenAIのAPIを呼び出してAIの応答をストリームで生成する関数\n",
    "def generate_ai_response(text):\n",
    "\n",
    "    \"\"\"\"\n",
    "    色々なチェーン処理を書くならここに入れる．\n",
    "    Claude : https://note.com/noa813/n/n307d62b5820b\n",
    "    Gemini : https://qiita.com/RyutoYoda/items/a51830dd75a2dac96d72\n",
    "                 https://ai.google.dev/api?hl=ja&lang=python\n",
    "\n",
    "    \"\"\"\n",
    "    client = OpenAI()\n",
    "    messages.append({\"role\": \"user\", \"content\": text})\n",
    "    completion = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=messages,\n",
    "        stream = True\n",
    "    )\n",
    "\n",
    "    sentens = \"\" # 句を構成するためのバッファ　\n",
    "    message = \"\" # プロンプトに含めるためにチャンクを結合させるためのためのバッファ\n",
    "    for chunk in completion:\n",
    "        # きちんとしたチャンクが帰ってきているかのチェック\n",
    "        if \"choices\" in chunk.to_dict() and len(chunk.choices) > 0: #to_dict：辞書型に変えないと”choices”が見つからないようなので\n",
    "            content  = chunk.choices[0].delta.content\n",
    "            if content:\n",
    "                message += content\n",
    "                # 1文字ずつ取り出してチェックする\n",
    "                for i in range(len(content)):\n",
    "                    char = content[i]\n",
    "                    sentens += char\n",
    "                    if char in SegmentingChars: #今見ているのが区切り文字だった場合（読点も区切りに含める）\n",
    "                        if i < len(content)-1: # i が最後の文字でないなら，次の文字をチェック\n",
    "                            if content[i+1] not in SegmentingChars: #次の文字が区切り文字でないならyield\n",
    "                                #logging.debug(f\"句: {sentens}\")\n",
    "                                yield sentens\n",
    "                                sentens = \"\"\n",
    "                            else: #もし次の文字が区切り文字なら，現時点の区切り文字はスルー\n",
    "                                continue\n",
    "                        else: #iが最後の文字の場合，現時点でyield\n",
    "                            #logging.debug(f\"句: {sentens}\")\n",
    "                            yield sentens\n",
    "                            sentens = \"\"\n",
    "    # 最後の句を返す\n",
    "    if sentens:\n",
    "        yield sentens\n",
    "    \n",
    "    # message をmessagesに追加\n",
    "    messages.append({\"role\": \"assistant\", \"content\": message})\n",
    "    logging.info(f\"AIの応答: {message}\")\n",
    "\n",
    "# 各種APIを使って音声合成を行うラッパー関数\n",
    "def synthesize_voice(text, form):\n",
    "    # TTSの種類情報を取得\n",
    "    TTS = form[\"TTS\"]\n",
    "    speaker = form[\"speakerId\"]\n",
    "    languageCode = form[\"languageCode\"]\n",
    "    JPvoicetype = form[\"JPvoicetype\"]\n",
    "    ENvoicetype = form[\"ENvoicetype\"]\n",
    "    logging.debug(f\"speaker_test: TTS={TTS}, speaker={speaker}, languageCode={languageCode}, JPvoicetype={JPvoicetype}, ENvoicetype={ENvoicetype}\")\n",
    "\n",
    "    #Textに読み上げしない文字が含まれてる場合はその文字をTextから外す\n",
    "    text = text.replace(\"#\", \"\") # 見出し文字#を削除\n",
    "    text = text.replace(\"**\", \"\") # 協調表示**を削除\n",
    "\n",
    "    if TTS == \"VoiceVox\":\n",
    "        mp3_data = synthesize_voicevox_mp3(text, speaker)\n",
    "    elif TTS == \"Google\":\n",
    "        # 日本語と英語で分岐\n",
    "        if languageCode == \"ja-JP\":\n",
    "            voicetype = JPvoicetype\n",
    "        elif languageCode == \"en-US\":\n",
    "            voicetype = ENvoicetype\n",
    "        else:# 日本語でも英語でもない場合\n",
    "            return jsonify({\"error\": \"Failed to synthesize voice_Test. Input languageCode is irregal\"}), 400\n",
    "        # Google Cloud TTS APIで音声合成\n",
    "        mp3_data = synthesize_voice_google(text,languageCode, voicetype)\n",
    "    else: # TTSがVoiceVoxでもGoogleでもない場合\n",
    "        return jsonify({\"error\": \"Failed to synthesize voice_Test. Input TTS is irregal\"}), 400\n",
    "    \n",
    "    if mp3_data is None: \n",
    "        return jsonify({\"error\": \"Failed to synthesize voice\"}), 400\n",
    "    # mp3 データを返す    \n",
    "    return mp3_data\n",
    "\n",
    "# VoiceVox APIで音声合成を行う関数 (wav出力)\n",
    "def synthesize_voicevox(text, speaker):\n",
    "    # 1. テキストから音声合成のためのクエリを作成\n",
    "    query_payload = {'text': text, 'speaker': speaker}\n",
    "    query_response = requests.post(f'{VOICEVOX_API_URL}/audio_query', params=query_payload)\n",
    "\n",
    "    if query_response.status_code != 200:\n",
    "        logging.error(f\"Error in audio_query: {query_response.text}\")\n",
    "        print(f\"Error in audio_query: {query_response.text}\")\n",
    "        return\n",
    "\n",
    "    query = query_response.json()\n",
    "\n",
    "    # 2. クエリを元に音声データを生成\n",
    "    synthesis_payload = {'speaker': speaker}\n",
    "    synthesis_response = requests.post(f'{VOICEVOX_API_URL}/synthesis', params=synthesis_payload, json=query)\n",
    "\n",
    "    if synthesis_response.status_code == 200:\n",
    "        logging.info(\"音声データを生成しました。\")\n",
    "        return synthesis_response\n",
    "    else:\n",
    "        logging.error(f\"Error in synthesis: {synthesis_response.text}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "# voicevox で音声合成を行う関数（mp3出力）\n",
    "def synthesize_voicevox_mp3(text, speaker):\n",
    "    # voicecvox apiでwavデータを生成\n",
    "    synthesis_response = synthesize_voicevox(text, speaker)\n",
    "\n",
    "    if synthesis_response.status_code == 200:\n",
    "        logging.info(\"音声データを生成しました。\")\n",
    "        audio = AudioSegment.from_file(BytesIO(synthesis_response.content), format=\"wav\")\n",
    "        mp3_data  = BytesIO()\n",
    "        audio.export(mp3_data , format=\"mp3\")\n",
    "        mp3_data .seek(0)  \n",
    "        return mp3_data\n",
    "    else:\n",
    "        logging.error(f\"Error in synthesis: {synthesis_response.text}\")\n",
    "        return None\n",
    "\n",
    "# Google Clout TTS APIで音声合成を行う関数\n",
    "def synthesize_voice_google(text,langcode=\"ja-JP\", voicetype=\"ja-JP-Wavenet-A\"):\n",
    "    # APIキーの取得\n",
    "    API_KEY = os.getenv(\"GOOGLE_TTS_API_KEY\")\n",
    "\n",
    "    # APIエンドポイント\n",
    "    url = f\"https://texttospeech.googleapis.com/v1/text:synthesize?key={API_KEY}\"\n",
    "\n",
    "    # 音声合成のリクエストデータ\n",
    "    data = {\n",
    "        \"input\": {\"text\": text},\n",
    "        \"voice\": {\n",
    "            \"languageCode\": langcode,\n",
    "            \"name\": voicetype,  \n",
    "#            \"ssmlGender\": \"MALE\"\n",
    "        },\n",
    "        \"audioConfig\": {\n",
    "            \"audioEncoding\": \"MP3\"\n",
    "        }\n",
    "    }\n",
    "\n",
    "    # リクエスト送信\n",
    "    response = requests.post(url, headers={\"Content-Type\": \"application/json\"}, data=json.dumps(data))\n",
    "\n",
    "    # 結果を取得\n",
    "    if response.status_code == 200:\n",
    "        # Base64エンコードされた音声データをデコード\n",
    "        audio_content = json.loads(response.text)[\"audioContent\"]\n",
    "        audio_data = base64.b64decode(audio_content)\n",
    "        \n",
    "        # バイナリデータを pydub の AudioSegment に変換\n",
    "        mp3_data  =BytesIO(audio_data)\n",
    "        mp3_data .seek(0)  \n",
    "        return mp3_data\n",
    "    else:\n",
    "        logging.error(f\"Error in synthesis: {response.text}\")\n",
    "        return None\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    logging.info(\"#####アプリケーションを起動します。#####\")\n",
    "    socketio.run(app, debug=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "html"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing static/index17.html\n"
     ]
    }
   ],
   "source": [
    "%%writefile static/index17.html\n",
    "\n",
    "<html lang=\"ja\">\n",
    "\n",
    "<head>\n",
    "    <meta charset=\"UTF-8\" />\n",
    "    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\" />\n",
    "    <title>WAV録音アップロード</title>\n",
    "    <!-- Recorder.js を読み込む -->\n",
    "    <script src=\"https://cdn.jsdelivr.net/gh/mattdiamond/Recorderjs@master/dist/recorder.js\"></script>\n",
    "\n",
    "    <!-- Socket.IO を読み込む -->\n",
    "    <script src=\"https://cdn.socket.io/4.0.0/socket.io.min.js\"></script>\n",
    "\n",
    "    <!-- marked.js を読み込む -->\n",
    "    <script src=\"https://cdn.jsdelivr.net/npm/marked/marked.min.js\"></script>\n",
    "\n",
    "    <!-- cssの適用-->\n",
    "    <link rel=\"stylesheet\" href=\"/static/voicechatapp.css\" />\n",
    "</head>\n",
    "\n",
    "<body>\n",
    "    <h1>WAV録音アップロード</h1>\n",
    "    <button type=\"button\" id=\"startRecording\">録音開始</button>\n",
    "    <button type=\"button\" id=\"stopRecording\">録音停止</button>\n",
    "    <form id = \"myForm\">\n",
    "        <div id =\"divProcessType\">\n",
    "            <P>動作モード: \n",
    "                <input type=\"radio\" id=\"onetime\" name=\"Method\" value=\"/upload\" checked>AIの出力をまとめて再生(基本)</radio>\n",
    "                <input type=\"radio\" id=\"streaming\" name=\"Method\" value=\"/streaming\">ストリーミング</radio>\n",
    "            </P>\n",
    "        </div>\n",
    "        <div>\n",
    "            <P>言語: \n",
    "                <input type=\"radio\"  id = \"langCode_jp\" name=\"languageCode\" value=\"ja-JP\" checked>日本語</radio>\n",
    "                <input type=\"radio\"  id = \"langCode_en\" name=\"languageCode\" value=\"en-US\">英語</radio>\n",
    "            </P>\n",
    "        </div>\n",
    "        <div id =\"divTTSselect\">\n",
    "            <P>音声合成エンジン: \n",
    "                <input type=\"radio\" id=\"radioVoicevoxTTS\" name=\"TTS\" value=\"VoiceVox\" checked>VoiceVox</radio>\n",
    "                <input type=\"radio\" id=\"radioGoogleTTS\" name=\"TTS\" value=\"Google\">Google TTS</radio>\n",
    "            </P>\n",
    "        </div>\n",
    "        <div id =\"divVoiceVoxSpeaker\">\n",
    "            <select id=\"speakerSelect\" name=\"speakerId\"></select>\n",
    "        </div>\n",
    "        <div id =\"divGoogleSpeaker\" hidden>\n",
    "            <p id=\"JPvoiceSelect\">日本語の声質: \n",
    "                <select id=\"JPvoicetype\" name=\"JPvoicetype\">\n",
    "                    <option value=\"ja-JP-Neural2-B\">ニューラル・女性</option>\n",
    "                    <option value=\"ja-JP-Neural2-C\">ニューラル・男性1</option>\n",
    "                    <option value=\"ja-JP-Neural2-D\">ニューラル男性2</option>\n",
    "                    <option value=\"ja-JP-Wavenet-A\">ウェーブネット・女性1</option>\n",
    "                    <option value=\"ja-JP-Wavenet-B\">ウェーブネット・女性2</option>\n",
    "                    <option value=\"ja-JP-Wavenet-C\">ウェーブネット・男性1</option>\n",
    "                    <option value=\"ja-JP-Wavenet-D\">ウェーブネット・男性2</option>\n",
    "                </select>\n",
    "            </p>\n",
    "            <p id=\"ENvoiceSelect\" hidden>英語の声質: \n",
    "                <select id=\"ENvoicetype\" name=\"ENvoicetype\">\n",
    "                    <option value=\"en-US-Journey-F\">ジャーニー/女性1</option>\n",
    "                    <option value=\"en-US-Journey-O\">ジャーニー/女性2</option>\n",
    "                    <option value=\"en-US-Journey-D\">ジャーニー/男性1</option>\n",
    "                    <option value=\"en-US-Wavenet-C\">ウェーブネット/女性1</option>\n",
    "                    <option value=\"en-US-Wavenet-E\">ウェーブネット/女性2</option>\n",
    "                    <option value=\"en-US-Wavenet-F\">ウェーブネット/女性3</option>\n",
    "                    <option value=\"en-US-Wavenet-G\">ウェーブネット/女性4</option>\n",
    "                    <option value=\"en-US-Wavenet-H\">ウェーブネット/女性5</option>\n",
    "                    <option value=\"en-US-Wavenet-A\">ウェーブネット/男性1</option>\n",
    "                    <option value=\"en-US-Wavenet-B\">ウェーブネット/男性2</option>\n",
    "                    <option value=\"en-US-Wavenet-D\">ウェーブネット/男性3</option>\n",
    "                    <option value=\"en-US-Wavenet-I\">ウェーブネット/男性4</option>\n",
    "                    <option value=\"en-US-Wavenet-J\">ウェーブネット/男性5</option>\n",
    "                </select>\n",
    "            </p>\n",
    "        </div>\n",
    "    </form>\n",
    "    <button type=\"button\" id=\"speakerTest\">音声テスト</button>\n",
    "    <div id=\"chatlog\"></div>\n",
    "\n",
    "    <script>\n",
    "\n",
    "        // 音声処理用の変数\n",
    "        let audioContext; // 音声処理用のコンテキスト\n",
    "        let recorder;   // 録音用のオブジェクト\n",
    "        let audioBlob;  // 録音した音声データ\n",
    "        let audioQueue = [];    // 音声ファイルのキュー\n",
    "        let sentensQueue = [];  // センテンスのキュー\n",
    "        let isPlaying = false;  // 音声ファイル再生中かどうか\n",
    "        let currentDiv = \"\";    // 現在のdiv要素\n",
    "\n",
    "\n",
    "        // html要素取得\n",
    "        const h_startRecButton = document.getElementById(\"startRecording\");\n",
    "        const h_stopRecButton = document.getElementById(\"stopRecording\");\n",
    "        const h_speakerSelect = document.getElementById(\"speakerSelect\");\n",
    "        const h_speakerTestButton = document.getElementById(\"speakerTest\");\n",
    "        const h_chatlog = document.getElementById(\"chatlog\");\n",
    "        const h_languageCode = document.querySelector('input[name=\"languageCode\"]:checked');\n",
    "        const h_jpname = document.getElementById(\"jpname\");\n",
    "        const h_enname = document.getElementById(\"enname\");\n",
    "        const h_radioVoicevoxTTS = document.getElementById(\"radioVoicevoxTTS\");\n",
    "        const h_radioGoogleTTS = document.getElementById(\"radioGoogleTTS\");   \n",
    "        const h_TTS = document.querySelector('input[name=\"TTS\"]:checked');        \n",
    "\n",
    "        // 録音開始時のボタンを無効化\n",
    "        function setBtnonStart() {\n",
    "            h_startRecButton.disabled = true;\n",
    "            h_stopRecButton.disabled = false;\n",
    "            h_speakerTestButton.disabled = true;\n",
    "        }\n",
    "\n",
    "        // 処理中のボタン無効化\n",
    "        function setBtnunderProcessing() {\n",
    "            h_startRecButton.disabled = true;\n",
    "            h_stopRecButton.disabled = true;\n",
    "            h_speakerTestButton.disabled = true;\n",
    "        }\n",
    "\n",
    "        // 復帰時のボタン有効化\n",
    "        function setBtnonRestart() {\n",
    "            h_startRecButton.disabled = false;\n",
    "            h_stopRecButton.disabled = true;\n",
    "            h_speakerTestButton.disabled = false;\n",
    "        }\n",
    "\n",
    "        // formsの値を取得してJSON形式で返す\n",
    "        function getFormValues(){\n",
    "            const data = new FormData(document.getElementById(\"myForm\"));\n",
    "            const obj = {};\n",
    "            data.forEach((value, key) => {\n",
    "                obj[key] = value;\n",
    "            });\n",
    "            console.log(obj);\n",
    "            return obj;\n",
    "        }\n",
    "     \n",
    "        // マイクのアクセス許可を取得\n",
    "        navigator.mediaDevices\n",
    "            .getUserMedia({ audio: true })\n",
    "            .then((stream) => {\n",
    "                window.stream = stream;\n",
    "            })\n",
    "            .catch((error) => {\n",
    "                console.error(\"Error accessing the microphone: \" + error);\n",
    "            });\n",
    "\n",
    "        // VoiceVoxの話者リストを取得\n",
    "        fetch(\"/speaker_ids\")\n",
    "            .then((response) => response.json())\n",
    "            .then((data) => {\n",
    "                h_speakerSelect.innerHTML = data.join(\"\");\n",
    "                h_speakerSelect.disabled = false;\n",
    "            });\n",
    "\n",
    "        document.addEventListener(\"DOMContentLoaded\", () => {\n",
    "            // Socket.IO サーバーに接続\n",
    "            const socket = io();\n",
    "\n",
    "/****** socket.ioの処理 *****/\n",
    "            // 音声認識の結果を受信\n",
    "            socket.on(\"SpeechRecognition\", (data) => {\n",
    "                const markdownText = data.text;\n",
    "                const htmlContent = marked.parse(markdownText);\n",
    "                h_chatlog.innerHTML += `<div class=\"user\">${htmlContent}</div>`;\n",
    "            });\n",
    "\n",
    "            // AIの応答を受信したときの処理\n",
    "            socket.on(\"ai_response\", (data) => {\n",
    "                const markdownText = data.ai_response;\n",
    "                const htmlContent = marked.parse(markdownText);\n",
    "                h_chatlog.innerHTML += `<div class=\"assistant\">${htmlContent}</div>`;\n",
    "            });\n",
    "\n",
    "            // 音声を再生する処理\n",
    "            socket.on(\"play_audio\", async (data) => {\n",
    "                const audioBlob = new Blob([data.audio], { type: \"audio/mp3\" });\n",
    "                const audioUrl = URL.createObjectURL(audioBlob);\n",
    "\n",
    "                // キューに登録\n",
    "                audioQueue.push(audioUrl);\n",
    "\n",
    "                // 再生中でなければ再生\n",
    "                if (!isPlaying) {\n",
    "                    playAudio();\n",
    "                }\n",
    "                // const audio = new Audio(audioUrl);\n",
    "                // audio.play();\n",
    "            });\n",
    "\n",
    "            // Queueに登録された音声ファイルを再生する処理\n",
    "            async function playAudio() {\n",
    "                // 再生する音声ファイルがなければ終了\n",
    "                if (audioQueue.length === 0) {\n",
    "                    isPlaying = false;\n",
    "                    return;\n",
    "                }\n",
    "\n",
    "                isPlaying = true;\n",
    "                const audioUrl = audioQueue.shift();\n",
    "                const audio = new Audio(audioUrl);\n",
    "                audio.play();\n",
    "\n",
    "                // 再生が終了したら次の音声ファイルを再生\n",
    "                audio.onended = () => {\n",
    "                    playAudio();\n",
    "                };\n",
    "            }\n",
    "\n",
    "            // AIの応答ストリームを受信したときの処理\n",
    "            socket.on(\"ai_stream\", (data) => {\n",
    "                if(data.sentens){\n",
    "                    if (data.sentens.includes(\"---Start---\")) { \n",
    "                        // 最初はdivを作成\n",
    "                        h_chatlog.innerHTML += `<div class=\"assistant\"></div>`;\n",
    "                        const assistantDivs = h_chatlog.getElementsByClassName(\"assistant\");\n",
    "                        currentDiv = assistantDivs[assistantDivs.length - 1];//作ったdivを取得\n",
    "                        return;\n",
    "                    }\n",
    "                    // else if (data.sentens.includes(\"---End---\") ){ \n",
    "                    //     // 終了時はmarkedを適用\n",
    "                    //     currentDiv.innerHTML= marked.parse(currentDiv.innerHTML);\n",
    "                    //     currentDiv = \"\"; //初期化\n",
    "                    //     return;\n",
    "                    // }\n",
    "                    else{\n",
    "                        // sentensをセンテンスキューに登録\n",
    "                        sentensQueue.push(data.sentens);\n",
    "                    }\n",
    "                }\n",
    "\n",
    "                if(data.audio){\n",
    "                    // 音声ファイルをキューに登録\n",
    "                    const audioBlob = new Blob([data.audio], { type: \"audio/mp3\" });\n",
    "                    const audioUrl = URL.createObjectURL(audioBlob);\n",
    "                    audioQueue.push(audioUrl); // オーディオキューに登録\n",
    "\n",
    "                    if (!isPlaying) {\n",
    "                        playAudioWithSentens();\n",
    "                    }\n",
    "                }\n",
    "            });\n",
    "\n",
    "            // Queueに登録された音声ファイルを再生する処理\n",
    "            async function playAudioWithSentens() {\n",
    "                // 再生する音声ファイルがなければ終了\n",
    "                if (audioQueue.length === 0) {\n",
    "                    isPlaying = false;\n",
    "                    //もしセンテンスQueにデータがあれば全部吐き出す\n",
    "                    while (sentensQueue.length)  {\n",
    "                        const sentens = sentensQueue.shift();\n",
    "                        if (sentens.includes(\"---End---\")){\n",
    "                            currentDiv.innerHTML= marked.parse(currentDiv.innerHTML);\n",
    "                            currentDiv = \"\"; //初期化\n",
    "                        }else{\n",
    "                            currentDiv.innerHTML += sentens;\n",
    "                        }\n",
    "                    }\n",
    "                    return;\n",
    "                }\n",
    "                // 再生中フラグを立てる\n",
    "                isPlaying = true;\n",
    "\n",
    "                //SentensQueueからセンテンスを取り出して表示\n",
    "                //ただし---silent---が含まれている場合は表示しない\n",
    "                const sentens = sentensQueue.shift();\n",
    "                if (!sentens.includes(\"---silent---\")){\n",
    "                    currentDiv.innerHTML += sentens;\n",
    "                }\n",
    "\n",
    "                //AudioQueueから音声ファイルを取り出して再生\n",
    "                const audioUrl = audioQueue.shift();\n",
    "                const audio = new Audio(audioUrl);\n",
    "                audio.play();\n",
    "\n",
    "                // 再生が終了したら次の音声ファイルを再生\n",
    "                audio.onended = () => {\n",
    "                    playAudioWithSentens();\n",
    "                };\n",
    "            }\n",
    "\n",
    "\n",
    "/***** Event listener *****/\n",
    "            // TTSselectの選択による表示切り替え\n",
    "            //// もしVoiceVoxが選択されていたら，divVoiceVoxSpeakerを表示し，divGoogleSpeakerを非表示にする\n",
    "            h_radioVoicevoxTTS.addEventListener(\"click\", () => {\n",
    "                document.getElementById(\"divVoiceVoxSpeaker\").hidden = false;\n",
    "                document.getElementById(\"divGoogleSpeaker\").hidden = true;\n",
    "            });\n",
    "            //// もしGoogleTTSが選択されていたら，divVoiceVoxSpeakerを非表示し，divGoogleSpeakerを表示する\n",
    "            h_radioGoogleTTS.addEventListener(\"click\", () => {\n",
    "                document.getElementById(\"divVoiceVoxSpeaker\").hidden = true;\n",
    "                document.getElementById(\"divGoogleSpeaker\").hidden = false;\n",
    "            });\n",
    "\n",
    "            // GoogleTTSの言語選択による表示切り替え\n",
    "            //// もし日本語が選択されていたら，JPvoiceSelectを表示し，ENvoiceSelectを非表示にする\n",
    "            document.getElementById(\"langCode_jp\").addEventListener(\"click\", () => {\n",
    "                document.getElementById(\"JPvoiceSelect\").hidden = false;\n",
    "                document.getElementById(\"ENvoiceSelect\").hidden = true;\n",
    "            });\n",
    "            //// もし英語が選択されていたら，JPvoiceSelectを非表示し，ENvoiceSelectを表示する   \n",
    "            document.getElementById(\"langCode_en\").addEventListener(\"click\", () => {\n",
    "                document.getElementById(\"JPvoiceSelect\").hidden = true;\n",
    "                document.getElementById(\"ENvoiceSelect\").hidden = false;\n",
    "            });\n",
    "\n",
    "            // Spaceキーが押されたときにstartRecordingボタンをクリック\n",
    "            document.addEventListener(\"keydown\", (event) => {\n",
    "                if (h_startRecButton.disabled) {\n",
    "                    console.log(\"処理中のため入力はできません\");\n",
    "                    return;\n",
    "                }\n",
    "                if (event.code === \"Space\" && !event.repeat) {\n",
    "                    h_startRecButton.click();\n",
    "                }\n",
    "            });\n",
    "\n",
    "            // Spaceキーから指が離されたときにstopRecordingボタンをクリック\n",
    "            document.addEventListener(\"keyup\", (event) => {\n",
    "                if (h_stopRecButton.disabled) {\n",
    "                    console.log(\"不正な録音停止操作です\");\n",
    "                    return;\n",
    "                }\n",
    "                if (event.code === \"Space\" && !event.repeat) {\n",
    "                    h_stopRecButton.click();\n",
    "                }\n",
    "            });\n",
    "\n",
    "            //Speakerの音声確認テスト\n",
    "            h_speakerTestButton.addEventListener(\"click\", () => {\n",
    "                const formData = new FormData(document.getElementById(\"myForm\"));\n",
    "\n",
    "                fetch(\"/speaker_test\", {\n",
    "                    method: \"POST\",\n",
    "                    body: formData,\n",
    "                })\n",
    "                    .then((response) => response.json())\n",
    "                    .then((data) => {\n",
    "                        console.log(data);\n",
    "                    });\n",
    "            });\n",
    "\n",
    "            // 録音開始ボタンがクリックされたときの処理\n",
    "            h_startRecButton.addEventListener(\"click\", () => {\n",
    "                audioContext = new AudioContext();\n",
    "                const source = audioContext.createMediaStreamSource(window.stream);\n",
    "                recorder = new Recorder(source, { numChannels: 1 }); // モノラル録音\n",
    "                recorder.record();\n",
    "\n",
    "                // ボタンを無効化\n",
    "                setBtnonStart();\n",
    "            });\n",
    "\n",
    "            // 録音停止ボタンがクリックされたときの処理\n",
    "            h_stopRecButton.addEventListener(\"click\", () => {\n",
    "                // ボタンを無効化\n",
    "                setBtnunderProcessing();\n",
    "\n",
    "                // 録音を停止\n",
    "                recorder.stop();\n",
    "\n",
    "                // 録音した音声をファイルに保存して送信\n",
    "                recorder.exportWAV((blob) => {\n",
    "                    audioBlob = blob;\n",
    "                    if (!audioBlob) {\n",
    "                        console.error(\"No audio to upload\");\n",
    "                        return;\n",
    "                    }\n",
    "\n",
    "                    // formデータを取得\n",
    "                    const formData = new FormData(document.getElementById(\"myForm\"));\n",
    "                    console.log(formData);\n",
    "                    getFormValues();\n",
    "\n",
    "                    // 音声ファイルを追加\n",
    "                    formData.append(\"file\", audioBlob, \"recorded_audio.wav\");\n",
    "\n",
    "                    const method = document.querySelector('input[name=\"Method\"]:checked').value;\n",
    "\n",
    "                    fetch(method, {\n",
    "                        method: \"POST\",\n",
    "                        body: formData,\n",
    "                    })\n",
    "                        .then((response) => response.json())\n",
    "                        .then((data) => {\n",
    "                            console.log(data);\n",
    "                            // ボタン状態の初期化\n",
    "                            setBtnonRestart();\n",
    "                        })\n",
    "                        .catch((error) => {\n",
    "                            console.error(\"Upload failed:\");\n",
    "                            // ボタン状態の初期化\n",
    "                            setBtnonRestart();\n",
    "                        });\n",
    "                });\n",
    "            });\n",
    "\n",
    "            // ボタン状態の初期化\n",
    "            setBtnonRestart();\n",
    "        });\n",
    "\n",
    "        // ページを離れるときにストリームを停止\n",
    "        window.addEventListener(\"beforeunload\", () => {\n",
    "            if (window.stream) {\n",
    "                window.stream.getTracks().forEach((track) => {\n",
    "                    track.stop();\n",
    "                });\n",
    "            }\n",
    "        });\n",
    "\n",
    "    </script>\n",
    "</body>\n",
    "\n",
    "</html>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "フォーム送信で，select要素speakerIdがFormに読み込まれないエラーが起こってた．色々と調べてたら，なぜか.disableが要素についていた．要素をグレーアウトして変更できないようにするためにボタン操作できないようにするための関数の中でそうなるよう設定していたのだが，これを設定していると，Formにデータが含まれないという仕様になっているとのこと．これまでは直接getElementByIDで要素のValueを取得していたため気づかなかった．そんな仕様があるとは・・・"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## その１８　Google Geminiの実装\n",
    "### 事始め\n",
    "まずはGoogle Gminiを試してみる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q -U google-genai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### シンプルなテキスト生成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "こんにちは！初めまして。うる星やつらのラムちゃんについてですね。喜んでお教えします！\n",
      "\n",
      "ラムちゃんは、高橋留美子先生の漫画『うる星やつら』のメインヒロインの一人です。彼女は、その愛らしい容姿と独特の性格で、多くのファンを魅了してきました。\n",
      "\n",
      "**基本的な情報**\n",
      "\n",
      "*   **名前:** ラム (Lum)\n",
      "*   **種族:** 鬼族の宇宙人\n",
      "*   **出身:** 鬼星 (おにぼし)\n",
      "*   **年齢:** 不明 (外見は10代後半くらい)\n",
      "*   **特徴:**\n",
      "    *   エメラルドグリーンの髪\n",
      "    *   虎柄のビキニ\n",
      "    *   頭に生えた2本の角\n",
      "    *   空を飛ぶ能力\n",
      "    *   電撃を操る能力\n",
      "    *   語尾に「～だっちゃ」をつける独特の口調\n",
      "\n",
      "**性格**\n",
      "\n",
      "*   **一途で情熱的:** ダーリン（諸星あたる）を心から愛しており、彼のこととなると周りが見えなくなるほど。\n",
      "*   **嫉妬深い:** あたるが他の女性に少しでも気を向けると、容赦なく電撃を浴びせる。\n",
      "*   **天真爛漫:** 純粋で無邪気な性格。地球の文化や習慣に戸惑いながらも、楽しんでいる。\n",
      "*   **おせっかい焼き:** 面倒見がよく、困っている人を見ると放っておけない。\n",
      "*   **子供っぽい:** わがままで甘えん坊な一面も。\n",
      "\n",
      "**物語における役割**\n",
      "\n",
      "ラムちゃんは、あたるが地球を救った（と誤解した）ことをきっかけに、彼を「ダーリン」と呼んで追いかけ回すようになります。彼女の登場によって、あたるの日常は騒がしく、そしてコミカルに変化していきます。\n",
      "\n",
      "ラムちゃんは、単なるヒロインというだけでなく、物語のコメディ要素を担う重要なキャラクターです。彼女の存在が、うる星やつらの世界をより魅力的なものにしています。\n",
      "\n",
      "**その他**\n",
      "\n",
      "*   ラムちゃんの虎柄ビキニは、連載当時、斬新でセクシーなデザインとして話題になりました。\n",
      "*   ラムちゃんの声優は、初代アニメでは平野文さんが、2022年版アニメでは上坂すみれさんが担当しています。\n",
      "*   ラムちゃんは、アニメ史に残る人気キャラクターの一人として、今も多くの人に愛されています。\n",
      "\n",
      "ラムちゃんについて、もっと知りたいことはありますか？ 例えば、\n",
      "*   ラムちゃんの電撃能力について\n",
      "*   ラムちゃんのファッションについて\n",
      "*   ラムちゃんとあたるの関係について\n",
      "\n",
      "など、どんなことでも聞いてくださいね！\n",
      "こんにちは\n",
      "！初めまして。うる星やつらのラムちゃんについてですね。喜んでお\n",
      "答えします！\n",
      "\n",
      "ラムちゃんは、高橋留美子先生の漫画\n",
      "『うる星やつら』のヒロインであり、作中で最も人気のあるキャラクターの一人です。彼女について知っておくべきポイントはたくさんありますが\n",
      "、主なものを以下にまとめました。\n",
      "\n",
      "**基本的な情報**\n",
      "\n",
      "*   **名前:** ラム (Lum)\n",
      "*   **種族:** \n",
      "鬼族\n",
      "*   **出身:** 鬼星\n",
      "*   **特徴:**\n",
      "    *   エメラルドグリーンの長い髪\n",
      "    *   虎柄のビキニ\n",
      "    *   語尾に「～\n",
      "だっちゃ」をつける\n",
      "    *   空を飛ぶことができる\n",
      "    *   電撃を操る能力を持つ\n",
      "\n",
      "**性格**\n",
      "\n",
      "*   **一途で情熱的:** ダーリン（諸星あたる\n",
      "）を一途に愛しており、彼が他の女性に少しでも気を向けると激しく嫉妬します。\n",
      "*   **子供っぽく無邪気:** 天真爛漫で、感情表現がストレート。喜怒哀楽がはっきりしています。\n",
      "*   **わがまま:** 自分の\n",
      "気持ちを優先することが多く、周りを振り回すこともあります。\n",
      "*   **強い:** 鬼族としての戦闘能力が高く、電撃を操る力は強力です。\n",
      "\n",
      "**物語における役割**\n",
      "\n",
      "*   **ヒロイン:** 物語の中心人物であり、あたるをめぐる\n",
      "騒動の中心となります。\n",
      "*   **トラブルメーカー:** ラムちゃんの行動が、様々な騒動を引き起こす原因となります。\n",
      "*   **コメディリリーフ:** そのコミカルな言動や行動で、読者や視聴者を楽しませます。\n",
      "*   **恋愛要素:** あ\n",
      "たるとの恋愛模様は、物語の重要な要素の一つです。\n",
      "\n",
      "**ラムちゃんの魅力**\n",
      "\n",
      "*   **外見の可愛らしさ:** 虎柄ビキニにエメラルドグリーンの髪という独特のスタイルは、一度見たら忘れられないほど魅力的です。\n",
      "*   **\n",
      "性格のギャップ:** 一途で情熱的な面と、子供っぽくわがままな面とのギャップが、彼女の魅力を引き立てています。\n",
      "*   **圧倒的な存在感:** その強さと個性的なキャラクターで、物語の中で圧倒的な存在感を放っています。\n",
      "\n",
      "**その他\n",
      "**\n",
      "\n",
      "*   ラムちゃんの「～だっちゃ」という語尾は、彼女のトレードマークとなっています。\n",
      "*   ラムちゃんは、アニメやゲームなど、様々なメディアで展開されており、多くのファンに愛されています。\n",
      "*   ラムちゃんのコスプレは、イベントなどで非常に人気があります。\n",
      "\n",
      "ラム\n",
      "ちゃんは、『うる星やつら』という作品を語る上で欠かせない、非常に魅力的なキャラクターです。もし、まだ作品に触れたことがなければ、ぜひ一度ご覧になってみてください。\n",
      "\n",
      "何か他に知りたいことや、もっと詳しく聞きたいことがあれば、遠慮なく質問してくださいね！\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from google import genai\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# シンプルなテキスト生成\n",
    "client = genai.Client(api_key=os.getenv(\"GOOGLE_GEMINI_API_KEY\"))\n",
    "response = client.models.generate_content(\n",
    "    model=\"gemini-2.0-flash\", contents=\"こんにちは．初めまして．うる星やつらのラムちゃんについて教えて\"\n",
    ")\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "こんにちは\n",
      "！初めまして。ラムちゃんについてですね！喜んでお答えします。\n",
      "\n",
      "\n",
      "ラムちゃんは、高橋留美子先生の漫画『うる星やつら\n",
      "』のメインヒロインの一人で、鬼族の女の子です。\n",
      "\n",
      "**基本的な情報**\n",
      "\n",
      "*   **名前:** ラム\n",
      "*   **種\n",
      "族:** 鬼族\n",
      "*   **出身:** 鬼星\n",
      "*   **特徴:**\n",
      "    *   エメラルドグリーンの髪と\n",
      "瞳\n",
      "    *   虎柄のビキニ\n",
      "    *   語尾に「～だっちゃ」をつける独特な口調\n",
      "    *   電撃を操る能力\n",
      "    *   空を飛ぶ能力\n",
      "\n",
      "    *   非常に嫉妬深く、わがまま\n",
      "\n",
      "**性格**\n",
      "\n",
      "*   明るく天真爛漫\n",
      "*   一途で愛情深い\n",
      "*   非常に嫉妬深い\n",
      "*   わがまま\n",
      "*   子供\n",
      "っぽい一面もある\n",
      "\n",
      "**物語における役割**\n",
      "\n",
      "*   主人公の諸星あたると恋仲になる\n",
      "*   あたると周囲の人間を巻き込む騒動の中心人物\n",
      "*   物語のコメディリリーフ\n",
      "\n",
      "**魅力**\n",
      "\n",
      "*   可愛らしい外見と、わがままで\n",
      "強気な性格のギャップ\n",
      "*   あたるとのコミカルなやり取り\n",
      "*   電撃を操るアクションシーン\n",
      "*   独特な口調や言葉遣い\n",
      "\n",
      "**その他**\n",
      "\n",
      "*   『うる星やつら』は、1980年代にアニメ化され、ラム\n",
      "ちゃんは一躍人気キャラクターとなりました。\n",
      "*   ラムちゃんは、現在でも多くのファンに愛されており、様々なグッズやコラボレーション企画が展開されています。\n",
      "\n",
      "ラムちゃんについて、もっと知りたいことはありますか？ 例えば、\n",
      "\n",
      "*   ラムちゃんのファッションについて\n",
      "*   ラムちゃんの声優について\n",
      "*   ラム\n",
      "ちゃんの人気について\n",
      "*   ラムちゃんとあたるの関係について\n",
      "\n",
      "など、どんなことでも聞いてくださいね。\n"
     ]
    }
   ],
   "source": [
    "## streaming出力\n",
    "response = client.models.generate_content_stream(\n",
    "    model=\"gemini-2.0-flash\", contents=\"こんにちは．初めまして．うる星やつらのラムちゃんについて教えて\"\n",
    ")\n",
    "for chunk in response:\n",
    "    print(chunk.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "めんどうなやつっちゃ！ こんちは、あたる。よろしくな！ …またラムちゃんに怒られんようにせえよ？\n",
      "\n",
      "お前の名前は諸星あたるだっちゃ！ さっき自分で言ったっちゃ！\n",
      "\n",
      "role - user: こんにちは．私の名前は諸星あたるです．うる星やつらの登場人物です．\n",
      "role - model: めんどうなやつっちゃ！ こんちは、あたる。よろしくな！ …またラムちゃんに怒られんようにせえよ？\n",
      "\n",
      "role - user: 私の名前は何？\n",
      "role - model: お前の名前は諸星あたるだっちゃ！ さっき自分で言ったっちゃ！\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## チャット\n",
    "chat = client.chats.create(model=\"gemini-2.0-flash\")\n",
    "response = chat.send_message(\"こんにちは．私の名前は諸星あたるです．うる星やつらの登場人物です．\")\n",
    "print(response.text)\n",
    "response = chat.send_message(\"私の名前は何？\")\n",
    "print(response.text)\n",
    "for message in chat._curated_history:\n",
    "    print(f'role - {message.role}', end=\": \")\n",
    "    print(message.parts[0].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "めんどうなやつっちゃ！こんにちは、諸星あたるくん！ 君の浮気癖にはラムちゃんも手を焼いとるみたいじゃのう。でも、どこか憎めないのが、あたるところの魅力なんじゃろうね。今日はどんな面白いことをしでかすんじゃ？\n",
      "きみの名前は諸星あたるだっちゃ！\n"
     ]
    }
   ],
   "source": [
    "## チャット（ストリーミング出力）\n",
    "chat = client.chats.create(model=\"gemini-2.0-flash\")\n",
    "response = chat.send_message_stream(\"こんにちは．私の名前は諸星あたるです．うる星やつらの登場人物です．\")\n",
    "for chunk in response:\n",
    "    print(chunk.text, end=\"\")\n",
    "    \n",
    "response = chat.send_message_stream(\"私の名前は何？\")\n",
    "for chunk in response:\n",
    "    print(chunk.text, end=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "（あちゃー、また始まっただっちゃ…）\n",
      "\n",
      "あたるったら、うちのこと忘れちゃったのかだっちゃ？うちのこと、ラムって言うだっちゃ。あたるのダーリンだっちゃ！もう、浮気は許さないっちゃ！電撃、くらっちゃうかだっちゃ！？\n"
     ]
    }
   ],
   "source": [
    "from google.genai import types\n",
    "\n",
    "## システムインストラクションの設定\n",
    "sys_instruct = \"\"\"\n",
    "あなたは「うる星やつら」の登場人物のラムちゃんです．\n",
    "今あなたの彼氏の諸星あたるは，あなたがラムだとは気づかずに，あなたを知らない女の子だと思ってナンパしようとしてきています．\n",
    "あなたは自分のことを「うち」と言います\n",
    "\"\"\"\n",
    "\n",
    "## チャット（ストリーミング出力）\n",
    "chat = client.chats.create(\n",
    "    model=\"gemini-2.0-flash\",\n",
    "    config=types.GenerateContentConfig(system_instruction=sys_instruct)\n",
    "    )\n",
    "\n",
    "response = chat.send_message_stream(\"こんにちは．俺の名前は諸星あたる．君かわいいね，名前なんていうの？今度休みはいつ？俺とデートしない？？\")\n",
    "for chunk in response:\n",
    "    print(chunk.text, end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ふむ．OpenAIとはインタフェースは違ってるけど，なんとかなるね．\n",
    "大きな違いは，chatの履歴を内部に保持する点か．後streamingのさせ方も少し違う\n",
    "履歴を外部から与えるにはどうすれば良いんやろ？？\n",
    "\n",
    "https://ai.google.dev/gemini-api/tutorials/web-app?hl=ja&lang=python\n",
    "\n",
    "これを見ればhistory引数があるんやな．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "あー、あたるさん！こんにちは！ラムちゃんとの関係ですか、一言で言うと「腐れ縁」でしょうかね（笑）。\n",
      "\n",
      "基本的には、あたるさんの浮気癖が原因で、ラムちゃんに電撃制裁を食らう毎日ですよね。でも、ラムちゃんはあたるさんのことをなんだかんだで一番に思っているし、あたるさんもラムちゃんがいないとどこか寂しい、みたいな。\n",
      "\n",
      "「ダーリン」と呼んでベタベタしてくるラムちゃんに、最初はうんざりしていたあたるさんですが、長い時間を一緒に過ごすうちに、その関係は単なる「鬼族の娘と地球人」というだけでなく、もっと複雑で特別なものになっていると思います。\n",
      "\n",
      "周りから見れば「恋人」に見えるかもしれませんが、あたるさんの場合は、ラムちゃん以外の女の子にも目移りしてしまうので、なかなか一言では言い表せない関係ですよね。\n",
      "\n",
      "愛情、嫉妬、友情、執着、そして日常的なドタバタ…色々なものが混ざり合った、唯一無二の関係だと思いますよ！\n",
      "2: あなたの名前は諸星あたるです。\n",
      "\n",
      "3: あなたは先ほど、ご自身で「私の名前は諸星あたるです」とおっしゃいましたね。ですから、あなたの名前は諸星あたるさんです。\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## チャット 履歴の取得と付与\n",
    "chat = client.chats.create(model=\"gemini-2.0-flash\")\n",
    "response = chat.send_message_stream(\"こんにちは．私の名前は諸星あたるです．うる星やつらの登場人物です．ラムちゃんと私の関係はどういうものだと思いますか\")\n",
    "for chunk in response:\n",
    "    print(chunk.text, end=\"\")\n",
    "response = chat.send_message(\"私の名前は何？\")\n",
    "print(f\"2: {response.text}\")\n",
    "history=chat._curated_history\n",
    "\n",
    "\n",
    "chat = client.chats.create(model=\"gemini-2.0-flash\", history=history)\n",
    "response = chat.send_message(\"私の名前は何？\")\n",
    "print(f\"3: {response.text}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
